[
    {
        "question": "Question 1\n\nA company collects data for temperature, humidity, and atmospheric pressure in cities across multiple\ncontinents. The average volume of data that the company collects from each site daily is 500 GB. Each site\nhas a high-speed Internet connection.\n\nThe company wants to aggregate the data from all these global sites as quickly as possible in a single Amazon\nS3 bucket. The solution must minimize operational complexity.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket.",
            "Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket. Then remove the data from the origin S3 bucket.",
            "Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site to the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket.",
            "Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic BlockStore (Amazon EBS) volume. At regular intervals, take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. Restore the EBS volume in that Region."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 2\n\nA company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON\nformat in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs\nto perform the analysis with minimal changes to the existing architecture.\n\nWhat should the solutions architect do to meet these requirements with the LEAST amount of operational\noverhead?\n",
        "options": [
            "Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.",
            "Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon CloudWatch console.",
            "Use Amazon Athena directly with Amazon S3 to run the queries as needed.",
            "Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 3\n\nA company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains\nproject reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.\nWhich solution meets these requirements with the LEAST amount of operational overhead?\n",
        "options": [
            "Add the aws PrincipalOrg!D global condition key with a reference to the organization ID to the S3 bucket policy.",
            "Create an organizational unit 7 for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.",
            "Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3bucket policy accordingly.",
            "Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 4\n\nAn application runs on an Amazon EC2 instance in a VPC. The application processes logs that are stored in an Amazon S3 bucket. The EC2 instance needs to access the\nS3 bucket without connectivity to the internet. *\nWhich solution will provide private network connectivity to Amazon S3?\n",
        "options": [
            "Create a gateway VPC endpoint to the S3 bucket.",
            "Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.",
            "Create an instance profile on Amazon EC2 to allow S3 access.",
            "Create an Amazon API Gateway API with a private link to access the S3 endpoint."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 5\n\nA company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon EBS volume. For better\nscalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind\nan Application Load Balancer. After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or\nthe other, but never all of the documents at the same time.\n\nWhat should a solutions architect propose to ensure users see all of their documents at once?\n",
        "options": [
            "Copy the data so both EBS volumes contain all the documents",
            "Configure the Application Load Balancer to direct a user to the server with the documents",
            "Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS",
            "Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 6\n\nA company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in size from 1 MB to 500 GB. The total storage is 70 TB\nand is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as soon as possible while using the\nleast possible network bandwidth.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the AWS CLI to copy all files locally to the S3 bucket.",
            "Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device sothat AWS can import the data into Amazon S3.",
            "Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share onthe S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.",
            "Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3 File Gateway on premises. Create a public virtual interface(VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transferthe data from the existing NFS file share to the S3 File Gateway."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 7\n\nA company has an application that ingests incoming messages. Dozens of other applications and microservices then quickly consume these messages. The number of\nmessages varies drastically and sometimes increases suddenly to 100,000 each second. The\u2122ompany wants to decouple the solution and increase scalability.\nWhich solution meets these requirements?\n",
        "options": [
            "Persist the messages to Amazon Kinesis Data Analytics. Configure the consumer applications to read and process the messages.",
            "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale the number of EC2 instances based on CPU metrics.",
            "Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to preprocess messages and store them in AmazonDynamoDB. Configure the consumer applications to read from DynamoDB to process the messages.",
            "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SOS) subscriptions.Configure the consumer applications to process the messages from the queues."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 8\n\nAcompany is migrating a distributed application to AWS. The application serves variable workloads. The legacy platform consists of a primary server that coordinates\njobs across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resiliency and scalability.\nHow should a solutions architect design the architecture to meet these requirements?\n",
        "options": [
            "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that aremanaged in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.",
            "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that aremanaged in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue.",
            "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure AWS CloudTrail as adestination for the jobs. Configure EC2 Auto Scaling based on the load on the primary server.",
            "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure Amazon EventBridge(Amazon CloudWatch Events) as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 9\n\nA company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created.\nAfter 7 days the files are rarely accessed.\n\nThe total data size is increasing and is close to the company\u2019s total storage capacity. A solutions architect must increase the company's available storage space without\nlosing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
            "Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7days.",
            "Create an Amazon FSx for Windows File Server file system to extend the company's storage space.",
            "Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 10\n\nA company is building an ecommerce web application on AWS. The application sends information about new orders to an Amazon API Gateway REST API to process. The\nTS\ncompany wants to ensure that orders are processed in the order that they are received.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the application receives an order.Subscribe an AWS Lambda function to the topic to perform processing.",
            "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order.Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.",
            "Use an API Gateway authorizer to block any requests while the application processes an order.",
            "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the application receives an order.Configure the SQS standard queue to invoke an AWS Lambda function for processing."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 11\n\nA company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The EC2 instances connect to the database by using user\nnames and passwords that are stored locally \u2122 file. The company wants to minimize the operational overhead of credential management.\nWhat should a solutions architect do to accomplish this goal?\n",
        "options": [
            "Use AWS Secrets Manager. Turn on automatic rotation.",
            "Use AWS Systems Manager Parameter Store. Turn on automatic rotation.",
            "Create an Amazon S3 bucket to store objects that are encrypted with an AWS Key Management Service (AWS KMS) encryption key. Migrate the credential file to theS3 bucket. Point the application to the S3 bucket.",
            "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume for each EC2 instance. Attach the new EBS volume to each EC2 instance. Migrate thecredential file to the new EBS volume. Point the application to the new EBS volume."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 12\n\nA global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has static data and dynamic data.\nThe company stores its static data in an Amazon S3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic data. The\ncompany is using its own domain name registered with Amazon Route 53.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure Route 53 to route traffic to the CloudFront distribution.",
            "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as anendpoint Configure Rute 53 to route traffic to the CloudFront distribution.",
            "Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and theCloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for theweb application.",
            "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as anendpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNSname for static content. Use the domain names as endpoints for the web application."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 13\n\nA company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the company needs to rotate the credentials for its Amazon\nRDS for MySQL databases across multiple AWS Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the required Regions. Configure Secrets Manager to rotate thesecrets on a schedule.",
            "Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter. Use multi-Region secret replication for the required Regions.Configure Systems Manager to rotate the secrets on a schedule.",
            "Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke anAWS Lambda function to rotate the credentials.",
            "Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Store the secrets in an AmazonDynamoDB global table. Use an AWS Lambda function to retrieve the secrets from DynamoDB. Use the RDS API to rotate the secrets."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 14\n\nAcompany runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group\nacross multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL\n8.0 database that is hosted on a large EC2 instance.\n\nThe database's performance degrades quickly as application load increases. The application handles more read requests than write transactions. The company wants a\nsolution that will automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Use Amazon Redshift with a single node for leader and compute functionality.",
            "Use Amazon RDS with a Single-AZ deployment Configure Amazon RDS to add reader instances in a different Availability Zone.",
            "Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas.",
            "Use Amazon ElastiCache for Memcached with EC2 Spot Instances."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 15\n\nA company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in and out of the production VPC. The company had an inspection\nserver in its on-premises data center. The inspection server performed specific operations such as traffic flow inspection and traffic filtering. The company wants to have\nthe same functionalities in the AWS Cloud.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC.",
            "Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.",
            "Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC.",
            "Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for the production VPC."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 16\n\nAccompany hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that\nprovides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations.\nThe rest of the company should have only limited access.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboardswith the appropriate IAM roles.",
            "Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboardswith the appropriate users and groups.",
            "Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reportsto Amazon S3. Use S3 bucket policies to limit access to the reports.",
            "Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL.Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 17\n\nA company is implementing a new business application. The application runs on two Amazon EC2 instances and uses an Amazon S3 bucket for document storage. A\nsolutions architect needs to ensure that the EC2 instances can access the S3 bucket.\nWhat should the solutions architect do to meet this requirement?\n",
        "options": [
            "Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.",
            "Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.",
            "Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.",
            "Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 18\n\nAn application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the\nweb interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in\nits compressed form in a different S3 bucket.\n\nA solutions architect needs to design a solution that uses durable, stateless components to process the images automatically.\n\nWhich combination of actions will meet these requirements? \n",
        "options": [
            "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to theS3 bucket.",
            "Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfullyprocessed, delete the message in the queue.",
            "Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, write the file name to a text file in memory and usethe text file to keep track of the images that were processed.",
            "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a textfile on the EC2 instance and invoke the Lambda function.",
            "Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon ampleNotification Service (Amazon SNS) topic with the application owner's email address for further processing.(> (PD))"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 19\n\nA company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database\nservers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC.\nThe appliance is configured with an IP interface that can accept IP packets.\n\nA solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.",
            "Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.",
            "Deploy a transit gateway in the inspection VPConfigure route tables to route the incoming packets through the transit gateway.",
            "Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to theappliance."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 20\n\nAccompany wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2\ninstances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that\naccesses this data requires consistently high I/O performance.\n\nA solutions architect needs to minimize the time that is required to clone the production data into the test environment.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment.",
            "Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumesto the EC2 instances in the test environment.",
            "Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environmentbefore restoring the volumes from the production EBS snapshots.",
            "Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBSvolumes. Attach the new EBS volumes to EC2 instances in the test environment."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 21\n\nAn ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one product on sale for a period of 24 hours. The company\nwants to be able to handle millions of requests each hour with millisecond latency during peak hours.\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store theorder data in Amazon S3.",
            "Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple Availability Zones. Add an Application Load Balancer (ALB) todistribute the website traffic. Add another ALB for the backend APIs. Store the data in Amazon RDS for MySQL.",
            "Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscalerto increase and decrease the number of pods to process bursts in traffic. Store the data in Amazon RDS for MySQL.",
            "Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon APIGateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 22\n\nA solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. The media files must be resilient to the loss of an\nAvailability Zone. Some files are accessed frequently while other files are rarely accessed in an unpredicta'ygyoattern. The solutions architect must minimize the costs of\nstoring and retrieving the media files.\n\nWhich storage option meets these requirements?\n",
        "options": [
            "S3 Standard",
            "S3 Intelligent-Tiering",
            "S3 Standard-Infrequent Access (S3 Standard-IA)",
            "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 23\n\nAcompany is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for 1 month. However, the files are not accessed after 1\nmonth. The company must keep the files indefinitely.\nWhich storage solution will meet these requirements MOST cost-effectively?\n",
        "options": [
            "Configure S3 Intelligent-Tiering to automatically migrate objects.",
            "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.",
            "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month.",
            "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 24\n\nA company observes an increase in Amazon EC2 costs in its most recent bill. The billing team notices unwanted vertical scaling of instance types for a couple of EC2\ninstances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the\nvertical ecaling.\n\nHow tifa the solutions architect generate the information with the LEAST operational overhead?\n",
        "options": [
            "Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.",
            "Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.",
            "Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.",
            "Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate aninteractive graph based on instance types."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 25\n\nAccompany is designing an application. The application uses an AWS Lambda function to receive information through Amazon API Gateway and to store the information\nin an Amazon Aurora Po At eSQL database.\n\nDuring the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the high volumes of data that the company needs to load into\nthe database. A solutions architect must recommend a new design to improve scalability and minimize the configuration effort.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity(JDBC) drivers.",
            "Change the platform from Aurora to Amazon DynamoDProvision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDBAPI calls at the DAX cluster.",
            "Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integratethe Lambda functions by using Amazon Simple Notification Service (Amazon SNS).",
            "Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integratethe Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 26\n\nA company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have unauthorized configuration changes.\nWhat should a solutions architect do to accomplish this goal?\n\n=",
        "options": [
            "Turn on AWS Config with the appropriate rules.",
            "Turn on AWS Trusted Advisor with the appropriate checks.",
            "Turn on Amazon Inspector with the appropriate assessment template.",
            "Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch Events)."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 27\n\nA company is launching a new application and will display application metrics on an Amazon CloudWatch dashboard. The company's product manager needs to access\nthis dashboard periodically. The product manager does not have an AWS account. A solutions architect must provide access to the product manager by following the\nprinciple of least privilege.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for thedashboard to the product manager.",
            "Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentialswith the product manager. Share the browser URL of the correct dashboard with the product manager.",
            "Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the IAM user. Share the new login credentials with theproduct manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section.",
            "Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On thebastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view thedashboard."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 28\n\nA company is migrating applications to AWS. The applications are deployed in different accounts. The company manages the accounts centrally by using AWS\nordlizations. The company\u2019s security team needs a single sign-on (SSO) solution across all the company's accounts. The company must continue managing the users\nand groups in its on-premises self-managed Microsoft Active Directory.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or a one-way domain trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.",
            "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to connect the company\u2019s self-managed Microsoft ActiveDirectory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.",
            "Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory.",
            "Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 29\n\nAccompany provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service consists of Amazon EC2 instances that run in an Auto Scaling\ngroup. The company has deployments across multiple AWS Regions.\n\nThe company needs to route users to the Region with the lowest latency. The company also needs automated failover between Regions.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Use the NLB as an AWS GlobalAccelerator endpoint in each Region.",
            "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Use the ALB as an AWS GlobalAccelerator endpoint in each Region.",
            "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53latency record that points to aliases for each NLB. Create an Amazon CloudFront distribution that uses the latency record as an origin.",
            "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53weighted record that points to aliases for each ALB. Deploy an Amazon CloudFront distribution that uses the weighted record as an origin."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 30\n\nA development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled. The testing\nlasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and\nmemory attributes of the DB instance.\n\nWhich solution meets these requirements MOST cost-effectively?\n",
        "options": [
            "Stop the DB instance when tests are completed. Restart the DB instance when required.",
            "Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.",
            "Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.",
            "Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again when required."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 31\n\nAcompany that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS DB instances. and Amazon Redshift clusters are configured\nwith tags. The company wants to minimize the effort of configuring and operating this check.\nWhat should a solutions architect do to accomplish this?\n",
        "options": [
            "Use AWS Config rules to define and detect resources that are not properly tagged.",
            "Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.",
            "Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 instance.",
            "Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code.Click to finish recording. Don't show this aaain_ 1:00 PMra BS wchae AGORBAY) f apsy03a= \u00a9 Type here to search rn Bi"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 32\n\nA development team needs to host a website that will be accessed by other teams. The website contents consist of HTML, CSS, client-side JavaScript, and images.\nWhich method is the MOST cost-effective for hosting the website?\n",
        "options": [
            "Containerize the website and host it in AWS Fargate.",
            "Create an Amazon S3 bucket and host the website there.",
            "Deploy a web server on an Amazon EC2 instance to host the website.",
            "Configure an Application Load Balancer with an AWS Lambda target that uses the Express.js framework."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 33\n\nA company runs an online marketplace web application on AWS. The application serves hundreds of thousands of users during peak hours. The company needs a\nscalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed\nto remove sensitive data before being stored in a document database for low-latency retrieval.\n\nWhat should a solutions architect recommend to meet these requirements?\n",
        "options": [
            "Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon write. Use DynamoDBStreams to share the transactions data with other applications.",
            "Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with KinesisData Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.",
            "Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store thetransactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.",
            "Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in AmazonS3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 34\n\nA company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS\nresources and record a history of API calls made to these resources.\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
            "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
            "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.",
            "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 35\n\nA company is preparing to launch a public-facing web application in the AWS Cloud. The architecture consists of Amazon EC2 instances within a VPC behind an Elastic\nLoad Balancer (ELB). A third-party service is used for the DNS. The company's solutions architect must recommend a solution to detect and protect against large-scale\nDDoS attacks.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Enable Amazon GuardDuty on the account.",
            "Enable Amazon Inspector on the EC2 instances.",
            "Enable AWS Shield and assign Amazon Route 53 to it.",
            "Enable AWS Shield Advanced and assign the ELB to it."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 36\n\nA company is building an application in the AWS Cloud. The application will store data in Amazon S3 buckets in two AWS Regions. The company must use an AWS Key\nManagement Service (AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be encrypted and\ndecrypted with the same KMS key. The data and the key must be stored in each of the two Regions.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            1
        ],
        "options": [
            "Create an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configurereplication between the S3 buckets.",
            "Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure replication between the S3 buckets. Configure the application touse the KMS key with client-side encryption.",
            "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managedencryption keys (SSE-S3). Configure replication between the S3 buckets.",
            "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with AWS KMS keys (SSE-KMS).Configure replication between the S3 buckets."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 37\n\nA company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer\n\nthe instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected\nFramework.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Use the EC2 serial console to directly access the terminal interface of each instance for administration.",
            "Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.",
            "Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration ofeach instance.",
            "Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect directly to the instances by using SSHkeys across the VPN tunnel."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 38\n\nA company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. The website is experiencing increased demand from around the world. The\ncompany must decrease latency for users who access the website.\nWhich solution meets these requirements MOST cost-effectively?\n",
        "options": [
            "Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation routing entries.",
            "Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3 bucket. Edit the Route 53 entries to point to the IP addresses ofthe accelerators.",
            "Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to the CloudFront distribution.",
            "Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint."
        ],
        "answers": [
            2
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 39\n\nA company maintains a searchable repository of items on its website. The data is stored in an Amazon RDS for MySQL database table that contains more than 10 million\ntows. The database has 2 TB of General Purpose SSD storage. There are millions of updates against this data every day through the company's website.\n\nThe compzlil#lias noticed that some insert operations are taking 10 seconds or longer. The company has determined that the database storage performance is the\nproblem.\n\nWhich solution addresses this performance issue?\n",
        "options": [
            "Change the storage type to Provisioned IOPS SSD.",
            "Change the DB instance to a memory optimized instance class.",
            "Change the DB instance to a burstable performance instance class.",
            "Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 40\n\nA company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs\nto implement a solution to ingest and store the alerts for future analysis.\n\nThe company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the\ncompany wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days.\n\nWhat is the MOST operationally efficient solution that meets these requirements?\n",
        "options": [
            "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.",
            "Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.",
            "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an AmazonOpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster to take manualsnapshots every day and delete data from the cluster that is older than 14 days.",
            "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts, and set the message retention period to 14 days. Configure consumersto poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy themessage to an Amazon S3 bucket and delete the message from the SQS queue."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 41\n\nA company's application integrates with multiple software-as-a-service (SaaS) sources for data collection. The company runs Amazon EC2 instances to receive the data\nand to upload the data to an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when an\nupload is complete. The company has noticed slow application performance and wants to improve the performance as much as possible.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event notification to send events to an Amazon Simple Notification Service(Amazon SNS) topic when the upload to the S3 bucket is complete.",
            "Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket. Configure an S3 event notification to send events to an AmazonSimple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to send output data. Configure the S3 bucket as the rule's target. Create asecond EventBridge (Cloud Watch Events) rule to send events when the upload to the S3 bucket is complete. Configure an Amazon Simple Notification Service(Amazon SNS) topic as the second rule's target.",
            "Create a Docker container to use instead of an EC2 instance. Host the containerized application on Amazon Elastic Container Service (Amazon ECS). ConfigureAmazon CloudWatch Container Insights to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 42\n\nA company runs a highly available image-processing application on Amazon EC2 instances in a single VPC. The EC2 instances run inside several subnets across multiple\nAvailability Zones. The EC2 instances do not communicate with each other. However, the EC2 instances download images from Amazon S3 and upload images to Amazon\nS3 through a single NAT gateway. The company is concerned about data transfer charges.\n\nWhat is the MOST cost-effective way for the company to avoid Regional data transfer charges?\n",
        "options": [
            "Launch the NAT gateway in each Availability Zone.",
            "Replace the NAT gateway with a NAT instance.",
            "Deploy a gateway VPC endpoint for Amazon S3.",
            "Provision an EC2 Dedicated Host to run the EC2 instances."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 43\n\nAccompany has an on-premises application that generates a large amount of time-sensitive data that is backed up to Amazon S3. The application has grown and there are\nuser complaints about internet bandwidth limitations. A solutions architect needs to design a long-term solution that allows for both timely backups to Amazon S3 and\nwith minimal impact on internet connectivity for internal users.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint.",
            "Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.",
            "Order daily AWS Snowball devices. Load the data onto the Snowball devices and return the devices to AWS each day.",
            "Submit a support ticket through the AWS Management Console. Request the removal of S3 service limits from the account."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 44\n\nA company has an Amazon S3 bucket that contains critical data. The company must protect the data from accidental deletion.\nWhich combination of steps should a solutions architect take to meet these requirements? \n",
        "options": [
            "Enable versioning on the S3 bucket.",
            "Enable MFA Delete on the S3 bucket.",
            "Create a bucket policy on the S3 bucket.",
            "Enable default encryption on the S3 bucket.",
            "Create a lifecycle policy for the objects in the S3 bucket."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 45\n\nA company has a data ingestion workflow that consists of the following:\n\n+ An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data deliveries\n\n+ An AWS Lambda function to process the data and record metadata\n\nThe company observes that the ingestion workflow fails occasionally because of network connectivity issues. When such a failure occurs, the Lambda function does not\ningest the corresponding data unless the company manually reruns the job.\n\nWhich combination of actions should a solutions architect take to ensure that the Lambda function ingests all data in the future? \n",
        "options": [
            "Deploy the Lambda function in multiple Availability Zones.",
            "Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.",
            "Increase the CPU and memory that are allocated to the Lambda function.",
            "Increase provisioned throughput for the Lambda function.",
            "Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.(ns) (PD)OQ"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 46\n\nA company has an application that provides marketing services to stores. The services are based on previous purchases by store customers. The stores upload\ntransaction data to the company through SFTP, and the data is processed and analyzed to generate new marketing offers. Some of the files can exceed 200 GB in size.\nRecently, the company discovered that some of the stores have uploaded files that contain personally identifiable information (PII) that should not have been included.\nThe company wants administrators to be alerted if Pll is shared again. The company also wants to automate remediation.\n\nWhat should a solutions architect do to meet these requirements with the LEAST development effort?\n",
        "options": [
            "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain Pll, trigger an S3 Lifecycle policyto remove the objects that contain Pll.",
            "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain Pll, use Amazon Simple NotificationService (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain Pll.",
            "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain Pll, useAmazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain Pll.",
            "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain Pll, useAmazon Simple Email Service (Amazon SES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the meats that contain Pll."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 47\n\nA company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region for an upcoming event that will last 1 week.\nWhat should the company do to guarantee the EC2 capacity?\n",
        "options": [
            "Purchase Reserved Instances that specify the Region needed.",
            "Create an On-Demand Capacity Reservation that specifies the Region needed.",
            "Purchase Reserved Instances that specify the Region and three Availability Zones needed.",
            "Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 48\n\nA company's website uses an Amazon EC2 instance store for its catalog of items. The company wants to make sure that the catalog is highly available and that the\ncatalog is stored in a durable location.\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Move the catalog to Amazon ElastiCache for Redis.",
            "Deploy a larger EC2 instance with a larger instance store.",
            "Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.",
            "Move the catalog to an Amazon Elastic File System (Amazon EFS) file system."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 49\n\nAcompany stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call, but users access the files infrequently after 1 year. The\ncompany wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older\nfiles is acceptable.\n\nWhich solution will meet these requirements MOST cost-effectively?\n",
        "options": [
            "Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the files from S3 Glacier Instant Retrieval.",
            "Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve thefiles that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.",
            "Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive in Amazon S3 Standard storage. Use S3 Lifecycle policiesto move the files to S3 Glacier Instant Retrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.",
            "Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Deep Archive after 1 year. Store search metadata inAmazon RDS. Query the files from Amazon RDS. Retrieve the files from S3 Glacier Deep Archive."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 50\n\nA company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by third-party software. The company needs to patch the\nthird-party software on all EC2 instances as quickly as possible to remediate a critical security vulnerability.\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Create an AWS Lambda function to apply the patch to all EC2 instances.",
            "Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.",
            "Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.",
            "Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 51 \n\nA company is developing an application that provides order shipping statistics for retrieval by a REST API. The company wants to extract the shipping statistics, organize\nthe data into an easy-to-read HTML format, and send the report to several email addresses at the same time every morning.\nWhich combination of steps should a solutions architect take to meet these requirements?  \n",
        "options": [
            "Configure the application to send the data to Amazon Kinesis Data Firehose.",
            "Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.",
            "Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 52\n\nA company wants to migrate its on-premises application to AWS. The application produces output files that vary in size from tens of gigabytes to hundreds of terabytes.\nThe application data must be stored in a standard file system structure. The company wants a solution that scales automatically. is highly available, and requires\nminimum operational overhead.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon S3 for storage.",
            "Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic Block Store (Amazon EBS) for storage.",
            "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.",
            "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic Block Store (Amazon EBS) for storage.Click to finish recording. Don't show this aaain> 2C Clouly AGORM A) f SVM Cym= \u00a9 Type here to search SS Bi"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 53\n\nAccompany needs to store its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then must be archived for an additional 9\nyears. No one at the company, including administrative users and root users, can be able to delete the records during the entire 10-year period. The records must be\nstored with maximum resiliency.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletion of the records for a period of 10 years.",
            "Store the records by using S3 Intelligent-Tiering. Use an IAM policy to deny deletion of the records. After 10 years, change the IAM policy to allow deletion.",
            "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a periodof 10 years.",
            "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 year. Use S3 Object Lock ingovernance mode for a period of 10 years."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 54\n\nA company runs multiple Windows workloads on AWS. The company's employees use Windows file shares that are hosted on two Amazon EC2 instances. The file shares\nsynchronize data between themselves and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves how users\ncurrently access the files.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.",
            "Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 instances.",
            "Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.",
            "Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 55\n\nA solutions architect is developing a VPC architecture that includes multiple subnets. The architecture will host applications that use Amazon EC2 instances and Amazon\nRDS DB instances. The architecture consists of six subnets in two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a dedicated\nsubnet for databases. Only EC2 instances that run in the private subnets can have access to the RDS databases.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Create a new route table that excludes the route to the public subnets\u2019 CIDR blocks. Associate the route table with the database subnets.",
            "Create a security group that denies inbound traffic from the security group that is assigned to instances in the public subnets. Attach the security group to the DBinstances.",
            "Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DBinstances.",
            "Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and thedatabase subnets."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 56\n\nA company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in ca-centeral-1 Region as a public interface for it\nbackend microservice APIs. Third-party services consume the APIs securely. The company wants to design its AP\ncorresponding certificate so that the third-party services can use HTTPS.\n\nWhich solution will meet these requirements?\n\nfe company's domain name and\n\nassociated with the company's domain name into AWS Certificate Manager (ACM).\n",
        "options": [
            "Create stage variables in API Gateway with Name='Endpoint-URL' and Value = 'Company Domain name' to overwire the default URL. Import the public certificate associated with the company's domain anme into AWS Certificate Manager(ACM).",
            "Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region",
            "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with thecompany's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to routetraffic to the API Gateway endpoint",
            "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into the AWS Certificate Manager (ACM) in the us-east-1 Region. Atteach the certificate to the API Gateway API's. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 57\n\nA company is running a popular social media website. The website gives users the ability to upload images to share with other users. The company wants to make sure\nthat the images do not contain inappropriate content. The company needs a solution that minimizes development effort.\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Use Amazon Comprehend to detect inappropriate content. Use human review for low-confidence predictions.",
            "Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence predictions.",
            "Use Amazon SageMaker to detect inappropriate content. Use ground truth to label low-confidence predictions.",
            "Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content. Use ground truth to label low-confidence predictions."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 58\n\nA company wants to run its critical applications in containers to meet requirements for scalability and availability. The company prefers to focus on maintenance of the\ncritical applications. The company does not want to be responsible for provisioning and managing the underlying infrastructure that runs the containerized workload.\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Use Amazon EC2 instances, and install Docker on the instances.",
            "Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes.",
            "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.",
            "Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-optimized Amazon Machine Image (AMI)."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 59\n\nAcompany hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day.\nWhat should a solutions architect do to transmit and process the clickstream data?\n",
        "options": [
            "Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster with the data to generate analytics.",
            "Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 data lake for Amazon Redshift to use for analysis.",
            "Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is added to the S3 bucket. run an AWS Lambda function to processthe data for analysis.",
            "Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in AmazonRedshift for analysis."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 60\n\nA company has a website hosted on AWS. The website is behind an Application Load Balancer (ALB) that is configured to handle HTTP and HTTPS separately. The\ncompany wants to forward all requests to the website so that the requests will use HTTPS.\nWhat should a solutions architect do to meet this requirement?\n",
        "options": [
            "Update the ALB's network ACL to accept only HTTPS traffic.",
            "Create a rule that replaces the HTTP in the URL with HTTPS.",
            "Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.",
            "Replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI)."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 61\n\nA company is developing a two-tier web application on AWS. The company's developers have deployed the application on an Amazon EC2 instance that connects directly\nto a backend Amazon RDS database. The company must not hardcode database credentials in the application. The company must also implement a solution to\nautomatically rotate the database credentials on a regular basis.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Store the database credentials in the instance metadata. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda functionthat updates the RDS credentials and instance metadata at the same time.",
            "Store the database credentials in a configuration file in an encrypted Amazon S3 bucket. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run ascheduled AWS Lambda function that updates the RDS credentials and the credentials in the configuration file at the same time. Use S3 Versioning to ensure theability to fall back to previous values.",
            "Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the secret. Attach the required permission to the EC2 role togrant access to the secret.",
            "Store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store. Turn on automatic rotation for the encrypted parameters.Attach the required permission to the EC2 role to grant access to the encrypted parameters."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 62\n\nA company is deploying a new public = application to AWS. The application will run behind an Application Load Balancer (ALB). The application needs to be encrypted\nat the edge with an SSL/TLS certificate that is issued by an external certificate authority (CA). The certificate must be rotated each year before the certificate expires.\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate thecertificate.",
            "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Import the key material from the certificate. Apply the certificate to the ALUse the managedrenewal feature to automatically rotate the certificate.",
            "Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA. Apply the certificate to the ALB. Use themanaged renewal feature to automatically rotate the certificate.",
            "Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the ALB. Use Amazon EventBridge (Amazon CloudWatch Events) tosend a notification when the certificate is nearing expiration. Rotate the certificate manually."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 63\n\nA company runs its infrastructure on AWS and has a registered base of 700,000 users for its document management application. The company intends to create a\nproduct that converts large .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files. A\nsolutions architect must design a scalable solution to accommodate demand that will grow rapidly over time.\n\nWhich solution meets these requirements MOST cost-effectively?\n",
        "options": [
            "Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in AmazonS3.",
            "Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS Lambda function to convert the files to .jpg format and store themback in DynamoDB.",
            "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an AutoScaling group. Use a program in the EC2 instances to convert the files to .jpg format. Save the .pdf files and the .jpg files in the EBS store.",
            "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an AutoScaling group. Use a program in the EC2 instances to convert the file to .jpg format. Save the .pdf files and the .jpg files in the EBS store."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 64\n\nAcompany has more than 5 TB of file data on Windows file servers that run on premises. Users and applications interact with the data each day.\nThe company is moving its Windows workloads to AWS. As the company continues this process, the company requires access to AWS and on-premises file storage with\n\nminimum latency. The company needs a solution that minimizes operational overhead and requires no significant changes to the existing file access patterns. The\ncompany uses an AWS Site-to-Site VPN connection for connectivity to AWS.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads touse FSx for Windows File Server on AWS.",
            "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to the S3 File Gateway. Reconfigure the on-premises workloads andthe cloud workloads to use the S3 File Gateway.",
            "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3directly or the S3 File Gateway. depending on each workload's location.",
            "Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on premises. Move the on-premises file datato the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 65\n\nA hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The hospital uses API Gateway and Lambda to upload reports that are in PDF\nformat and JPEG format. The hospital needs to modify the Lambda code to identify protected health information (PHI) in the reports.\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Use existing Python libraries to extract the text from the reports and to identify the PHI from the extracted text.",
            "Use Amazon Textract to extract the text from the reports. Use Amazon SageMaker to identify the PHI from the extracted text.",
            "Use Amazon Textract to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text.",
            "Use Amazon Rekognition to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 66\n\nAccompany has an application that generates a large number of files, each approximately 5 MB in size. The files are stored in Amazon S3. Company policy requires the\nfiles to be stored for 4 years before they can be deleted. Immediate accessibility is always required as the files contain critical business data that is not easy to\nreproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days.\n\nWhich storage solution is MOST cost-effective?\n",
        "options": [
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation. Delete the files 4 years after object creation.",
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-1A) 30 days from object creation. Delete thefiles 4 years after object creation.",
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete thefiles 4 years after object creation.",
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Move the filesto S3 Glacier 4 years after object creation."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 67\n\nA company hosts an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table,\nand deletes the message from the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages.\nWhat should a solutions architect do to ensure messages are being processed once only?\n",
        "options": [
            "Use the CreateQueue API call to create a new queue.",
            "Use the AddPermission API call to add appropriate permissions.",
            "Use the ReceiveMessage API call to set an appropriate wait time.",
            "Use the ChangeMessageVisibility API call to increase the visibility timeout."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 68\n\nA solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection\nwith consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.\nWhat should the solutions architect do to meet these requirements?\n",
        "options": [
            "Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.",
            "Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPNconnection fails.",
            "Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary DirectConnect connection fails.",
            "Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection ifthe primary Direct Connect connection fails."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 69\n\nA corfipany is running a business-critical web application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances are in an Auto Scaling group.\nThe application uses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The company wants the application to be highly available with\nminimum downtime and minimum loss of data.\n\nWhich solution will meet these requirements with the LEAST operational effort?\n",
        "options": [
            "Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect traffic. Use Aurora PostgreSQL Cross-Region Replication.",
            "Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ. Configure an Amazon RDS Proxy instance for thedatabase.",
            "Configure the Auto Scaling group to use one Availability Zone. Generate hourly snapshots of the database. Recover the database from the snapshots in the event ofa failure.",
            "Configure the Auto Scaling group to use multiple AWS Regions. Write the data from the application to Amazon S3. Use S3 Event Notifications to launch an AWSLambda function to write the data to the database.Click to finish recording. Don't show this again\u2014ta \u00a9 Type here to search >? =i] e | a ~] (o} SG 6 @ ds WC lightrin A BORE A f ie a"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 70\n\nAcompany's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group is configured to use an Amazon EC2 Auto Scaling group with multiple\nEC2 instances that run the web servig\n\nThe company notices that the NLB is not detecting HTTP errors for the application. These errors require a manual restart of the EC2 instances that run the web service.\nThe company needs to improve the application's availability without writing custom scripts or code.\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Enable HTTP health checks on the NLB, supplying the URL of the company's application.",
            "Add a cron job to the EC2 instances to check the local application's logs once each minute. If HTTP errors are detected. the application will restart.",
            "Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the URL of the company's application. Configure an Auto Scalingaction to replace unhealthy instances.",
            "Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the NLB. Configure an Auto Scaling action to replace unhealthy instanceswhen the alarm is in the ALARM state."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 71\n\nA company runs a shoprigy application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a\nsolution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour.\nWhat should the solutions architect recommend to meet these requirements?\n",
        "options": [
            "Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS Region.",
            "Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.",
            "Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data from S3 Glacier to DynamoDB.",
            "Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table by usingthe EBS snapshot."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 72\n\nA company runs a photo processing application that needs to frequently upload and download pictures from Amazon S3 buckets that are located in the same AWS\nRegion. A solutions architect has noticed an increased cost in data transfer fees and needs to implement a solution to reduce these costs.\n\nHow can the solutions architect meet this requirement?\n",
        "options": [
            "Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through it.",
            "Deploy a NAT gateway into a public subnet and attach an endpoint policy that allows access to the S3 buckets.",
            "Deploy the application into a public subnet and allow it to route through an internet gateway to access the S3 buckets.",
            "Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 73\n\nA company recently launched Linux-based application instances on Amazon EC2 in a private subnet and launched a Linux-based bastion host on an Amazon EC2 instance\nin a public subnet of a VPC. A solutions architect needs to connect from the on-premises network, through the company's internet connection, to the bastion host, and to\nthe application servers. The solutions architect must make sure that the security groups of all the EC2 instances will allow that access.\n\nWhich combination of steps should the solutions architect take to meet these requirements? \n",
        "options": [
            "Replace the current security group of the bastion host with one that only allows inbound access from the application instances.we. Replace the current security group of the bastion host with one that only allows inbound access from the internal IP range for the company.",
            "Replace the current security group of the bastion host with one that only allows inbound access from the external IP range for the company.",
            "Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.",
            "Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 74\n\nA solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database\ntier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company.\nHow should security groups be configured in this situation? \n",
        "options": [
            "Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.",
            "Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.",
            "Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier.",
            "Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier.",
            "Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier.Q\u00ae"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 75\n\nA company wants to move a multi-tiered application from on premises to the AWS Cloud to improve the application's performance. The application consists of application\ntiers that communicate with each other by way of RESTful services. Transactions are dropped when one tier becomes overloaded. A solutions architect must design a\nsolution that resolves these issues and modernizes the application.\n\nWhich solution meets these requirements and is the MOST operationally efficient?\n",
        "options": [
            "Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application layer. Use Amazon Simple Queue Service (Amazon SQS) as thecommunication layer between application services.",
            "Use Amazon CloudWatch metrics to analyze the application performance history to determine the servers\u2019 peak utilization during the performance failures.Increase the size of the application server's Amazon EC2 instances to meet the peak requirements.",
            "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group.Use Amazon CloudWatch to monitor the SNS queue length and scale up and down as required.",
            "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. UseAmazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 76\n\nA company receives 10 TB of instrumentation data each day from several machines located at a single factory. The data consists of JSON files stored on a storage area\nnetwork (SAN) in an on-premises data center located within the factory. The company wants to send this data to Amazon S3 where it can be accessed by several\nadditional systems that provide critical near-real-time analytics. A secure transfer is important because the data is considered sensitive.\n\nWhich solution offers the MOST reliable data transfer?\n",
        "options": [
            "AWS DataSync over public internet",
            "AWS DataSync over AWS Direct Connect",
            "AWS Database Migration Service (AWS DMS) over public internet",
            "AWS Database Migration Service (AWS DMS) over AWS Direct Connect"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 77\n\nAcompany needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is\nstreamed, and a storage solution for the data.\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream thatuses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data toAmazon S3.",
            "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination checking on the EC2 instance. Use AWS Glue to transformthe data and to send the data to Amazon S3.",
            "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses theKinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to AmazonS3.",
            "Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform the data. Use AWS Glue to send the data to Amazon S3."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 78\n\nA company needs to keep user transaction data in an Amazon DynamoDB table. The company must retain the data for 7 years.\nWhat is the MOST operationally efficient solution that meets these requirements?\n",
        "options": [
            "Use DynamoDB point-in-time recovery to back up the table continuously.",
            "Use AWS Backup to create backup schedules and retention policies for the table.",
            "Create an on-demand backup of the table by using the DynamoDB console. Store the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3bucket.",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda function. Configure the Lambda function to back up the table and tostore the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 79\n\nA company is planning to use an Amazon DynamoDB table for data storage. The company is concerned about cost optimization. The table will not be used on most\nmornings. In the evenings, the read and write traffic will often be unpredictable. When traffic spikes occur, they will happen very quickly.\nWhat should a solutions architec: recommend?\n",
        "options": [
            "Create a DynamoDB table in on-demand capacity mode.",
            "Create a DynamoDB table with a global secondary index.",
            "Create a DynamoDB table with provisioned capacity and auto scaling.",
            "Create a DynamoDB table in provisioned capacity mode, and configure it as a global table."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 80\n\nA company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an application migration initiative. A solutions architect needs\nta share an Amazon Machine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The AMI is backed by Amazon Elastic Block Store\n(Amazon EBS) and uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt EBS volume snapshots.\n\nWhat is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's AWS account?\n",
        "options": [
            "Make the encrypted AMI and snapshots publicly available. Modify the key policy to allow the MSP Partner's AWS account to use the key.",
            "Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to allow the MSP Partner's AWSaccount to use the key.",
            "Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to trust a new KMS key that isowned by the MSP Partner for encryption.",
            "Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS account, Encrypt the S3 bucket with a new KMS key that is owned bythe MSP Partner. Copy and launch the AMI in the MSP Partner's AWS account."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 81 \n\nA solutions architect is designing the cloud architecture for a new application being deployed on AWS. The process should run in parallel while adding and removing\napplication nodes as needed based on the number of jobs to be processed. The processor application is stateless. The solutions architect must ensure that the\napplication is loosely coupled and the job items are durably stored.\nWhich design should the solutions architect use?",
        "options": [
            "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application.Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add\nand remove nodes based on CPU usage.",
            "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application.Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to addand remove nodes based on network usage.",
            "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application.Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue.",
            "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application.Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 82\n\nAccompany hosts its web applications in the AWS Cloud. The company configures Elastic Load Balancers to use certificates that are imported into AWS Certificate\nManager (ACM). The company's security team must be notified 30 days before the expiration of each certificate.\nWhat should a solutions architect recommend to meet this requirement?\n",
        "options": [
            "Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service (Amazon SNS) topic every day, beginning 30 days before any certificatewill expire.",
            "Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure Amazon EventBridge (Amazon CloudWatch Events) to invoke acustom alert by way of Amazon Simple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource.",
            "Use AWS Trusted Advisor to check for certificates that will expire within 30 days. Create an Amazon CloudWatch alarm that is based on Trusted Advisor metrics forcheck status changes. Configure the alarm to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates that will expire within 30 days. Configure the rule to invoke an AWSLambda function. Configure the Lambda function to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS)."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 83\n\nA company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site\nloading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is\n\nneeded.\nWhat should the solutions architect recommend?\n",
        "options": [
            "Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.",
            "Move the website to Amazon S3. Use Cross-Region Replication between Regions.",
            "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
            "Use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 84\n\nA company wants to reduce the cost of its existing three-tier web architecture. The web, application, and database servers are running on Amazon EC2 instances for the\ndevelopment, test, and production environments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization during non-peak hours.\n\nThe production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 hours each day. The company plans to implement\nautomation to stop the development and test EC2 instances when they are not in use.\n\nWhich EC2 instance purchasing solution will meet the company's requirements MOST cost-effectively?\n",
        "options": [
            "Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.",
            "Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and test EC2 instances.",
            "Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.",
            "Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and test EC2 instances."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 85\n\nA company has a production web application in which users upload documents through a web interface or a mobile app. According to a new regulatory requirement. new\ndocuments cannot be modified or deleted after they are stored.\nWhat should a solutions architect 99 to meet this requirement?\n\nBs\n",
        "options": [
            "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled.",
            "Store the uploaded documents in an Amazon S3 bucket. Configure an S3 Lifecycle policy to archive the documents periodically.",
            "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled. Configure an ACL to restrict all access to read-only.",
            "Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume. Access the data by mounting the volume in read-only mode.Click to finish recording, Don't show this again,\u20143 Type here to search . i e a [| ~] (-} ie} 6 [Po BS 33\u00b0C Hae ABODE) f apes %"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 86\n\nA company has several web servers that need to frequently access a common Amazon RDS MySQL Multi-AZ DB instance. The company wants a secure method for the\nweb servers to connect to the database while meeting a security requirement to rotate user credentials frequently.\nWhich solution meets these requirements?\n",
        "options": [
            "Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow the web servers to access AWS Secrets Manager.",
            "Store the database user credentials in AWS Systems Manager OpsCenter. Grant the necessary IAM permissions to allow the web servers to access OpsCenter.",
            "Store the database user credentials in a secure Amazon S3 bucket. Grant the necessary IAM permissions to allow the web servers to retrieve credentials andaccess the database.",
            "Store the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the web server file system. The web server should be ableto decrypt the files and access the database."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 87\n\nA company hosts an application on AWS Lambda functions that are invoked by an Amazon API Gateway API. The Lambda functions save customer data to an Amazon\nAurora MySQL database. Whenever the company upgrades the database, the Lambda functions fail to establish database connections until the upgrade is complete. The\nresult is that customer data is not recorded for some of the event.\n\nA solutions architect needs to design a solution that stores customer data that is created during database upgrades.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Provision an Amazon RDS proxy to sit between the Lambda functions and the database. Configure the Lambda functions to connect to the RDS proxy.",
            "Increase the run time of the Lambda functions to the maximum. Create a retry mechanism in the code that stores the customer data in the database.",
            "Persist the customer data to Lambda local storage. Configure new Lambda functions to scan the local storage to save the customer data to the database.",
            "Store the customer data in an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Create a new Lambda function that polls the queue and stores thecustomer data in the database."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 88\n\nA survey company has gathered data for several years from areas in the United States. The company hosts the data in an Amazon S3 bucket that is 3 TB in size and\ngrowing. The company has started to share the data with a Eurcggen marketing firm that has S3 buckets. The company wants to ensure that its data transfer costs\nremain as low as possible.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Configure the Requester Pays feature on the company's S3 bucket.",
            "Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing firm's S3 buckets.",
            "Configure cross-account access for the marketing firm so that the marketing firm has access to the company's S3 bucket.",
            "Configure the company's S3 bucket to use S3 Intelligent-Tiering. Sync the S3 bucket to one of the marketing firm's S3 buckets."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 89\n\nA company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to restrict access to audit team IAM user credentials according\nto the principle of least privilege. Company managers are worried about accidental deletion of documents in the S3 bucket and want a more secure solution.\nWhat should a solutions architect do to secure the audit documents?\n",
        "options": [
            "Enable the versioning and MFA Delete features on the S3 bucket.",
            "Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account.",
            "Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during audit dates.",
            "Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 90\n\nA company is using a SQL database to store movie data that is publicly accessible. The database runs on an Amazon RDS Single-AZ DB instance. A script runs queries at\nrandom intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours.\n\nThe company's development team notices that the database performance is inadequate for development tasks when the script is running. A solutions architect must\nTecommend a solution to resolve this issue.\n\nWhich solution will meet this requirement with the LEAST operational overhead?\n",
        "options": [
            "Modify the DB instance to be a Multi-AZ deployment.",
            "Create a read replica of the database. Configure the script to query only the read replica.",
            "Instruct the development team to manually export the entries in the database at the end of each day.",
            "Use Amazon ElastiCache to cache the common queries that the script runs against the database."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 91\n\nA company has applications that run on Amazon EC2 instances in a VPC. One of the applications needs to call the Amazon S3 API to store and read objects. According to\nthe company's security regulations, no traffic from the applications is allowed to travel across the internet.\nWhich solution will meet these requirements?\n",
        "options": [
            "Configure an S3 gateway endpoint.",
            "Create an S3 bucket in a private subnet.",
            "Create an S3 bucket in the same AWS Region as the EC2 instances.",
            "Configure a NAT gateway in the same subnet as the EC2 instances."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 92\n\nAcompany is storing sensitive user information in an Amazon S3 bucket. The company wants to provide secure access to this bucket from the application tier running on\nAmazon EC2 instances inside a VPC.\nWhich combination of steps should a solutions architect take to accomplish this? \n",
        "options": [
            "Configure a VPC gateway endpoint for Amazon S3 within the VPC.",
            "Create a bucket policy to make the objects in the S3 bucket public.",
            "Create a bucket policy that limits access to only the application tier running in the VPC.",
            "Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance.",
            "Create a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 93\n\nA company runs an on-premises application that is powered by a MySQL database. The company is migrating the application to AWS to increase the application's\nelasticity and availability.\n\nThe current architecture shows heavy read activity on the database during times of normal operation. Every 4 hours, the company's development team pulls a full export\nof the production database to populate a database in the staging environment. During this period, users experience unacceptable application latency. The development\nteam is unable to use the staging environment until the procedure completes.\n\nA solutions architect must recommend replacement architecture that alleviates the application latency issue. The replacement architecture also must give the\ndevelopment team the ability to continue using the staging environment without delay.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging database by implementing a backup and restore process that usesthe mysqldump utility.",
            "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.",
            "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Use the standby instance for the staging database.",
            "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate the staging database by implementing a backup and restoreprocess that uses the mysqldump utility."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 94\n\nAccompany is designing an application where users upload small files into Amazon S3. After a user uploads a file, the file requires one-time simple processing to\ntransform the data and save the data in JSON format for later analysis.\n\nEach file must be processes quickly as possible after it is uploaded. Demand will vary. On some days, users will upload a high number of files. On other days, users will\nupload a few files or no files.\n\nWhich solution meets these requirements with the LEAST operational overhead?\n",
        "options": [
            "Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the data. Store the resulting JSON file in an Amazon Aurora DBcluster.",
            "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queueand process the data. Store the resulting JSON file in Amazon DynamoDB.",
            "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from thequeue and process the data. Store the resulting JSON file in Amazon DynamoDB.",
            "Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new file is uploaded. Use an AWS Lambdafunction to consume the event from the stream and process the data. Store the resulting JSON file in an Amazon Aurora DB cluster."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 95\n\nAn application allows users at a company's headquarters to access product data. The product data is stored in an Amazon RDS MySQL DB instance. The operations team\nhas isolated an application performance slowdown and wants to separate read traffic from write traffic. A solutions architect needs to optimize the application's\n\nperformance quickly.\nWhat should the solutions architect recommend?\n",
        "options": [
            "Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary Availability Zone.",
            "Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary Availability Zone.",
            "Create read replicas for the database. Configure the read replicas with half of the compute and storage resources as the source database.",
            "Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 96\n\nAn Amazon EC2 adminstrator created the fowlling policy associated with an IAM group containing several users:\n\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": \"ec2:terminateInstances\",\n\"Resource\": \"*\"\n\"Condition\": {\n\"IpAddress\": {\n\"aws:SourceIp\":\"10.100.100.0/24\"\n}\n}\n},{\n\"Effect\": \"Deny\",\n\"Action\": \"ec2:*\",\n\"Resource\": \"*\"\n\"Condition\": {\n\"StringNotEquals\":{\n \"ec2:Region\": \"us-east-1\", \n}\n}\n}\n]\n\nWhich combination of actions will the users be able to perform? \n",
        "options": [
            "Users can terminate an EC2 instance in any AWS Region except us-east-1.",
            "Users can terminate an EC2 instance iwth the IP address 10.100.100.1 in the us-east-1 Region.",
            "Users can terminate an EC2 instance in the us-east-1 Region when hte user's souce IP is 10.100.100.254.",
            "Users can not terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 97\n\nA company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft Windows shared file storage. The company wants to migrate this\nworkload to the AWS Cloud and is considering various storage Mons. The storage solution must be highly available and integrated with Active Directory for access\ncontrol.\n\nWhich solution will satisfy these requirements?\n",
        "options": [
            "Configure Amazon EFS storage and set the Active Directory domain for authentication.",
            "Create an SMB file share on an AWS Storage Gateway file gateway in two Availability Zones.",
            "Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume.",
            "Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 98\n\nAn image-processing company has a web application that users use to upload images. The application uploads the images into an Amazon S3 bucket. The company has\nset up S3 event notific sj is to publish the object creation events to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as the event\nsource for an AWS Lambda function that processes the images and sends the results to users through email.\n\nUsers report that they are receiving multiple email messages for every uploaded image. A solutions architect determines that SQS messages are invoking the Lambda\nfunction more than once, resulting in multiple email messages.\n\nWhat should the solutions architect do to resolve this issue with the LEAST operational overhead?\n",
        "options": [
            "Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds.",
            "Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard duplicate messages.",
            "Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout.",
            "Modify the Lambda function to delete each message from the SQS queue immediately after the message is read before processing.Click to finish recording, Don't show this againEO Type here to search \" # @QrBAHBGBES Ss Vf Plicame A GOB BAY) fF aa ay"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 99\n\nA company is implementing a shared storage solution for a gaming application that is hosted in an on-premises data center. The company needs the ability to use Lustre\nclients to access data. The solution must be fully managed.\nWhich solution meets these requirements?\n",
        "options": [
            "Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.",
            "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.",
            "Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect theapplication server to the file system.",
            "Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 100\n\nA company's containerized application runs on an Amazon EC2 instance. The application needs to download security certificates before it can communicate with other\nbusiness applications. The company wants a highly secure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in\nhighly available storage after the data is encrypted.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as needed. Control access to the data by using fine-grained [AMaccess.",
            "Create an AWS Lambda function that uses the Python cryptography library to receive and perform encryption operations. Store the function in an Amazon S3bucket.",
            "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypteddata on Amazon S3.",
            "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypteddata on Amazon Elastic Block Store (Amazon EBS) volumes."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 101\n\nAccompany is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message\nService (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.",
            "Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.",
            "Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.",
            "Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.Click to finish recording, Don't show this again"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 102\n\nA company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored in the S3 bucket. Additionally, the encryption key must be\nautomatically rotated every year.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3encryption keys.",
            "Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the S3 bucket\u2019s default encryption behavior to usethe customer managed KMS key. Move the data to the S3 bucket.",
            "aw A AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket's default encryption behavior to use the customer managed KMSkey. Move the data to the S3 bucket. Manually rotate the KMS key every year.",
            "Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS Key Management Service (AWS KMS) key without keymaterial. Import the customer key material into the KMS key. Enable automatic key rotation."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 103\n\nThe customers of a finance company request appointments with financial advisors by sending text messages. A web application that runs on Amazon EC2 instances\naccepts the appointment requests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another\napplication that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages to the customers. After successful scheduling, this\napplication stores the meeting information in an Amazon DynamoDB database.\n\nAs the company expands, customers report that their meeting invitations are taking longer to arrive.\n\nWhat should a solutions architect recommend to resolve this issue?\n",
        "options": [
            "Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.",
            "Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.",
            "Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the appointment requests.",
            "Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SQS queue."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 104\n\nAn online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers\nand stores this data in Amazon S3. Additional customer data is stored in Amazon RDS.\n\nThe company wants to make all the data available to various teams so that the teams can perform analytics. The solution must provide the ability to manage fine-grained\npermissions for the data and must minimize operational overhead.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.",
            "Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an AWS Glue crawler. Use Amazon Athena to query the data.Use S3 policies to limit access.",
            "Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use LakeFormation access controls to limit access.",
            "Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from Amazon S3 and Amazon RDS to Amazon Redshift. UseAmazon Redshift access controls to limit access."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 105\n\nA company hosts a marketing website in an on-premises data center. The website consists of static documents and runs on a single server. An administrator updates the\nwebsite content infrequently and uses an SFTP client to upload new documents.\n\nThe company decides to host its website on AWS and to use Amazon CloudFront. The company\u2019s solutions architect creates a CloudFront distribution. The solutions\narchitect must design the most cost-effective and resilient architecture for website hosting to serve as the CloudFront origin.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Create a virtual server by using Amazon Lightsail. Configure the web server in the Lightsail instance. Upload website content by using an SFTP client.",
            "Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer. Upload website content by using an SFTP client.",
            "Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAl). Upload website content by using theAWS CLI.",
            "Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload website content by using the SFTP client."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 106\n\nA company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where the AMIs were created. The company\n\nneeds to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 Createlmage API operation is called within the company's\naccount.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a Createlmage API call is detected.",
            "Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that occurs when updated logs are sent to Amazon S3. UseAmazon Athena to create a new table and to query on Createlmage when an API call is detected.",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. Configure the target as an Amazon Simple Notification Service(Amazon SNS) topic to send an alert when a Createlmage API call is detected.",
            "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to anAmazon Simple Notification Service (Amazon SNS) topic when a Createlmage API call is detected."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 107\n\nA company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate microservice for\n\nprocessing. The company is using Amazon API Gateway to deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests\n\nbefore dispatching them to the processing microservices.\nThe company provisioned as much DynamoDB throughput as its budget allows, but the company is still experiencing availability issues and is losing user requests.\n\nWhat should a solutions architect do to address this issue without impacting existing users?\n",
        "options": [
            "Add throttling on the API Gateway with server-side throttling limits.",
            "Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.",
            "Create a secondary index in DynamoDB for the table with the user requests.",
            "Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 108\n\nA company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through\npublic ie routes. Only the EC2 instance can have access to upload data to the S3 bucket.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2instance's IAM role for access.",
            "Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint.Attach a resource policy to the S3 bucket to only allow the EC2 instance\u2019s IAM role for access.",
            "Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket\u2019s service API endpoint. Create a route in the VPC route table toprovide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.",
            "Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket's service API endpoint. Create a route in the VPC routetable to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 109\n\nA solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The application will run on Amazon EC2 On-Demand Instances\nand will automatically scale across multiple Availability Zones. The EC2 instances will scale up and down frequently throughout the day. An Application Load Balancer\n(ALB) will handle the load distribution. The architecture needs to support distributed session data management. The company is willing to make changes to code if\nneeded.\n\nWhat should the solutions architect do to ensure that the architecture supports distributed session data management?\n",
        "options": [
            "Use Amazon ElastiCache to manage and store session data.",
            "Use session affinity (sticky sessions) of the ALB to manage session data.",
            "Use Session Manager from AWS Systems Manager to manage the session.",
            "Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage the session."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 110\n\nA company offers a food delivery service that is growing rapidly. Because of the growth, the company\u2019s order processing system is experiencing scaling problems during\npeak traffic hours. The current architecture includes the following:\n\n+ Agroup of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the application\n+ Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders\n\nThe order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event.\n\nA solutions architect must ensure that the order collection process and the order fulfillment process can both scale properly during peak traffic hours. The solution must\noptimize utilization of the company\u2019s AWS resources.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure each Auto Scaling group's minimum capacityaccording to peak workload values.",
            "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure a CloudWatch alarm to invoke an Amazon SimpleNotification Service (Amazon SNS) topic that creates additional Auto Scaling groups on demand.",
            "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to polltheir respective queue. Scale the Auto Scaling groups based on notifications that the queues send.",
            "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to polltheir respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 111\n\nA company hosts multiple production applications. One of the applications consists of resources from Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple\nNotification Service (Amazon SNS), and Amazon Simple Queue Service (Amazon SQS) across multiple AWS Regions. All company resources are tagged with a tag name\nof \u201capplication\u201d and a value that corresponds to each application. A solutions architect must provide the quickest solution for identifying all of the tagged components.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Use AWS CloudTrail to generate a list of resources with the application tag.",
            "Use the AWS CLI to query each service across all Regions to report the tagged components.",
            "Run a query in Amazon CloudWatch Logs Insights to report on the components with the application tag.",
            "Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag.Click to finish recording. Don't show this again, 1"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 112\n\nA company needs to export its database once a day to Amazon S3 for other teams to access. The exported object size varies between 2 GB and 5 GB. The S3 access\npattern for the data is variable and changes rapidly. The data must be immediately available and must remain accessible for up to 3 months. The company needs the\nmost cost-effective solution that will not increase retrieval time.\n\nWhich S3 storage class should the company use to meet these requirements?\n",
        "options": [
            "S3 Intelligent-Tiering",
            "S3 Glacier Instant Retrieval",
            "S3 Standard",
            "S3 Standard-Infrequent Access (S3 Standard-IA)"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 113\n\nAccompany is developing a new mobile app. The company must implement proper traffic filtering to protect its Application Load Balancer (ALB) against common\napplication-level attacks, such as cross-site scripting or SQL injection. The company has minimal infrastructure and operational staff. The company needs to reduce its\nshare of the responsibility in managing, updating, and securing servers for its AWS environment.\n\nWhat should a solutions architect recommend to meet these requirements?\n",
        "options": [
            "Configure AWS WAF rules and associate them with the ALB.",
            "Deploy the application using Amazon S3 with public hosting enabled.",
            "Deploy AWS Shield Advanced and add the ALB as a protected resource.",
            "Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall, which then passes the traffic to the current ALB."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 114\n\nAcompany\u2019s reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company must convert these files to Apache Parquet format and\nmust store the files in a transformed data bucket.\n\nWhich solution will meet these requirements with the LEAST development effort?\n",
        "options": [
            "Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the data. Use EMR File System (EMRFS) to write files to thetransformed data bucket.",
            "Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed databucket in the output step.",
            "Use AWS Batch to create a job definition with Bash syntax to transform the data and output the data to the transformed data bucket. Use the job definition tosubmit a job. Specify an array job as the job type.",
            "Create an AWS Lambda function to transform the data and output the data to the transformed data bucket. Configure an event notification for the S3 bucket.Specify the Lambda function as the destination for the event notification."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 115\n\nA company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup data need to be accessible for infrequent regulatory\ntequests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1\nmonth. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer.\n\nWhat should a solutions architect do to migrate and store the data at the LOWEST cost?\n",
        "options": [
            "Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.",
            "Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on premises to Amazon S3 Glacier.",
            "Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier DeepArchive.",
            "Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync task to copy files from the on-premises NAS storage toAmazon S3 Glacier. 5"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 116\n\nA company has a serverless website with millions of objects in an Amazon S3 bucket. The company uses the S3 bucket as the origin for an Amazon CloudFront\ndistribution. The company did not set encryption on the S3 bucket before the objects were loaded. A solutions architect needs to enable encryption for all existing objects\nand for all objects that are added to the S3 bucket in the future.\n\nWhich solution will meet these requirements with the LEAST amount of effort?\n",
        "options": [
            "Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download all existing objects to temporary local storage. Upload the objectsto the new S3 bucket.",
            "Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 BatchOperations job that uses the copy command to encrypt those objects.",
            "Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the settings on the S3 bucket to use server-side encryption with AWSKMS managed encryption keys (SSE-KMS). Turn on versioning for the S3 bucket.",
            "Navigate to Amazon S3 in the AWS Management Console. Browse the S3 bucket's objects. Sort by the encryption field. Select each unencrypted object. Use theModify button to apply default encryption settings to every unencrypted object in the S3 bucket. 6"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 117\n\nA company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company\nneeds to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when\nthe primary infrastructure is healthy.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in asecond AWS Region.",
            "Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora Replica inthe second Region.",
            "Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora database that is restoredfrom the latest snapshot.",
            "Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-passivefailover. Create an Aurora second primary instance in the second Region."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 118\n\nA company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2\ninstance. The default network ACL has been modified to block all traffic. A solutions architect needs to make the web server accessible from everywhere on port 443.\n\nWhich combination of steps will accomplish this task? \n",
        "options": [
            "Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.",
            "Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.",
            "Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.",
            "Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0.",
            "Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.Click to finish recording. Don't show this aaainDPECERITATIAN! TITLE \u00b0\u201d =\u00bb - _ 11:21 PM2\u00ae 22\u00b0C Mostly louly AGORA ay)\u2018Type here to search"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 119\n\nA company's application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used\nAWS CloudFormation to deploy infrastructure and used the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are reporting\ndelays when the users attempt to access the application.\n\nWhich solution will resolve these issues in the MOST operationally efficient way?\n",
        "options": [
            "Replace the EC2 instances with T3 EC2 instances that run in an Auto Scaling group. Make the changes by using the AWS Management Console.",
            "Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the desired capacity and the maximum capacity of the AutoScaling group manually when an increase is necessary.",
            "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track theapplication performance for future capacity planning.",
            "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generatecustom application latency metrics for future capacity planning. 9"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 120\n\nA solutions architect is designing a new API using Amazon API Gateway that will receive requests from users. The volume of requests is highly variable; several hours can\npass without receiving a single request. The data processing will take place asynchronously, but should be completed within a few seconds after a request is made.\n\nWhich compute service should the solutions architect have the API invoke to deliver the requirements at the lowest cost?\n",
        "options": [
            "An AWS Glue job",
            "An AWS Lambda function",
            "A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS)",
            "A containerized service hosted in Amazon ECS with Amazon EC2 10"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 121\n\nAcompany runs an application on a group of Amazon Linux EC2 instances. For compliance reasons, the company must retain all application log files for 7 years. The log\nfiles will be analyzed by a reporting tool that must be able to access all the files concurrently.\n\nWhich storage solution meets these requirements MOST cost-effectively?\n",
        "options": [
            "Amazon Elastic Block Store (Amazon EBS)",
            "Amazon Elastic File System (Amazon EFS)",
            "Amazon EC2 instance store",
            "Amazon S3 11"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 122\n\nA company has hired an external vendor to perform work in the company's AWS account. The vendor uses an automated tool that is hosted in an AWS account that the\nvendor owns. The vendor does not have IAM access to the company\u2019s AWS account.\n\nHow should a solutions architect grant this access to the vendor?\n",
        "options": [
            "Create an IAM role in the company\u2019s account to delegate access to the vendor's IAM role. Attach the appropriate IAM policies to the role for the permissions thatthe vendor requires.",
            "Create an IAM user in the company\u2019s account with a password that meets the password complexity requirements. Attach the appropriate IAM policies to the userfor the permissions that the vendor requires.",
            "Create an IAM group in the company's account. Add the tool's IAM user from the vendor account to the group. Attach the appropriate IAM policies to the group forthe permissions that the vendor requires.",
            "Create a new identity provider by choosing \u201cAWS account\u201d as the provider type in the IAM console. Supply the vendor's AWS account ID and user name. Attach theappropriate IAM policies to the new provider for the permissions that the vendor requires. 12"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 123\n\nA company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic Kubernetes Service (Amazon EKS) in private subnets. The application\nneeds to write data to an Amazon DynamoDB table. A solutions architect must ensure that the application can interact with the DynamoDB table without exposing traffic\nto the internet.\n\nWhich combination of steps should the solutions architect take to accomplish this goal? \n",
        "options": [
            "Attach an IAM role that has sufficient privileges to the EKS pod.",
            "Attach an IAM user that has sufficient privileges to the EKS pod.",
            "Allow outbound connectivity to the DynamoDB table through the private subnets\u2019 network ACLs.",
            "Create a VPC endpoint for DynamoDB.",
            "Embed the access keys in the Java Spring Boot code. 13"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 124\n\nA company recently migrated its web application to AWS by rehosting the application on Amazon EC2 instances in a single AWS Region. The company wants to redesign\n\nits application architecture to be highly available and fault tolerant. Traffic must reach all running EC2 instances randomly.\n\nWhich combination of steps should the company take to meet these requirements? \n",
        "options": [
            "Create an Amazon Route 53 failover routing policy.",
            "Create an Amazon Route 53 weighted routing policy.",
            "Create an Amazon Route 53 multivalue answer routing policy.",
            "Launch three EC2 instances: two instances in one Availability Zone and one instance in another Availability Zone.",
            "Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.14"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 125\n\nA media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to\ngrow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new\ndata with SQL.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon S3 bucket.",
            "Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.",
            "Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as the data arrives in the S3 bucket.",
            "Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones. Configure the service to forward data to an Amazon RDSMulti-AZ database.poe  15@"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 126\n\nA company collects data from thousands of remote devices by using a RESTful web services application that runs on an Amazon EC2 instance. The EC2 instance receives\nthe raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket. The number of remote devices will increase into the millions soon. The company\nneeds a highly scalable solution that minimizes operational overhead.\n\nWhich combination of steps should a solutions architect take to meet these requirements? \n",
        "options": [
            "Use AWS Glue to process the raw data in Amazon S3.",
            "Use Amazon Route 53 to route traffic to different EC2 instances.",
            "Add more EC2 instances to accommodate the increasing amount of incoming data.",
            "Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to process the data.",
            "Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source todeliver the data to Amazon S3. 16"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 127\n\nA company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing CloudTrail across a set of AWS accounts by using AWS Organizations from the\nparent account. The CloudTrail target S3 bucket is configured with S3 Versioning enabled. An S3 Lifecycle policy is in place to delete current objects after 3 years.\n\nAfter the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of objects has continued to rise. However, the number of new CloudTrail logs\nthat are delivered to the S3 bucket has remained consistent.\n\nWhich solution will delete objects that are older than 3 years in the MOST cost-effective manner?\n",
        "options": [
            "Configure the organization's centralized CloudTrail trail to expire objects after 3 years.",
            "Configure the S3 Lifecycle policy to delete previous versions as well as current versions.",
            "Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older than 3 years.",
            "Configure the parent account as the owner of all objects that are delivered to the S3 bucket. 17"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 128\n\nA company has an API that receives real-time data from a fleet of monitoring devices. The API stores this data in an Amazon RDS DB instance for later analysis. The\namount of data that the monitoring devices send to the API fluctuates. During periods of heavy traffic, the API often returns timeout errors.\n\nAfter an inspection of the logs, the company determines that the database is not capable of processing the volume of write traffic that comes from the API. A solutions\narchitect must minimize the number of connections to the database and must ensure that data is not lost during periods of heavy traffic.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Increase the size of the DB instance to an instance type that has more available memory.",
            "Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all active RDS DB instances.",
            "Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to writedata from the queue to the database.",
            "Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an AWS Lambda function that Amazon SNS invokes towrite data from the topic to the database. 18"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 129\n\nA company manages its own Amazon EC2 instances that run MySQL databases. The company is manually managing replication and scaling as demand increases or\ndecreases. The company needs a new solution that simplifies the process of adding or removing compute capacity to or from its database tier as needed. The solution\nalso must offer improved performance, scaling, and durability with minimal effort from operations.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.",
            "Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL.",
            "Combine the databases into one larger MySQL database. Run the larger database on larger EC2 instances.",
            "Create an EC2 Auto Scaling group for the database tier. Migrate the existing databases to the new environment. 19"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 130\n\nA company is concerned that two NAT instances in use will no longer be able to support the traffic needed for the company's application. A solutions architect wants to\n\nimplement a solution that is highly available, fault tolerant, and automatically scalable.\n\nWhat should the solutions architect recommend?\n",
        "options": [
            "Remove the two NAT instances and replace them with two NAT gateways in the same Availability Zone.",
            "Use Auto Scaling groups with Network Load Balancers for the NAT instances in different Availability Zones.",
            "Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones.",
            "Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a Network Load Balancer."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 131\n\nAn application runs on an Amazon EC2 instance that has an Elastic IP address in VPC A. The application requires access to a database in VPC B. Both VPCs are in the\nsame AWS account.\n\nWhich solution will provide the required access MOST securely?\n",
        "options": [
            "Create a DB instance security group that allows all traffic from the public IP address of the application server in VPC A.",
            "Configure a VPC peering connection between VPC A and VPC B.",
            "Make the DB instance publicly accessible. Assign a public IP address to the DB instance.",
            "Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the new EC2 instance. DS 21."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 132\n\nA company runs demonstration environments for its customers on Amazon EC2 instances. Each environment is isolated in its own VPC. The company's operations team\nneeds to be notified when RDP or SSH access to an environment has been established.\n",
        "options": [
            "Configure Amazon CloudWatch Application Insights to create AWS Systems Manager Opsitems when RDP or SSH access is detected.",
            "Configure the EC2 instances with an IAM instance profile that has an IAM role with the AmazonSSMManagedinstanceCore policy attached.",
            "Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon CloudWatch metric alarm with a notification action for whenthe alarm is in the ALARM state.",
            "Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification. Configure an Amazon Simple Notification Service(Amazon SNS) topic as a target. Subscribe the operations team to the topic. 22"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 133\n\nA solutions architect has created a new AWS account and must secure AWS account root user access.\n\nWhich combination of actions will accomplish this? \n",
        "options": [
            "Ensure the root user uses a strong password.",
            "Enable multi-factor authentication to the root user.",
            "Store root user access keys in an encrypted Amazon S3 bucket.",
            "Add the root user to a group containing administrative permissions.",
            "Apply the required permissions to the root user with an inline policy document.23"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 134\n\nAccompany is building a new web-based customer relationship management application. The application will use several Amazon EC2 instances that are backed by\nAmazon Elastic Block Store (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the\napplication must be encrypted at rest and in transit.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use AWS Certificate Manager (ACM) to encrypt the EBS volumesand Aurora database storage at rest.",
            "Use the AWS root account to log in to the AWS Management Console. Upload the company's encryption certificates. While in the root account, select the option toturn on encryption for all data at rest and in transit for the account.",
            "Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM)certificate to the ALB to encrypt data in transit.",
            "Use BitLocker to encrypt all data at rest. Import the company\u2019s TLS certificate keys to AWS Key Management Service (AWS KMS) Attach the KMS keys to the ALB toencrypt data in transit. 24"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 135\n\nA company is moving its on-premises Oracle database to Amazon Aurora PostgreSQL. The database has several applications that write to the same tables. The\n\napplications need to be migra\u2019 gy one by one with a month in between each migration. Management has expressed concerns that the database has a high number of\nreads and writes. The data must be kept in sync across both databases throughout the migration.\n\nWhat should a solutions architect recommend?\n",
        "options": [
            "Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task and a tablemapping to select all tables.",
            "Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication taskand a table mapping to select all tables.",
            "Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load pluschange data capture (CDC) replication task and a table mapping to select all tables.",
            "Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a compute optimized replication instance. Create a full load pluschange data capture (CDC) replication task and a table mapping to select the largest tables. 25"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 136\n\nAccompany has a three-tier application for image sharing. The application uses an Amazon EC2 instance for the front-end layer, another EC2 instance for the application\nlayer, and a third EC2 instance for a MySQL database. A solutions architect must design a scalable and highly available solution that requires the least amount of change\nto the application.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer. Move the database to an Amazon DynamoDB table. Use AmazonS3 to store and serve users\u2019 images.",
            "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS DBinstance with multiple read replicas to serve users\u2019 images.",
            "Use Amazon S3 to host the front-end layer. Use a fleet of EC2 instances in an Auto Scaling group for the application layer. Move the database to a memoryoptimized instance type to store and serve users\u2019 images.",
            "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZDB instance. Use Amazon S3 to store and serve users\u2019 images. 26"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 137\n\nAn application running on an Amazon EC2 instance in VPC-A needs to access files in another EC2 instance in VPC-B. Both VPCs are in separate AWS accounts. The\n\nnetwork administrator needs to design a solution to configure secure access to EC2 instance in VPC-B from VPC-A. The connectivity should not have a single point of\nfailure or bandwidth concerns.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Set up a VPC peering connection between VPC-A and VPC-B.",
            "Set up VPC gateway endpoints for the EC2 instance running in VPC-B.",
            "Attach a virtual private gateway to VPC-B and set up routing from VPC-A.",
            "Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate routes from VPC-A. 27"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 138\n\nAccompany wants to experiment with individual AWS accounts for its engineer team. The company wants to be notified as soon as the Amazon EC2 instance usage for a\ngiven month exceeds a specific threshold for each account.\n\nWhat should a solutions architect do to meet this requirement MOST cost-effectively?\n",
        "options": [
            "Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service(Amazon SES) notification when a threshold is exceeded.",
            "Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple EmailService (Amazon SES) notification when a threshold is exceeded.",
            "Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an alert threshold for the budget.Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.",
            "Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report data with Amazon Athena. Use Amazon EventBridge to schedule anAthena query. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.=  28@) @)"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 139\n\nA solutions architect needs to design a new microservice for a company\u2019s application. Clients must be able to call an HTTPS endpoint to reach the microservice. The\nmicroservice also must use AWS Identity and Access Management (IAM) to authenticate calls. The solutions architect will write the logic for this microservice by using a\n\nsingle AWS Lambda function that is written in Go 1.x.\nWhich solution will deploy the function in the MOST operationally efficient way?\n",
        "options": [
            "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function. Enable IAM authentication on the API.",
            "Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.",
            "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM authentication logic into the Lambda@Edge function.",
            "Create an Amazon CloudFront distribution. Deploy the function to CloudFront Functions. Specify AWS_IAM as the authentication type. 29"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 140\n\nA company previously migrated its data warehouse solution to AWS. The company also has an AWS Direct Connect connection. Corporate office users query the data\nwarehouse using a visualization tool. The average size of a query returned by the data warehouse is 50 MB and each webpage sent by the visualization tool is\ne approximately 500 KB. Result sets returned by the data warehouse are not cached.\n\nWhich solution provides the LOWEST data transfer egress cost for the company?\n",
        "options": [
            "Host the visualization tool on premises and query the data warehouse directly over the internet.",
            "Host the visualization tool in the same AWS Region as the data warehouse. Access it over the internet.",
            "Host the visualization tool on premises and query the data warehouse directly over a Direct Connect connection at a location in the same AWS Region.",
            "Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct Connect connection at a location in the same Region.Click to finish recording. Don't show this againPPECENTATIAN TITLE an@Qreeosgsesss 22 22\u00b0C Mostly cdouly A GORE Ae) f ON Bw\u2018Type here to search Fapa"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 141\n\nAn online learning company is migrating to the AWS Cloud. The company maintains its student records in a PostgreSQL database. The company needs a solution in which\nits data is available and online across multiple AWS Regions at all times.\n\nWhich solution will meet these requirements with the LEAST amount of operational overhead?\n",
        "options": [
            "Migrate the PostgreSQL database to a PostgreSQL cluster on Amazon EC2 instances.",
            "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance with the Multi-AZ feature turned on.",
            "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a read replica in another Region.",
            "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Set up DB snapshots to be copied to another Region. 31"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 142\n\nA company hosts its web application on AWS using seven Amazon EC2 instances. The company requires that the IP addresses of all healthy EC2 instances be returned in\nresponse to DNS queries.\n@\n\nWhich policy should be used to meet this requirement?\n",
        "options": [
            "Simple routing policy",
            "Latency routing policy",
            "Multivalue routing policy",
            "Geolocation routing policy 32:"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 143\n\nA medical research lab produces data that is related to a new study. The lab wants to make the data available with minimum latency to clinics across the country for their\non-premises, file-based applications. The data files are stored in an Amazon S3 bucket that has read-only permissions for each clinic.\n\nWhat should a solutions architect recommend to meet these requirements?\n",
        "options": [
            "Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic",
            "Migrate the files to each clinic's on-premises applications by using AWS DataSync for processing.",
            "Deploy an AWS Storage Gateway volume gateway as a virtual machine (VM) on premises at each clinic.",
            "Attach an Amazon Elastic File System (Amazon EFS) file system to each clinic's on-premises servers. 33"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 144 \n\nAccompany is using a content management system that runs on a single Amazon EC2 instance. The EC2 instance contains both the web server and the database\nsoftware. The company must make its website platform highly available and must enable the website to scale to meet user demand.\n\nWhat should a solutions architect recommend to meet these requirements?\n",
        "options": [
            "Move the database to Amazon RDS, and enable automatic backups. Manually launch another EC2 instance in the same Availability Zone. Configure an ApplicationLoad Balancer in the Availability Zone, and set the two instances as targets.",
            "Migrate the database to an Amazon Aurora instance with a read replica in the same Availability Zone as the existing EC2 instance. Manually launch another EC2instance in the same Availability Zone. Configure an Application Load Balancer, and set the two EC2 instances as targets.",
            "Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Configure anApplication Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.",
            "Move the database to a separate EC2 instance, and schedule backups to Amazon S3. Create an Amazon Machine Image (AMI) from the original EC2 instance.Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.Click to finish recording. Don't show this againDPECENTATION TITIE 2A9:28 PM6 6 xy 22\u00b0C Mostlycludy AGORA) fS wae |,\"*tH \u2018Type here to search %. % fat"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 145\n\nA company is launching an application on AWS. The application uses an Application Load Balancer (ALB) to direct traffic to at least two Amazon EC2 instances in a single\ntarget group. The instances are in an Auto Scaling group for each environment. The company requires a development environment and a production environment. The\nproduction environment will have periods of high traffic.\n\nWhich solution will configure the development environment MOST cost-effectively?\n",
        "options": [
            "Reconfigure the target group in the development environment to have only one EC2 instance as a target.",
            "Change the ALB balancing algorithm to least outstanding requests.",
            "Reduce the size of the EC2 instances in both environments.",
            "Reduce the maximum number of EC2 instances in the development environments Auto Scaling group. 35"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 146\n\nA company runs a web application on Amazon EC2 instances in multiple Availability Zones. The EC2 instances are in private subnets. A solutions architect implements an\ninternet-facing Application Load Balancer (ALB) and specifies the EC2 instances as the target group. However, the internet traffic is not reaching the EC2 instances.\n\nHow should the solutions architect reconfigure the architecture to resolve this issue?\n",
        "options": [
            "Replace the ALB with a Network Load Balancer. Configure a NAT gateway in a public subnet to allow internet traffic.",
            "Move the EC2 instances to public subnets. Add a rule to the EC2 instances\u2019 security groups to allow outbound traffic to 0.0.0.0/0.",
            "Update the route tables for the EC2 instances\u2019 subnets to send 0.0.0.0/0 traffic through the internet gateway route. Add a rule to the EC2 instances\u2019 security groupsto allow outbound traffic to 0.0.0.0/0.",
            "Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the route tables for the public subnets with a route to the privatesubnets. 36"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 147\n\nAccompany has deployed a database in Amazon RDS for MySQL. Due to increased transactions, the database support team is reporting slow reads against the DB\ninstance and recommends adding a read replica.\n\nWhich combination of actions should a solutions architect take before implementing this change? \n",
        "options": [
            "Enable binlog replication on the RDS primary node.",
            "Choose a failover priority for the source DB instance.",
            "Allow long-running transactions to complete on the source DB instance.",
            "Create a global table and specify the AWS Regions where the table will be available.",
            "Enable automatic backups on the source instance by setting the backup retention period to a value other than 0. 37"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 148\n\nA company runs analytics software on Amazon EC2 instances. The software accepts job requests from users to process data that has been uploaded to Amazon S3.\nUsers report that some submitted data is not being processed Amazon CloudWatch reveals that the EC2 instances have a consistent CPU utilization at or near 100%. The\ncompany wants to improve system performance and scale the system based on user load.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Create a copy of the instance. Place all instances behind an Application Load Balancer.",
            "Create an S3 VPC endpoint for Amazon S3. Update the software to reference the endpoint.",
            "Stop the EC2 instances. Modify the instance type to one with a more powerful CPU and more memory. Restart the instances.",
            "Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2 Auto Scaling group based on queue size. Update the software to readfrom the queue."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 149\n\nAcompany is implementing a shared storage solution for a media application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to\naccess data. The solution must be fully managed.\n\nWhich AWS solution meets these requirements?\n",
        "options": [
            "Create an AWS Storage Gateway volume gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.",
            "Create an AWS Storage Gateway tape gateway. Configure tapes to use Amazon S3. Connect the application server to the tape gateway.",
            "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.",
            "Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.39"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 150\n\nA company's security team requests that network traffic be captured in VPC Flow Logs. The logs will be frequently accessed for 90 days and then accessed intermittently.\n\nWhat should a solutions architect do to meet these requirements when configuring the logs?\n",
        "options": [
            "Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 90 days",
            "Use Amazon Kinesis as the target. Configure the Kinesis stream to always retain the logs for 90 days.",
            "Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and enable S3 Intelligent-Tiering.",
            "Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days. 40"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 151\n\nAn Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound internet access, but the EC2 instance needs the ability to\ndownload monthly security updates from an outside vendor.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway as the default route.",
            "Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.",
            "Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as thedefault route.",
            "Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure theprivate subnet route table to use the internet gateway as the default route.Click to finish recording. Don't show this again.PPCCENTATION TITLE }4 Wm cor BRAS wx2 = 11:48 PMwh 23\u00b0C Partlycoudy AGBOERR AY SL esa -3 Type here to search Me. oi e BB ~] a (o] S- ."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 152\n\nA solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time.\n\nThe files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Amazon Elastic File System (Amazon EFS)",
            "Amazon Elastic Block Store (Amazon EBS)",
            "Amazon S3 Glacier Deep Archive",
            "AWS BackupPDCCENTATION TITLE= \u2018Type here to search akClick to finish recording. Don't show this again.> s 11:52 PM.eh 23\u00b0C Partlycoudy AGOERR AY S Gea -"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 153\n\n A solutionos architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM user. Policy1: {'Version': '2012-10-17', 'Statement': [{'Effect': 'Allow', 'Action': ['iam:Get*', 'iam:List*kms:List*', 'ec2:*', 'ds:*', 'logs:Get*', 'logs:Describe*'], 'Resource': '*'}]}, Policy2: {'Version': '2012-10-17', 'Statement': [{'Effect': 'Deny', 'Action': 'ds:Delete*', 'Resource': '*'}]}",
        "options": [
            "Deleting IAM Users",
            "Deleting directories",
            "Deleting Amazon EC2 instances",
            "Deleting logs from Amazon CloudWatch Logs"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 154\n\nA company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to\nAmazon EC2 security group ingress and egress rules between the application tiers.\n\nWhat should a solutions architect do to correct this issue?\n",
        "options": [
            "Create security group rules using the instance ID as the source or destination.",
            "Create security group rules using the security group ID as the source or destination.",
            "Create security group rules using the VPC CIDR blocks as the source or destination.",
            "Create security group rules using the subnet CIDR blocks as the source or destination."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 155\n\nA company has an ecommerce checkout workflow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during\nthe checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction.\n\nHow should a solutions architect refactor this workflow to prevent the creation of multiple orders?\n",
        "options": [
            "Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis DataFirehose and process the order.",
            "Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call thepayment service, and pass in the order information.",
            "Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service topoll Amazon SNS, retrieve the message, and process the order.",
            "Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the paymentservice to retrieve the message and process the order. Delete the message from the queue."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 156\n\nA solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The solution must prevent accidental deletion of the\ndocuments and ensure that all versions of the documents are available. Users must be able to download, modify, and upload documents.\n\nWhich combination of actions should be taken to meet these requirements? \n",
        "options": [
            "Enable a read-only bucket ACL.",
            "Enable versioning on the bucket.",
            "Attach an IAM policy to the bucket.",
            "Enable MFA Delete on the bucket.",
            "Encrypt the bucket using AWS KMS."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 157\n\nA company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless\nsolution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The\nsolution must not affect the speed of EC2 instance launches.\n\nHow should the company move the data to Amazon S3 to meet these requirements?\n",
        "answers": [],
        "options": [
            "Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
            "Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
            "Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status datadirectly to Amazon S3.",
            "Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status dataand send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 158\n\nA company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket.\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the LambdaTN function for each S3 PUT event.",
            "Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function forhoi, each S3 PUT event to invoke the Spark job.",
            "Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodicallyuse Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.",
            "Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS\u2014e Lambda function for each S3 PUT event to invoke the ETL job."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 159\n\nA company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum\nperiod of 2 years. The backups must be consistent and restorable.\n\nWhich solution should a solutions architect recommend to meet these requirements?\n",
        "options": [
            "Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation.Assign the RDS DB instances to the backup plan.",
            "Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use AmazonData Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.",
            "Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.",
            "Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task tostream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 160\n\nAcompany's compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on-premises Active Directory\ncontrols access to the files and folders.\n\nThe company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure that the on-premises Active Directory groups restrict\naccess to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file\nsystem.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory groups to IAM groups to restrict access.",
            "Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to IAM groups to restrict access.",
            "Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict access.",
            "Join the file system to the Active Directory to restrict access."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 161\n\nA company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load\nBalancer. The instances run in an Auto Scaling group across multiple Availability Zones.\n\nThe company wants to provide its customers with different versions of content based on the devices that the customers use to access the website.\n\nWhich combination of actions should a solutions architect take to meet these requirements? \n",
        "options": [
            "Configure Amazon CloudFront to cache multiple versions of the content.",
            "Configure a host header in a Network Load Balancer to forward traffic to different instances.",
            "Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.",
            "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances.",
            "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances.(Click to finish recording, Don't show this again.2 4 12:16 AMA 23\u00b0C Partlycloudy AGORA BAY) Sf rape aPDECCENITATION TITHE= \u2018Type here to search ik Bi e nm ~] fia] (o} 6"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 162\n\nA company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a Cache VPC for the ElastiCache cluster and an App VPC for\nthe application's Amazon EC2 instances. Both VPCs are in the us-east-1 Region.\n\nThe solutions architect must implement a solution to provide the application's EC2 instances with access to the ElastiCache cluster.\n\nWhich solution will meet these requirements MOST cost-effectively?\n",
        "options": [
            "Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCachecluster's security group to allow inbound connection from the application's security group.",
            "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for theElastiCache cluster's security group to allow inbound connection from the application's security group.",
            "Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the peeringconnection's security group to allow inbound connection from the application's security group.",
            "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for theTransit VPC's security group to allow inbound connection from the application's security group."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 163\n\nAcompany is building an application that consists of several microservices. The company has decided to use container technologies to deploy its software on AWS. The\ncompany needs a solution that minimizes the amount of ongoing effort for maintenance and scaling. The company cannot manage additional infrastructure.\n\nWhich combination of actions should a solutions architect take to meet these requirements? \n",
        "options": [
            "Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.",
            "Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability Zones.",
            "Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch type. Specify a desired task number level of greater than or equalto2.",
            "Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.",
            "Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability Zones. Create a deployment that specifies two or more replicas for each microservice."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 164\n\nAccompany has a web application hosted over 10 Amazon EC2 instances with traffic directed by Amazon Route 53. The company occasionally experiences a timeout error\n\u2014\u2014\u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014\u2014\nwhen attempting to browse the application. The networking team finds that some DNS queries return IP addresses of unhealthy insta.ls, resulting in the timeout error.\n\nWhat should a solutions architect implement to overcome these timeout errors?\n",
        "options": [
            "Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.",
            "Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.",
            "Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.",
            "Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 165\n\nA solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the\nedge as possible, with the least delivery time.\n\nWhich solution meets these requirements and is MOST secure?\n",
        "options": [
            "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliverHTTPS content using the public ALB as the origin.",
            "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPScontent using the EC2 instances as the origin.",
            "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliverHTTPS content using the public ALB as the origin.",
            "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPScontent using the EC2 instances as the origin."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 166\n\nA company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair\nadvantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind\nApplication Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy\nendpoints.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region.Add the ALB as the endpoint.",
            "Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambdafunctions to optimize the traffic.",
            "Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambdafunctions to optimize the traffic.",
            "Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memorycache for DynamoDB hosting the application data."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 167\n\nA company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-\nreal time and must store the data in a centralized location in Apache Parquet format for further processing.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWSLambda function to send the data to the Kinesis Data Analytics application.",
            "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function tosend the data to the EMR cluster.",
            "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.",
            "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 168\n\nA gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application\nstores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The\ncompany wants to improve the user experience while minimizing changes to the application's architecture.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Use Amazon ElastiCache in front of the database.",
            "Use RDS Proxy between the application and the database.",
            "Migrate the application from EC2 instances to AWS Lambda.",
            "Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.(Click to finish recording. Don't show this again.PPCCENTATION TITHE |4 Mm won BRAS wx\u20143 Type here to search e i] ~] [| (o} S6 | iy(Ge) Heayrainsoon AGQREBMBE?) Sf ai Canee \u00bba"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 169\n\nAn ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance degradation is attributed to an increase in the\nnumber of read-only SQL queries triggered by business analysts. A solutions architect needs to solve the problem with minimal changes to the existing web application.\n\nWhat should the solutions architect recommend?\n",
        "options": [
            "Export the data to Amazon DynamoDB and have the business analysts run their queries.",
            "Load the data into Amazon ElastiCache and have the business analysts run their queries.",
            "Create a read replica of the primary database and have the business analysts run their queries.",
            "Copy the data into an Amazon Redshift cluster and have the business analysts run their queries."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 170\n\nAccompany is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions architect needs to ensure that the data is encrypted at rest\nbefore the data is uploaded to the S3 buckets. The data also must be encrypted in transit.\n\nWhich solution meets these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
            "Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
            "Create bucket policies that require the use of server-side encryption with S3 managed encryption keys (SSE-S3) for S3 uploads.",
            "Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management Service (AWS KMS) key."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 171\n\nA solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon EC2 capacity is reached. The peak\ncapacity is the \u2018same everyggnht and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the desired EC2\ncapacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete.\n\nWhat should the solutions architect do to meet these requirements?\n",
        "options": [
            "Increase the minimum capacity for the Auto Scaling group.",
            "Increase the maximum capacity for the Auto Scaling group.",
            "Configure scheduled scaling to scale up to the desired compute level.",
            "Change the scaling policy to add more EC2 instances during each scaling operation."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 172\n\nA company serves a dynamic website from a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages\nto serve customers around the world. The website's architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in\nother parts of the world.\n\nThe website needs to serve requests quickly and efficiently regardless of a user's location. However, the company does not want to recreate the existing architecture\nacross multiple Regions.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as theorigin. Set the cache behavior settings to cache based on the Accept-Language request header.",
            "Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.",
            "Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable theAPI cache based on the Accept-Language request header.",
            "Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind anAmazon Route 53 record set with a geolocation routing policy."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 173\n\nA rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must create a disaster recovery (DR) strategy that includes\na different AWS Region. The company wants its database to be up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region\nneeds to run at reduced capacity and must be able to scale up if necessary.\n\nWhich solution will meet these requirements with the LOWEST recovery time objective (RTO)?\n",
        "options": [
            "Use an Amazon Aurora global database with a pilot light deployment.",
            "Use an Amazon Aurora global database with a warm standby deployment.",
            "Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment.",
            "Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 174\n\nA company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery (DR) solution for the application. The DR solution needs\nto have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?\n",
        "options": [
            "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in thesecondary Region by using AWS Lambda and custom scripts.",
            "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in thesecondary Region by using AWS CloudFormation.",
            "Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.",
            "Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 175\n\nA company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an\nAmazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances\novernight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning.\n\nHow should the scaling be changed to address the staff complaints and keep costs to a minimum?\n",
        "options": [
            "Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.",
            "Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.",
            "Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.",
            "Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 176\n\nA company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application has data\nlayer that uses Oracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the\nRDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company\npredicts that traffic will continue to increase at a steady but unpredictable rate before leveling off.\n\nWhat should a solutions architect do to ensure the system can automatically scale for the increased traffic? \n",
        "options": [
            "Configure storage Auto Scaling on the RDS for Oracle instance.",
            "Migrate the database to Amazon Aurora to use Auto Scaling storage.",
            "Configure an alarm on the RDS for Oracle instance for low free storage space.",
            "Configure the Auto Scaling group to use the average CPU as the scaling metric.",
            "Configure the Auto Scaling group to use the average free memory as the scaling metric."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 177\n\nA company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File\nSystem (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the\npopularity of the service has grown over time, the storage costs have become too expensive.\n\nWhich storage solution is MOST cost-effective?\n",
        "options": [
            "Use AWS Storage Gateway for files to store and process the video content.",
            "Use AWS Storage Gateway for volumes to store and process the video content.",
            "Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).",
            "Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server forprocessing."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 178\n\nAccompany wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-traffic\nqueries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any financial information is present in\nthe employee data.\n\nWhich combination of steps should a solutions architect take to meet these requirements? \n",
        "options": [
            "Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.",
            "Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.",
            "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.",
            "Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share thedashboards with users.",
            "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple NotificationService (Amazon SNS) subscription."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 179\n\nA company has an application that is backed by an Amazon DynamoDB table. The company's compliance requirements specify that database backups must be taken\nevery month, must be available for 6 months, and must be retained for 7 years.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storageafter 6 months. Set the retention period for each backup to 7 years.",
            "Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.",
            "Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the firstday of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to coldstorage and to delete backups that are older than 7 years.",
            "Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each monthwith a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 180\n\nA company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company's\nAmazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.",
            "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.",
            "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.",
            "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 181\n\nAccompany runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine compliance check, the company sets a standard that requires a\nTecovery point objective (RPO) of less than 1 second for all its production databases.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Enable a Multi-AZ deployment for the DB instance.",
            "Enable auto scaling for the DB instance in one Availability Zone.",
            "Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.",
            "Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks.Click to finish recording. Don't show this again,ao ; = 7\u201d F 7ATPMGPC Heayrin AGORA AY\u201d A ieon,PPCCENTATION TITLE3 Type here to search YB 9a' @e\u00ae & eos"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 182\n\nA company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the\npublic subnets directs web traffic to the EC2 instances. The company wants to implement new security measures to restrict inbound traffic from the ALB to the EC2\ninstances while preventing access from any other source inside or outside the private subnet of the EC2 instances.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Configure a route in a route table to direct traffic from the internet to the private IP addresses of the EC2 instances.",
            "Configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB.",
            "Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.",
            "Configure the security group for the ALB to allow any TCP traffic on any port."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 183\n\nA research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs\nintermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an\nSMB file system.\n\nThe company maintains two synchronized file systems. This strategy is causing data duplication and inefficient resource usage. The company needs to migrate the\napplications to AWS without making code changes to either application.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.",
            "Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.",
            "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon SimpleQueue Service (Amazon SQS) to exchange data between the applications.",
            "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx forNetApp ONTAP for storage."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 184\n\nAs part of budget planning, management wants a report of AWS billed items listed by user. The data will be used to create department budgets. A solutions architect\nneeds to determine the most efficient way to obtain this report information.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Run a query with Amazon Athena to generate the report.",
            "Create a report in Cost Explorer and download the report.",
            "Access the bill details from the billing dashboard and download the bill.",
            "Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES)."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 185\n\nA company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side\ncomponents for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each\n\nmonth.\n\nWhich solution will meet these requirements MOST cost-effectively?\n",
        "options": [
            "Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.",
            "Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).",
            "Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with AmazonWorkMail.",
            "Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client-side scripting to build thecontact form. Integrate the form with Amazon WorkMail."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 186\n\nA company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website usecggyataba 1 company notices that the\npebell Saat\nwebsite does not reflect updates that have been made in the website's Git repository. The company checks the continuous integration and continuous delivery (CI/CD)\n\ncca ip\npipeline between the Git repository and Amazon S3. The company verifies that the webhooks are configured properly and that the CI/CD pipeline is sending messages that\nindicate successful deployments.\n\nA solutions architect needs to implement a solution that displays the updates on the website.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Add an Application Load Balancer.",
            "Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.",
            "Invalidate the CloudFront cache.",
            "Use AWS Certificate Manager (ACM) to validate the website's SSL certificate."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 187\n\nAccompany wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a\ndatabase tier with Microsoft SQL Server. The company wants to use specific features of SQL Server such as native backups and Data Quality Services. The company also\nneeds to share files for processing between the tiers.\n\nHow should a solutions architect design the architecture to meet these requirements?\n",
        "options": [
            "Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.",
            "Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.",
            "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS)for file sharing between the tiers.",
            "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (i02) Amazon ElasticBlock Store (Amazon EBS) volume for file sharing between the tiers."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 188\n\nA company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make\nany changes to the application.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "options": [
            "Create an Amazon S3 Standard bucket with access to the web servers.",
            "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.",
            "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.",
            "Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 189\n\nA company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account.\n\nWhich solution will meet these requirements in the MOST secure manner?\n",
        "options": [
            "Apply an S3 bucket policy that grants read access to the S3 bucket.",
            "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.",
            "Embed an access key and a secret key in the Lambda function's code to grant the required IAM permissions for read access to the S3 bucket.",
            "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all S3 buckets in the account."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 190\n\nA company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are in an Auto Scaling group that scales in response to user demand. The\ncompany wants to optimize cost savings without making a long-term commitment.\n\nWhich EC2 instance purchasing option should a solutions architect recommend to meet these requirements?\n",
        "options": [
            "Dedicated Instances only",
            "On-Demand Instances only",
            "A mix of On-Demand Instances and Spot Instances",
            "A mix of On-Demand Instances and Reserved Instances"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 191\n\nA media company uses Amazon CloudFront for its publicly available streaming video content. The company wants to secure the video content that is hosted in Amazon\nS3 by controlling who has access. Some of the company\u2019s users are using a custom HTTP client that does not support cookies. Some of the company\u2019s users are unable\n\nto change the hardcoded URLs that they are using for access.\n\nWhich services or methods will meet these requirements with the LEAST impact to the users? \n",
        "options": [
            "Signed cookies",
            "Signed URLs",
            "AWS AppSync",
            "JSON Web Token (JWT)",
            "AWS Secrets Manager(Click to finish recording. Don't show this again.oo. : _ Z ; 8:20 PMGP 26C Heayrin AGORA BAY HF oppo, FbPPECCENTATION TITIE= Type here to search Ae Bi e rr fa Ga \u00a9 6"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 192\n\nA company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the\ndata to Amazon S3. The company needs the ability to use SQL to query the transformed data.\n\nWhich solutions will meet these requirements? \n",
        "options": [
            "Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write thedata to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
            "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3.Use Amazon Athena to query the transformed data from Amazon S3.",
            "Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use AmazonAthena to query the transformed data from Amazon S3.",
            "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write thedata to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.",
            "Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Usethe Amazon RDS query editor to query the transformed data from Amazon S3."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 193\n\nA company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to\n\nmaintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely\ntransferred.\n\nWhich solution meets these requirements?\n",
        "options": [
            "Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises systems to mount the Snowball S3 endpoint to providelocal access to the data.",
            "Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on-premises systems withlocal access to the data.",
            "Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of datato cache locally. Mount the gateway storage volumes to provide local access to the data.",
            "Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storagevolumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 194\n\nAn application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Traffic must not traverse the internet.\n\nHow should a solutions architect configure access to meet these requirements?\n",
        "options": [
            "Create a private hosted zone by using Amazon Route 53.",
            "Set up a gateway VPC endpoint for Amazon S3 in the VPC.",
            "Configure the EC2 instances to use a NAT gateway to access the S3 bucket.",
            "Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 195\n\nAn ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the\ndata in three applications. Only one of the applications needs to process the Pll. The Pll must be removed before the other two applications process the data.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "options": [
            "Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.",
            "Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.",
            "Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each applicationto its respective S3 bucket.",
            "Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point eachapplication to its respective DynamoDB table."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 196\n\nA development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC\nin the same account. The new VPC will b-\u2018peered with the development VPC. The VPC CIDR block for the development VPC is 192.168.0.0/24. The solutions architect\nneeds to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC peering connection to the development VPC.\n\nWhat is the SMALLEST CIDR block that meets these requirements?\n",
        "answers": [],
        "options": [
            "10.0.1.0/32",
            "192.168.0.0/24",
            "192.168.1.0/32",
            "10.0.1.0/24"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 197\n\nA company deploys an application on five Amazon EC2 instances. An Application Load Balancer (ALB) distributes traffic to the instances by using a target group. The\naverage CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%.\n\nA solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must\nensure that the application has enough CPU resources when surges occur.\n\nWhich solution will meet these requirements?\n",
        "options": [
            "Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that theCloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.",
            "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scalingpolicy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the targetvalue to 50%. Add the EC2 instances to the Auto Scaling group.",
            "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2,the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.",
            "Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%.Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to anAmazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2instances that are running."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 198\n\nA company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and\naccess an Amazon RDS DB instance.\n\nThe design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must\nupdate the design to use a second Availability Zone.\n\nwi, solution will make the application highly available?\n",
        "options": [
            "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DBinstance with connections to each network.",
            "Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones.Configure the DB instance with connections to each network.",
            "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DBinstance for Multi-AZ deployment.",
            "Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones.Configure the DB instance for Multi-AZ deployment."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 199\n\nA research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond latencies and a minimum throughput of 6 GBps for the\nstorage subsystem. Hundreds of Amazon EC2 instances that rfPAmazon Linux will distribute and process the data.\n\nWhich solution will meet the performance requirements?\n",
        "options": [
            "Create an Amazon FSx for NetApp ONTAP file system. Sat each volume\u2019 tiering policy to ALL. Import the raw data into the file system. Mount the fila system on theEC2 instances.",
            "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import datafrom and export data to Amazon S3. Mount the file system on the EC2 instances.",
            "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import datafrom and export data to Amazon S3. Mount the file system on the EC2 instances.",
            "Create an Amazon FSx for NetApp ONTAP file system. Set each volume's tiering policy to NONE. Import the raw data into the file system. Mount the file system onthe EC2 instances.(Click to finish recording, Don't show this again.a 8:41 PMQ) 26\u00b0C Heayran AGORA) samen CyPDCCENTATION TITLE3 Type here to search  pael oi e ry ~~ J \u00a9 6"
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 200\n\nA company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24\nhours a day, 7 days a week. The application's database storage continues to grow over time.\n\nWhat should a solutions architect do to meet these requirements MOST cost-effectively?\n",
        "options": [
            "Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.",
            "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.",
            "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.",
            "Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    }
]
