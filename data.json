[
    {
        "question": "Question 1\n\nA company collects data for temperature, humidity, and atmospheric pressure in cities across multiple\ncontinents. The average volume of data that the company collects from each site daily is 500 GB. Each site\nhas a high-speed Internet connection.\n\nThe company wants to aggregate the data from all these global sites as quickly as possible in a single Amazon\nS3 bucket. The solution must minimize operational complexity.\n\nWhich solution meets these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket.",
            "Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket. Then remove the data from the origin S3 bucket.",
            "Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site to the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket.",
            "Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic BlockStore (Amazon EBS) volume. At regular intervals, take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. Restore the EBS volume in that Region."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question 2\n\nA company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON\nformat in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs\nto perform the analysis with minimal changes to the existing architecture.\n\nWhat should the solutions architect do to meet these requirements with the LEAST amount of operational\noverhead?\n",
        "answers": [
            2
        ],
        "options": [
            "Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.",
            "Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon CloudWatch console.",
            "Use Amazon Athena directly with Amazon S3 to run the queries as needed.",
            "Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question 3\n\nA company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains\nproject reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.\nWhich solution meets these requirements with the LEAST amount of operational overhead?\n",
        "answers": [
            0
        ],
        "options": [
            "Add the aws PrincipalOrg!D global condition key with a reference to the organization ID to the S3 bucket policy.",
            "Create an organizational unit 7 for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.",
            "Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3bucket policy accordingly.",
            "Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0,
        "tags": [],
        "attempt_history": []
    },
    {
        "question": "Question 4\n\nAn application runs on an Amazon EC2 instance in a VPC. The application processes logs that are stored in an Amazon S3 bucket. The EC2 instance needs to access the\nS3 bucket without connectivity to the internet. *\nWhich solution will provide private network connectivity to Amazon S3?\n",
        "answers": [
            0
        ],
        "options": [
            "Create a gateway VPC endpoint to the S3 bucket.",
            "Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.",
            "Create an instance profile on Amazon EC2 to allow S3 access.",
            "Create an Amazon API Gateway API with a private link to access the S3 endpoint."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 5\n\nA company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon EBS volume. For better\nscalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind\nan Application Load Balancer. After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or\nthe other, but never all of the documents at the same time.\n\nWhat should a solutions architect propose to ensure users see all of their documents at once?\n",
        "answers": [
            2
        ],
        "options": [
            "Copy the data so both EBS volumes contain all the documents",
            "Configure the Application Load Balancer to direct a user to the server with the documents",
            "Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS",
            "Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 6\n\nA company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in size from 1 MB to 500 GB. The total storage is 70 TB\nand is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as soon as possible while using the\nleast possible network bandwidth.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the AWS CLI to copy all files locally to the S3 bucket.",
            "Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device sothat AWS can import the data into Amazon S3.",
            "Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share onthe S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.",
            "Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3 File Gateway on premises. Create a public virtual interface(VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transferthe data from the existing NFS file share to the S3 File Gateway."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 7\n\nA company has an application that ingests incoming messages. Dozens of other applications and microservices then quickly consume these messages. The number of\nmessages varies drastically and sometimes increases suddenly to 100,000 each second. The Company wants to decouple the solution and increase scalability.\nWhich solution meets these requirements?\n",
        "options": [
            "Persist the messages to Amazon Kinesis Data Analytics. Configure the consumer applications to read and process the messages.",
            "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale the number of EC2 instances based on CPU metrics.",
            "Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to preprocess messages and store them in AmazonDynamoDB. Configure the consumer applications to read from DynamoDB to process the messages.",
            "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SQS) subscriptions.Configure the consumer applications to process the messages from the queues."
        ],
        "answers": [
            3
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 8\n\nAcompany is migrating a distributed application to AWS. The application serves variable workloads. The legacy platform consists of a primary server that coordinates\njobs across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resiliency and scalability.\nHow should a solutions architect design the architecture to meet these requirements?\n",
        "options": [
            "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that aremanaged in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.",
            "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that aremanaged in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue.",
            "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure AWS CloudTrail as adestination for the jobs. Configure EC2 Auto Scaling based on the load on the primary server.",
            "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure Amazon EventBridge(Amazon CloudWatch Events) as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes."
        ],
        "answers": [],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 9\n\n A company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed.  The total data size is increasing and is close to the company\u2019s total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.  Which solution will meet these requirements ? ",
        "answers": [
            1
        ],
        "options": [
            "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
            "Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7days.",
            "Create an Amazon FSx for Windows File Server file system to extend the company's storage space.",
            "Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 0,
        "current_probability": 1.0,
        "tags": [
            "On prem data"
        ]
    },
    {
        "question": "Question 10\n\n A company is building an ecommerce web application on AWS. The application sends information about new orders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are processed in the order that they are received.  Which solution will meet these requirements? ",
        "answers": [
            1
        ],
        "options": [
            "Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the application receives an order.Subscribe an AWS Lambda function to the topic to perform processing.",
            "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order.Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.",
            "Use an API Gateway authorizer to block any requests while the application processes an order.",
            "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the application receives an order.Configure the SQS standard queue to invoke an AWS Lambda function for processing."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 11\n\n A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The EC2 instances connect to the database by using user names and passwords that are stored locally in a file. The company wants to minimize the operational overhead of credential management. What should a solutions architect do to accomplish this goal? ",
        "answers": [
            0
        ],
        "options": [
            "Use AWS Secrets Manager. Turn on automatic rotation.",
            "Use AWS Systems Manager Parameter Store. Turn on automatic rotation.",
            "Create an Amazon S3 bucket to store objects that are encrypted with an AWS Key Management Service (AWS KMS) encryption key. Migrate the credential file to theS3 bucket. Point the application to the S3 bucket.",
            "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume for each EC2 instance. Attach the new EBS volume to each EC2 instance. Migrate thecredential file to the new EBS volume. Point the application to the new EBS volume."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 12\n\nA global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has static data and dynamic data.\nThe company stores its static data in an Amazon S3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic data. The\ncompany is using its own domain name registered with Amazon Route 53.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure Route 53 to route traffic to the CloudFront distribution.",
            "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as anendpoint Configure Rute 53 to route traffic to the CloudFront distribution.",
            "Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and theCloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for theweb application.",
            "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as anendpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNSname for static content. Use the domain names as endpoints for the web application."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 13\n\nA company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the company needs to rotate the credentials for its Amazon\nRDS for MySQL databases across multiple AWS Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            0
        ],
        "options": [
            "Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the required Regions. Configure Secrets Manager to rotate thesecrets on a schedule.",
            "Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter. Use multi-Region secret replication for the required Regions.Configure Systems Manager to rotate the secrets on a schedule.",
            "Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke an AWS Lambda function to rotate the credentials.",
            "Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Store the secrets in an AmazonDynamoDB global table. Use an AWS Lambda function to retrieve the secrets from DynamoDB. Use the RDS API to rotate the secrets."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 14\n\nAcompany runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group\nacross multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL\n8.0 database that is hosted on a large EC2 instance.\n\nThe database's performance degrades quickly as application load increases. The application handles more read requests than write transactions. The company wants a\nsolution that will automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Use Amazon Redshift with a single node for leader and compute functionality.",
            "Use Amazon RDS with a Single-AZ deployment Configure Amazon RDS to add reader instances in a different Availability Zone.",
            "Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas.",
            "Use Amazon ElastiCache for Memcached with EC2 Spot Instances."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 15\n\nA company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in and out of the production VPC. The company had an inspection\nserver in its on-premises data center. The inspection server performed specific operations such as traffic flow inspection and traffic filtering. The company wants to have\nthe same functionalities in the AWS Cloud.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC.",
            "Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.",
            "Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC.",
            "Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for the production VPC."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 16\n\nAccompany hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that\nprovides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations.\nThe rest of the company should have only limited access.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.",
            "Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.",
            "Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reportsto Amazon S3. Use S3 bucket policies to limit access to the reports.",
            "Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL.Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 17\n\nA company is implementing a new business application. The application runs on two Amazon EC2 instances and uses an Amazon S3 bucket for document storage. A\nsolutions architect needs to ensure that the EC2 instances can access the S3 bucket.\nWhat should the solutions architect do to meet this requirement?\n",
        "answers": [
            0
        ],
        "options": [
            "Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.",
            "Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.",
            "Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.",
            "Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 18\n\nAn application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the\nweb interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in\nits compressed form in a different S3 bucket.\n\nA solutions architect needs to design a solution that uses durable, stateless components to process the images automatically.\n\nWhich combination of actions will meet these requirements? \n",
        "answers": [
            0,
            1
        ],
        "options": [
            "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to theS3 bucket.",
            "Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfullyprocessed, delete the message in the queue.",
            "Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, write the file name to a text file in memory and usethe text file to keep track of the images that were processed.",
            "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a textfile on the EC2 instance and invoke the Lambda function.",
            "Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon ampleNotification Service (Amazon SNS) topic with the application owner's email address for further processing.(> (PD))"
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question 19\n\nA company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database\nservers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC.\nThe appliance is configured with an IP interface that can accept IP packets.\n\nA solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            3
        ],
        "options": [
            "Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.",
            "Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.",
            "Deploy a transit gateway in the inspection VPConfigure route tables to route the incoming packets through the transit gateway.",
            "Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to theappliance."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 20\n\nAccompany wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2\ninstances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that\naccesses this data requires consistently high I/O performance.\n\nA solutions architect needs to minimize the time that is required to clone the production data into the test environment.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment.",
            "Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumesto the EC2 instances in the test environment.",
            "Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environmentbefore restoring the volumes from the production EBS snapshots.",
            "Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 21\n\nAn ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one product on sale for a period of 24 hours. The company\nwants to be able to handle millions of requests each hour with millisecond latency during peak hours.\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store the order data in Amazon S3.",
            "Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple Availability Zones. Add an Application Load Balancer (ALB) todistribute the website traffic. Add another ALB for the backend APIs. Store the data in Amazon RDS for MySQL.",
            "Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscalerto increase and decrease the number of pods to process bursts in traffic. Store the data in Amazon RDS for MySQL.",
            "Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 22\n\n A solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. The media files must be resilient to the loss of an Availability Zone. Some files are accessed frequently while other files are rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of storing and retrieving the media files.  Which storage option meets these requirements? ",
        "answers": [
            1
        ],
        "options": [
            "S3 Standard",
            "S3 Intelligent-Tiering",
            "S3 Standard-Infrequent Access (S3 Standard-IA)",
            "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 23\n\nAcompany is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for 1 month. However, the files are not accessed after 1\nmonth. The company must keep the files indefinitely.\nWhich storage solution will meet these requirements MOST cost-effectively?\n",
        "answers": [
            1
        ],
        "options": [
            "Configure S3 Intelligent-Tiering to automatically migrate objects.",
            "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.",
            "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month.",
            "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 24\n\nA company observes an increase in Amazon EC2 costs in its most recent bill. The billing team notices unwanted vertical scaling of instance types for a couple of EC2\ninstances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the\nvertical ecaling.\n\nHow tifa the solutions architect generate the information with the LEAST operational overhead?\n",
        "answers": [
            1
        ],
        "options": [
            "Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.",
            "Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.",
            "Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.",
            "Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate aninteractive graph based on instance types."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 25\n\n Accompany is designing an application. The application uses an AWS Lambda function to receive information through Amazon API Gateway and to store the information in an Amazon Aurora PostegreSQL database.  During the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the high volumes of data that the company needs to load into the database. A solutions architect must recommend a new design to improve scalability and minimize the configuration effort.  Which solution will meet these requirements? ",
        "answers": [
            3
        ],
        "options": [
            "Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity(JDBC) drivers.",
            "Change the platform from Aurora to Amazon DynamoDProvision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster.",
            "Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integratethe Lambda functions by using Amazon Simple Notification Service (Amazon SNS).",
            "Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integratethe Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 26\n\n A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have unauthorized configuration changes. What should a solutions architect do to accomplish this goal?",
        "answers": [
            0
        ],
        "options": [
            "Turn on AWS Config with the appropriate rules.",
            "Turn on AWS Trusted Advisor with the appropriate checks.",
            "Turn on Amazon Inspector with the appropriate assessment template.",
            "Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch Events)."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 27\n\nA company is launching a new application and will display application metrics on an Amazon CloudWatch dashboard. The company's product manager needs to access\nthis dashboard periodically. The product manager does not have an AWS account. A solutions architect must provide access to the product manager by following the\nprinciple of least privilege.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.",
            "Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentialswith the product manager. Share the browser URL of the correct dashboard with the product manager.",
            "Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the IAM user. Share the new login credentials with theproduct manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section.",
            "Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On thebastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view thedashboard."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 28\n\nA company is migrating applications to AWS. The applications are deployed in different accounts. The company manages the accounts centrally by using AWS\nordlizations. The company\u2019s security team needs a single sign-on (SSO) solution across all the company's accounts. The company must continue managing the users\nand groups in its on-premises self-managed Microsoft Active Directory.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or a one-way domain trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.",
            "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to connect the company\u2019s self-managed Microsoft ActiveDirectory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.",
            "Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory.",
            "Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 29\n\n A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service consists of Amazon EC2 instances that run in an Auto Scaling group. The company has deployments across multiple AWS Regions.  The company needs to route users to the Region with the lowest latency. The company also needs automated failover between Regions.  Which solution will meet these requirements? ",
        "answers": [
            0
        ],
        "options": [
            "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Use the NLB as an AWS GlobalAccelerator endpoint in each Region.",
            "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Use the ALB as an AWS GlobalAccelerator endpoint in each Region.",
            "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB. Create an Amazon CloudFront distribution that uses the latency record as an origin.",
            "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each ALB. Deploy an Amazon CloudFront distribution that uses the weighted record as an origin."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 30\n\nA development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled. The testing\nlasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and\nmemory attributes of the DB instance.\n\nWhich solution meets these requirements MOST cost-effectively?\n",
        "answers": [
            2
        ],
        "options": [
            "Stop the DB instance when tests are completed. Restart the DB instance when required.",
            "Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.",
            "Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.",
            "Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again when required."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 31\n\nAcompany that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS DB instances. and Amazon Redshift clusters are configured\nwith tags. The company wants to minimize the effort of configuring and operating this check.\nWhat should a solutions architect do to accomplish this?\n",
        "answers": [
            0
        ],
        "options": [
            "Use AWS Config rules to define and detect resources that are not properly tagged.",
            "Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.",
            "Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 instance.",
            "Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question 32\n\nA development team needs to host a website that will be accessed by other teams. The website contents consist of HTML, CSS, client-side JavaScript, and images.\nWhich method is the MOST cost-effective for hosting the website?\n",
        "answers": [
            1
        ],
        "options": [
            "Containerize the website and host it in AWS Fargate.",
            "Create an Amazon S3 bucket and host the website there.",
            "Deploy a web server on an Amazon EC2 instance to host the website.",
            "Configure an Application Load Balancer with an AWS Lambda target that uses the Express.js framework."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 33\n\nA company runs an online marketplace web application on AWS. The application serves hundreds of thousands of users during peak hours. The company needs a\nscalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed\nto remove sensitive data before being stored in a document database for low-latency retrieval.\n\nWhat should a solutions architect recommend to meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon write. Use DynamoDBStreams to share the transactions data with other applications.",
            "Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with KinesisData Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.",
            "Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store thetransactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.",
            "Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in AmazonS3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 34\n\nA company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS\nresources and record a history of API calls made to these resources.\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
            "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
            "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.",
            "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 35\n\nA company is preparing to launch a public-facing web application in the AWS Cloud. The architecture consists of Amazon EC2 instances within a VPC behind an Elastic\nLoad Balancer (ELB). A third-party service is used for the DNS. The company's solutions architect must recommend a solution to detect and protect against large-scale\nDDoS attacks.\n\nWhich solution meets these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Enable Amazon GuardDuty on the account.",
            "Enable Amazon Inspector on the EC2 instances.",
            "Enable AWS Shield and assign Amazon Route 53 to it.",
            "Enable AWS Shield Advanced and assign the ELB to it."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 36\n\nA company is building an application in the AWS Cloud. The application will store data in Amazon S3 buckets in two AWS Regions. The company must use an AWS Key\nManagement Service (AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be encrypted and\ndecrypted with the same KMS key. The data and the key must be stored in each of the two Regions.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            1
        ],
        "options": [
            "Create an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configurereplication between the S3 buckets.",
            "Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure replication between the S3 buckets. Configure the application to use the KMS key with client-side encryption.",
            "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managedencryption keys (SSE-S3). Configure replication between the S3 buckets.",
            "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with AWS KMS keys (SSE-KMS).Configure replication between the S3 buckets."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question 37\n\nA company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer\n\nthe instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected\nFramework.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            1
        ],
        "options": [
            "Use the EC2 serial console to directly access the terminal interface of each instance for administration.",
            "Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.",
            "Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration ofeach instance.",
            "Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect directly to the instances by using SSH keys across the VPN tunnel."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 38\n\nA company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. The website is experiencing increased demand from around the world. The\ncompany must decrease latency for users who access the website.\nWhich solution meets these requirements MOST cost-effectively?\n",
        "answers": [
            2
        ],
        "options": [
            "Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation routing entries.",
            "Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3 bucket. Edit the Route 53 entries to point to the IP addresses ofthe accelerators.",
            "Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to the CloudFront distribution.",
            "Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question 39\n\n A company maintains a searchable repository of items on its website. The data is stored in an Amazon RDS for MySQL database table that contains more than 10 million rows. The database has 2 TB of General Purpose SSD storage. There are millions of updates against this data every day through the company's website.  The company noticed that some insert operations are taking 10 seconds or longer. The company has determined that the database storage performance is the problem.  Which solution addresses this performance issue? ",
        "answers": [
            0
        ],
        "options": [
            "Change the storage type to Provisioned IOPS SSD.",
            "Change the DB instance to a memory optimized instance class.",
            "Change the DB instance to a burstable performance instance class.",
            "Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 40\n\nA company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs\nto implement a solution to ingest and store the alerts for future analysis.\n\nThe company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the\ncompany wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days.\n\nWhat is the MOST operationally efficient solution that meets these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.",
            "Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.",
            "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an AmazonOpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster to take manualsnapshots every day and delete data from the cluster that is older than 14 days.",
            "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts, and set the message retention period to 14 days. Configure consumersto poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy themessage to an Amazon S3 bucket and delete the message from the SQS queue."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 41\n\nA company's application integrates with multiple software-as-a-service (SaaS) sources for data collection. The company runs Amazon EC2 instances to receive the data\nand to upload the data to an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when an\nupload is complete. The company has noticed slow application performance and wants to improve the performance as much as possible.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            1
        ],
        "options": [
            "Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event notification to send events to an Amazon Simple Notification Service(Amazon SNS) topic when the upload to the S3 bucket is complete.",
            "Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket. Configure an S3 event notification to send events to an AmazonSimple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to send output data. Configure the S3 bucket as the rule's target. Create asecond EventBridge (Cloud Watch Events) rule to send events when the upload to the S3 bucket is complete. Configure an Amazon Simple Notification Service(Amazon SNS) topic as the second rule's target.",
            "Create a Docker container to use instead of an EC2 instance. Host the containerized application on Amazon Elastic Container Service (Amazon ECS). ConfigureAmazon CloudWatch Container Insights to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0,
        "tags": [
            "AppFlow"
        ]
    },
    {
        "question": "Question 42\n\nA company runs a highly available image-processing application on Amazon EC2 instances in a single VPC. The EC2 instances run inside several subnets across multiple\nAvailability Zones. The EC2 instances do not communicate with each other. However, the EC2 instances download images from Amazon S3 and upload images to Amazon\nS3 through a single NAT gateway. The company is concerned about data transfer charges.\n\nWhat is the MOST cost-effective way for the company to avoid Regional data transfer charges?\n",
        "answers": [
            2
        ],
        "options": [
            "Launch the NAT gateway in each Availability Zone.",
            "Replace the NAT gateway with a NAT instance.",
            "Deploy a gateway VPC endpoint for Amazon S3.",
            "Provision an EC2 Dedicated Host to run the EC2 instances."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 43\n\nAccompany has an on-premises application that generates a large amount of time-sensitive data that is backed up to Amazon S3. The application has grown and there are\nuser complaints about internet bandwidth limitations. A solutions architect needs to design a long-term solution that allows for both timely backups to Amazon S3 and\nwith minimal impact on internet connectivity for internal users.\n\nWhich solution meets these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint.",
            "Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.",
            "Order daily AWS Snowball devices. Load the data onto the Snowball devices and return the devices to AWS each day.",
            "Submit a support ticket through the AWS Management Console. Request the removal of S3 service limits from the account."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 44\n\nA company has an Amazon S3 bucket that contains critical data. The company must protect the data from accidental deletion.\nWhich combination of steps should a solutions architect take to meet these requirements? \n",
        "answers": [
            0,
            1
        ],
        "options": [
            "Enable versioning on the S3 bucket.",
            "Enable MFA Delete on the S3 bucket.",
            "Create a bucket policy on the S3 bucket.",
            "Enable default encryption on the S3 bucket.",
            "Create a lifecycle policy for the objects in the S3 bucket."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 45\n\n A company has a data ingestion workflow that consists of the following:  \n- An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data deliveries.\n- An AWS Lambda function to process the data and record metadata.\nThe company observes that the ingestion workflow fails occasionally because of network connectivity issues. When such a failure occurs, the Lambda function does not ingest the corresponding data unless the company manually reruns the job.  Which combination of actions should a solutions architect take to ensure that the Lambda function ingests all data in the future?  ",
        "answers": [
            1,
            4
        ],
        "options": [
            "Deploy the Lambda function in multiple Availability Zones.",
            "Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.",
            "Increase the CPU and memory that are allocated to the Lambda function.",
            "Increase provisioned throughput for the Lambda function.",
            "Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 46\n\nA company has an application that provides marketing services to stores. The services are based on previous purchases by store customers. The stores upload\ntransaction data to the company through SFTP, and the data is processed and analyzed to generate new marketing offers. Some of the files can exceed 200 GB in size.\nRecently, the company discovered that some of the stores have uploaded files that contain personally identifiable information (PII) that should not have been included.\nThe company wants administrators to be alerted if Pll is shared again. The company also wants to automate remediation.\n\nWhat should a solutions architect do to meet these requirements with the LEAST development effort?\n",
        "answers": [
            1
        ],
        "options": [
            "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain Pll, trigger an S3 Lifecycle policyto remove the objects that contain Pll.",
            "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain Pll, use Amazon Simple NotificationService (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain Pll.",
            "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain Pll, useAmazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain Pll.",
            "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain Pll, useAmazon Simple Email Service (Amazon SES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the meats that contain Pll."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 47\n\nA company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region for an upcoming event that will last 1 week.\nWhat should the company do to guarantee the EC2 capacity?\n",
        "answers": [
            3
        ],
        "options": [
            "Purchase Reserved Instances that specify the Region needed.",
            "Create an On-Demand Capacity Reservation that specifies the Region needed.",
            "Purchase Reserved Instances that specify the Region and three Availability Zones needed.",
            "Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 48\n\nA company's website uses an Amazon EC2 instance store for its catalog of items. The company wants to make sure that the catalog is highly available and that the\ncatalog is stored in a durable location.\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Move the catalog to Amazon ElastiCache for Redis.",
            "Deploy a larger EC2 instance with a larger instance store.",
            "Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.",
            "Move the catalog to an Amazon Elastic File System (Amazon EFS) file system."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 49\n\nAcompany stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call, but users access the files infrequently after 1 year. The\ncompany wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older\nfiles is acceptable.\n\nWhich solution will meet these requirements MOST cost-effectively?\n",
        "answers": [
            1
        ],
        "options": [
            "Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the files from S3 Glacier Instant Retrieval.",
            "Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.",
            "Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive in Amazon S3 Standard storage. Use S3 Lifecycle policiesto move the files to S3 Glacier Instant Retrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.",
            "Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Deep Archive after 1 year. Store search metadata inAmazon RDS. Query the files from Amazon RDS. Retrieve the files from S3 Glacier Deep Archive."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 50\n\nA company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by third-party software. The company needs to patch the\nthird-party software on all EC2 instances as quickly as possible to remediate a critical security vulnerability.\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Create an AWS Lambda function to apply the patch to all EC2 instances.",
            "Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.",
            "Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.",
            "Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 51 \n\nA company is developing an application that provides order shipping statistics for retrieval by a REST API. The company wants to extract the shipping statistics, organize\nthe data into an easy-to-read HTML format, and send the report to several email addresses at the same time every morning.\nWhich combination of steps should a solutions architect take to meet these requirements?  \n",
        "answers": [
            1,
            3
        ],
        "options": [
            "Configure the application to send the data to Amazon Kinesis Data Firehose.",
            "Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.",
            "Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 52\n\nA company wants to migrate its on-premises application to AWS. The application produces output files that vary in size from tens of gigabytes to hundreds of terabytes.\nThe application data must be stored in a standard file system structure. The company wants a solution that scales automatically. is highly available, and requires\nminimum operational overhead.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon S3 for storage.",
            "Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic Block Store (Amazon EBS) for storage.",
            "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.",
            "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic Block Store (Amazon EBS) for storage."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 53\n\nAccompany needs to store its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then must be archived for an additional 9\nyears. No one at the company, including administrative users and root users, can be able to delete the records during the entire 10-year period. The records must be\nstored with maximum resiliency.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletion of the records for a period of 10 years.",
            "Store the records by using S3 Intelligent-Tiering. Use an IAM policy to deny deletion of the records. After 10 years, change the IAM policy to allow deletion.",
            "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.",
            "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for a period of 10 years."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 54\n\nA company runs multiple Windows workloads on AWS. The company's employees use Windows file shares that are hosted on two Amazon EC2 instances. The file shares\nsynchronize data between themselves and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves how users\ncurrently access the files.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.",
            "Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 instances.",
            "Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.",
            "Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 55\n\nA solutions architect is developing a VPC architecture that includes multiple subnets. The architecture will host applications that use Amazon EC2 instances and Amazon\nRDS DB instances. The architecture consists of six subnets in two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a dedicated\nsubnet for databases. Only EC2 instances that run in the private subnets can have access to the RDS databases.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Create a new route table that excludes the route to the public subnets\u2019 CIDR blocks. Associate the route table with the database subnets.",
            "Create a security group that denies inbound traffic from the security group that is assigned to instances in the public subnets. Attach the security group to the DB instances.",
            "Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.",
            "Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and thedatabase subnets."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 56\n\n A company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in ca-centeral-1 Region as a public interface for it backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with corresponding certificate so that the third-party services can use HTTPS.  Which solution will meet these requirements? ",
        "answers": [
            2
        ],
        "options": [
            "Create stage variables in API Gateway with Name='Endpoint-URL' and Value = 'Company Domain name' to overwire the default URL. Import the public certificate associated with the company's domain anme into AWS Certificate Manager(ACM).",
            "Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region",
            "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with thecompany's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to routetraffic to the API Gateway endpoint",
            "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into the AWS Certificate Manager (ACM) in the us-east-1 Region. Atteach the certificate to the API Gateway API's. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 57\n\nA company is running a popular social media website. The website gives users the ability to upload images to share with other users. The company wants to make sure\nthat the images do not contain inappropriate content. The company needs a solution that minimizes development effort.\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Use Amazon Comprehend to detect inappropriate content. Use human review for low-confidence predictions.",
            "Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence predictions.",
            "Use Amazon SageMaker to detect inappropriate content. Use ground truth to label low-confidence predictions.",
            "Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content. Use ground truth to label low-confidence predictions."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 58\n\nA company wants to run its critical applications in containers to meet requirements for scalability and availability. The company prefers to focus on maintenance of the\ncritical applications. The company does not want to be responsible for provisioning and managing the underlying infrastructure that runs the containerized workload.\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Use Amazon EC2 instances, and install Docker on the instances.",
            "Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes.",
            "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.",
            "Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-optimized Amazon Machine Image (AMI)."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 59\n\n A company hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day. What should a solutions architect do to transmit and process the clickstream data? ",
        "answers": [
            3
        ],
        "options": [
            "Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster with the data to generate analytics.",
            "Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 data lake for Amazon Redshift to use for analysis.",
            "Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is added to the S3 bucket. run an AWS Lambda function to processthe data for analysis.",
            "Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in AmazonRedshift for analysis."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 60\n\nA company has a website hosted on AWS. The website is behind an Application Load Balancer (ALB) that is configured to handle HTTP and HTTPS separately. The\ncompany wants to forward all requests to the website so that the requests will use HTTPS.\nWhat should a solutions architect do to meet this requirement?\n",
        "answers": [
            2
        ],
        "options": [
            "Update the ALB's network ACL to accept only HTTPS traffic.",
            "Create a rule that replaces the HTTP in the URL with HTTPS.",
            "Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.",
            "Replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI)."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 61\n\nA company is developing a two-tier web application on AWS. The company's developers have deployed the application on an Amazon EC2 instance that connects directly\nto a backend Amazon RDS database. The company must not hardcode database credentials in the application. The company must also implement a solution to\nautomatically rotate the database credentials on a regular basis.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            2
        ],
        "options": [
            "Store the database credentials in the instance metadata. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda functionthat updates the RDS credentials and instance metadata at the same time.",
            "Store the database credentials in a configuration file in an encrypted Amazon S3 bucket. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run ascheduled AWS Lambda function that updates the RDS credentials and the credentials in the configuration file at the same time. Use S3 Versioning to ensure theability to fall back to previous values.",
            "Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the secret. Attach the required permission to the EC2 role togrant access to the secret.",
            "Store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store. Turn on automatic rotation for the encrypted parameters.Attach the required permission to the EC2 role to grant access to the encrypted parameters."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 62\n\n A company is deploying a new public application to AWS. The application will run behind an Application Load Balancer (ALB). The application needs to be encrypted at the edge with an SSL/TLS certificate that is issued by an external certificate authority (CA). The certificate must be rotated each year before the certificate expires. What should a solutions architect do to meet these requirements? ",
        "answers": [
            3
        ],
        "options": [
            "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate thecertificate.",
            "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Import the key material from the certificate. Apply the certificate to the ALUse the managedrenewal feature to automatically rotate the certificate.",
            "Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA. Apply the certificate to the ALB. Use themanaged renewal feature to automatically rotate the certificate.",
            "Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the ALB. Use Amazon EventBridge (Amazon CloudWatch Events) tosend a notification when the certificate is nearing expiration. Rotate the certificate manually."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 63\n\nA company runs its infrastructure on AWS and has a registered base of 700,000 users for its document management application. The company intends to create a\nproduct that converts large .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files. A\nsolutions architect must design a scalable solution to accommodate demand that will grow rapidly over time.\n\nWhich solution meets these requirements MOST cost-effectively?\n",
        "answers": [
            0
        ],
        "options": [
            "Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Amazon S3.",
            "Save the .pdf files to Amazon DynamoDB Use the DynamoDB Streams feature to invoke an AWS Lambda function to convert the files to .jpg format and store them back in DynamoDB.",
            "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an AutoScaling group. Use a program in the EC2 instances to convert the files to .jpg format. Save the .pdf files and the .jpg files in the EBS store.",
            "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an AutoScaling group. Use a program in the EC2 instances to convert the file to .jpg format. Save the .pdf files and the .jpg files in the EBS store."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 64\n\nAcompany has more than 5 TB of file data on Windows file servers that run on premises. Users and applications interact with the data each day.\nThe company is moving its Windows workloads to AWS. As the company continues this process, the company requires access to AWS and on-premises file storage with\n\nminimum latency. The company needs a solution that minimizes operational overhead and requires no significant changes to the existing file access patterns. The\ncompany uses an AWS Site-to-Site VPN connection for connectivity to AWS.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads touse FSx for Windows File Server on AWS.",
            "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to the S3 File Gateway. Reconfigure the on-premises workloads andthe cloud workloads to use the S3 File Gateway.",
            "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3directly or the S3 File Gateway. depending on each workload's location.",
            "Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on premises. Move the on-premises file datato the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 65\n\nA hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The hospital uses API Gateway and Lambda to upload reports that are in PDF\nformat and JPEG format. The hospital needs to modify the Lambda code to identify protected health information (PHI) in the reports.\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            2
        ],
        "options": [
            "Use existing Python libraries to extract the text from the reports and to identify the PHI from the extracted text.",
            "Use Amazon Textract to extract the text from the reports. Use Amazon SageMaker to identify the PHI from the extracted text.",
            "Use Amazon Textract to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text.",
            "Use Amazon Rekognition to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 66\n\n A company has an application that generates a large number of files, each approximately 5 MB in size. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be deleted. Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days.  Which storage solution is MOST cost-effective? ",
        "answers": [
            2
        ],
        "options": [
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation. Delete the files 4 years after object creation.",
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-1A) 30 days from object creation. Delete thefiles 4 years after object creation.",
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete thefiles 4 years after object creation.",
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Move the files to S3 Glacier 4 years after object creation."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 67\n\nA company hosts an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table,\nand deletes the message from the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages.\nWhat should a solutions architect do to ensure messages are being processed once only?\n",
        "answers": [
            3
        ],
        "options": [
            "Use the CreateQueue API call to create a new queue.",
            "Use the AddPermission API call to add appropriate permissions.",
            "Use the ReceiveMessage API call to set an appropriate wait time.",
            "Use the ChangeMessageVisibility API call to increase the visibility timeout."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 68\n\nA solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection\nwith consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.\nWhat should the solutions architect do to meet these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.",
            "Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPNconnection fails.",
            "Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary DirectConnect connection fails.",
            "Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 69\n\n A company is running a business-critical web application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The company wants the application to be highly available with minimum downtime and minimum loss of data.  Which solution will meet these requirements with the LEAST operational effort? ",
        "answers": [
            1
        ],
        "options": [
            "Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect traffic. Use Aurora PostgreSQL Cross-Region Replication.",
            "Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ. Configure an Amazon RDS Proxy instance for the database.",
            "Configure the Auto Scaling group to use one Availability Zone. Generate hourly snapshots of the database. Recover the database from the snapshots in the event of a failure.",
            "Configure the Auto Scaling group to use multiple AWS Regions. Write the data from the application to Amazon S3. Use S3 Event Notifications to launch an AWSLambda function to write the data to the database."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 70\n\n A company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group is configured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the web service  The company notices that the NLB is not detecting HTTP errors for the application. These errors require a manual restart of the EC2 instances that run the web service. The company needs to improve the application's availability without writing custom scripts or code. What should a solutions architect do to meet these requirements? ",
        "answers": [
            2
        ],
        "options": [
            "Enable HTTP health checks on the NLB, supplying the URL of the company's application.",
            "Add a cron job to the EC2 instances to check the local application's logs once each minute. If HTTP errors are detected. the application will restart.",
            "Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the URL of the company's application. Configure an Auto Scalingaction to replace unhealthy instances.",
            "Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the NLB. Configure an Auto Scaling action to replace unhealthy instanceswhen the alarm is in the ALARM state."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question 71\n\n A company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour. What should the solutions architect recommend to meet these requirements? ",
        "answers": [
            1
        ],
        "options": [
            "Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS Region.",
            "Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.",
            "Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data from S3 Glacier to DynamoDB.",
            "Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table by usingthe EBS snapshot."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 72\n\nA company runs a photo processing application that needs to frequently upload and download pictures from Amazon S3 buckets that are located in the same AWS\nRegion. A solutions architect has noticed an increased cost in data transfer fees and needs to implement a solution to reduce these costs.\n\nHow can the solutions architect meet this requirement?\n",
        "answers": [
            3
        ],
        "options": [
            "Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through it.",
            "Deploy a NAT gateway into a public subnet and attach an endpoint policy that allows access to the S3 buckets.",
            "Deploy the application into a public subnet and allow it to route through an internet gateway to access the S3 buckets.",
            "Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 73\n\nA company recently launched Linux-based application instances on Amazon EC2 in a private subnet and launched a Linux-based bastion host on an Amazon EC2 instance\nin a public subnet of a VPC. A solutions architect needs to connect from the on-premises network, through the company's internet connection, to the bastion host, and to\nthe application servers. The solutions architect must make sure that the security groups of all the EC2 instances will allow that access.\n\nWhich combination of steps should the solutions architect take to meet these requirements? \n",
        "answers": [
            2,
            3
        ],
        "options": [
            "Replace the current security group of the bastion host with one that only allows inbound access from the application instances.",
            "Replace the current security group of the bastion host with one that only allows inbound access from the internal IP range for the company.",
            "Replace the current security group of the bastion host with one that only allows inbound access from the external IP range for the company.",
            "Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.",
            "Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 74\n\nA solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database\ntier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company.\nHow should security groups be configured in this situation? \n",
        "answers": [
            0,
            2
        ],
        "options": [
            "Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.",
            "Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.",
            "Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier.",
            "Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier.",
            "Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier.Q\u00ae"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 75\n\n A company wants to move a multi-tired application from on premises to the AWS Cloud to improve the application's performance. The application consists of application tiers that communicate with each other by way of RESTful services. Transactions are dropped when one tier becomes overloaded. A solutions architect must design a solution that resolves these issues and modernizes the application.  Which solution meets these requirements and is the MOST operationally efficient? ",
        "answers": [
            0
        ],
        "options": [
            "Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application layer. Use Amazon Simple Queue Service (Amazon SQS) as the communication layer between application services.",
            "Use Amazon CloudWatch metrics to analyze the application performance history to determine the servers\u2019 peak utilization during the performance failures. Increase the size of the application server's Amazon EC2 instances to meet the peak requirements.",
            "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group.Use Amazon CloudWatch to monitor the SNS queue length and scale up and down as required.",
            "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. UseAmazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 76\n\nA company receives 10 TB of instrumentation data each day from several machines located at a single factory. The data consists of JSON files stored on a storage area\nnetwork (SAN) in an on-premises data center located within the factory. The company wants to send this data to Amazon S3 where it can be accessed by several\nadditional systems that provide critical near-real-time analytics. A secure transfer is important because the data is considered sensitive.\n\nWhich solution offers the MOST reliable data transfer?\n",
        "answers": [
            1
        ],
        "options": [
            "AWS DataSync over public internet",
            "AWS DataSync over AWS Direct Connect",
            "AWS Database Migration Service (AWS DMS) over public internet",
            "AWS Database Migration Service (AWS DMS) over AWS Direct Connect"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 77\n\nAcompany needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is\nstreamed, and a storage solution for the data.\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            2
        ],
        "options": [
            "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream thatuses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data toAmazon S3.",
            "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination checking on the EC2 instance. Use AWS Glue to transformthe data and to send the data to Amazon S3.",
            "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.",
            "Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform the data. Use AWS Glue to send the data to Amazon S3."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 78\n\nA company needs to keep user transaction data in an Amazon DynamoDB table. The company must retain the data for 7 years.\nWhat is the MOST operationally efficient solution that meets these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Use DynamoDB point-in-time recovery to back up the table continuously.",
            "Use AWS Backup to create backup schedules and retention policies for the table.",
            "Create an on-demand backup of the table by using the DynamoDB console. Store the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3bucket.",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda function. Configure the Lambda function to back up the table and tostore the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 79\n\nA company is planning to use an Amazon DynamoDB table for data storage. The company is concerned about cost optimization. The table will not be used on most\nmornings. In the evenings, the read and write traffic will often be unpredictable. When traffic spikes occur, they will happen very quickly.\nWhat should a solutions architec: recommend?\n",
        "answers": [
            0
        ],
        "options": [
            "Create a DynamoDB table in on-demand capacity mode.",
            "Create a DynamoDB table with a global secondary index.",
            "Create a DynamoDB table with provisioned capacity and auto scaling.",
            "Create a DynamoDB table in provisioned capacity mode, and configure it as a global table."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 80\n\n A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an application migration initiative. A solutions architect needs to share an Amazon Machine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The AMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt EBS volume snapshots.  What is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's AWS account? ",
        "answers": [
            1
        ],
        "options": [
            "Make the encrypted AMI and snapshots publicly available. Modify the key policy to allow the MSP Partner's AWS account to use the key.",
            "Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to allow the MSP Partner's AWS account to use the key.",
            "Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to trust a new KMS key that is owned by the MSP Partner for encryption.",
            "Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS account, Encrypt the S3 bucket with a new KMS key that is owned by the MSP Partner. Copy and launch the AMI in the MSP Partner's AWS account."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 81 \n\nA solutions architect is designing the cloud architecture for a new application being deployed on AWS. The process should run in parallel while adding and removing\napplication nodes as needed based on the number of jobs to be processed. The processor application is stateless. The solutions architect must ensure that the\napplication is loosely coupled and the job items are durably stored.\nWhich design should the solutions architect use?",
        "answers": [
            2
        ],
        "options": [
            "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application.Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage.",
            "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to addand remove nodes based on network usage.",
            "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application.Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue.",
            "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application.Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 82\n\nAccompany hosts its web applications in the AWS Cloud. The company configures Elastic Load Balancers to use certificates that are imported into AWS Certificate\nManager (ACM). The company's security team must be notified 30 days before the expiration of each certificate.\nWhat should a solutions architect recommend to meet this requirement?\n",
        "answers": [
            1
        ],
        "options": [
            "Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service (Amazon SNS) topic every day, beginning 30 days before any certificatewill expire.",
            "Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure Amazon EventBridge (Amazon CloudWatch Events) to invoke acustom alert by way of Amazon Simple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource.",
            "Use AWS Trusted Advisor to check for certificates that will expire within 30 days. Create an Amazon CloudWatch alarm that is based on Trusted Advisor metrics forcheck status changes. Configure the alarm to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates that will expire within 30 days. Configure the rule to invoke an AWSLambda function. Configure the Lambda function to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS)."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 83\n\n A company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed. What should the solutions architect recommend? ",
        "answers": [
            2
        ],
        "options": [
            "Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.",
            "Move the website to Amazon S3. Use Cross-Region Replication between Regions.",
            "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
            "Use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 84\n\nA company wants to reduce the cost of its existing three-tier web architecture. The web, application, and database servers are running on Amazon EC2 instances for the\ndevelopment, test, and production environments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization during non-peak hours.\n\nThe production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 hours each day. The company plans to implement\nautomation to stop the development and test EC2 instances when they are not in use.\n\nWhich EC2 instance purchasing solution will meet the company's requirements MOST cost-effectively?\n",
        "answers": [
            1
        ],
        "options": [
            "Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.",
            "Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and test EC2 instances.",
            "Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.",
            "Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and test EC2 instances."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 85\n\n A company has a production web application in which users upload documents through a web interface or a mobile app. According to a new regulatory requirement. new documents cannot be modified or deleted after they are stored. What should a solutions architect do to meet this requirement?",
        "answers": [
            0
        ],
        "options": [
            "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled.",
            "Store the uploaded documents in an Amazon S3 bucket. Configure an S3 Lifecycle policy to archive the documents periodically.",
            "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled. Configure an ACL to restrict all access to read-only.",
            "Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume. Access the data by mounting the volume in read-only mode."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 86\n\nA company has several web servers that need to frequently access a common Amazon RDS MySQL Multi-AZ DB instance. The company wants a secure method for the\nweb servers to connect to the database while meeting a security requirement to rotate user credentials frequently.\nWhich solution meets these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow the web servers to access AWS Secrets Manager.",
            "Store the database user credentials in AWS Systems Manager OpsCenter. Grant the necessary IAM permissions to allow the web servers to access OpsCenter.",
            "Store the database user credentials in a secure Amazon S3 bucket. Grant the necessary IAM permissions to allow the web servers to retrieve credentials andaccess the database.",
            "Store the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the web server file system. The web server should be ableto decrypt the files and access the database."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 87\n\nA company hosts an application on AWS Lambda functions that are invoked by an Amazon API Gateway API. The Lambda functions save customer data to an Amazon\nAurora MySQL database. Whenever the company upgrades the database, the Lambda functions fail to establish database connections until the upgrade is complete. The\nresult is that customer data is not recorded for some of the event.\n\nA solutions architect needs to design a solution that stores customer data that is created during database upgrades.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Provision an Amazon RDS proxy to sit between the Lambda functions and the database. Configure the Lambda functions to connect to the RDS proxy.",
            "Increase the run time of the Lambda functions to the maximum. Create a retry mechanism in the code that stores the customer data in the database.",
            "Persist the customer data to Lambda local storage. Configure new Lambda functions to scan the local storage to save the customer data to the database.",
            "Store the customer data in an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Create a new Lambda function that polls the queue and stores thecustomer data in the database."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 88\n\n A survey company has gathered data for several years from areas in the United States. The company hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company has started to share the data with a European marketing firm that has S3 buckets. The company wants to ensure that its data transfer costs remain as low as possible.  Which solution will meet these requirements? ",
        "answers": [
            0
        ],
        "options": [
            "Configure the Requester Pays feature on the company's S3 bucket.",
            "Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing firm's S3 buckets.",
            "Configure cross-account access for the marketing firm so that the marketing firm has access to the company's S3 bucket.",
            "Configure the company's S3 bucket to use S3 Intelligent-Tiering. Sync the S3 bucket to one of the marketing firm's S3 buckets."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 89\n\nA company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to restrict access to audit team IAM user credentials according\nto the principle of least privilege. Company managers are worried about accidental deletion of documents in the S3 bucket and want a more secure solution.\nWhat should a solutions architect do to secure the audit documents?\n",
        "answers": [
            0
        ],
        "options": [
            "Enable the versioning and MFA Delete features on the S3 bucket.",
            "Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account.",
            "Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during audit dates.",
            "Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 90\n\nA company is using a SQL database to store movie data that is publicly accessible. The database runs on an Amazon RDS Single-AZ DB instance. A script runs queries at\nrandom intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours.\n\nThe company's development team notices that the database performance is inadequate for development tasks when the script is running. A solutions architect must\nTecommend a solution to resolve this issue.\n\nWhich solution will meet this requirement with the LEAST operational overhead?\n",
        "answers": [
            1
        ],
        "options": [
            "Modify the DB instance to be a Multi-AZ deployment.",
            "Create a read replica of the database. Configure the script to query only the read replica.",
            "Instruct the development team to manually export the entries in the database at the end of each day.",
            "Use Amazon ElastiCache to cache the common queries that the script runs against the database."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 91\n\nA company has applications that run on Amazon EC2 instances in a VPC. One of the applications needs to call the Amazon S3 API to store and read objects. According to\nthe company's security regulations, no traffic from the applications is allowed to travel across the internet.\nWhich solution will meet these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Configure an S3 gateway endpoint.",
            "Create an S3 bucket in a private subnet.",
            "Create an S3 bucket in the same AWS Region as the EC2 instances.",
            "Configure a NAT gateway in the same subnet as the EC2 instances."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 92\n\n A company is storing sensitive user information in an Amazon S3 bucket. The company wants to provide secure access to this bucket from the application tier running on Amazon EC2 instances inside a VPC. Which combination of steps should a solutions architect take to accomplish this?  ",
        "answers": [
            0,
            2
        ],
        "options": [
            "Configure a VPC gateway endpoint for Amazon S3 within the VPC.",
            "Create a bucket policy to make the objects in the S3 bucket public.",
            "Create a bucket policy that limits access to only the application tier running in the VPC.",
            "Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance.",
            "Create a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0,
        "tags": [
            "Security",
            "S3"
        ],
        "attempt_history": []
    },
    {
        "question": "Question 93\n\nA company runs an on-premises application that is powered by a MySQL database. The company is migrating the application to AWS to increase the application's\nelasticity and availability.\n\nThe current architecture shows heavy read activity on the database during times of normal operation. Every 4 hours, the company's development team pulls a full export\nof the production database to populate a database in the staging environment. During this period, users experience unacceptable application latency. The development\nteam is unable to use the staging environment until the procedure completes.\n\nA solutions architect must recommend replacement architecture that alleviates the application latency issue. The replacement architecture also must give the\ndevelopment team the ability to continue using the staging environment without delay.\n\nWhich solution meets these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging database by implementing a backup and restore process that usesthe mysqldump utility.",
            "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.",
            "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Use the standby instance for the staging database.",
            "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate the staging database by implementing a backup and restoreprocess that uses the mysqldump utility."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 94\n\n A company is designing an application where users upload small files into Amazon S3. After a user uploads a file, the file requires one-time simple processing to transform the data and save the data in JSON format for later analysis.  Each file must be processes quickly as possible after it is uploaded. Demand will vary. On some days, users will upload a high number of files. On other days, users will upload a few files or no files.  Which solution meets these requirements with the LEAST operational overhead? ",
        "answers": [
            2
        ],
        "options": [
            "Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the data. Store the resulting JSON file in an Amazon Aurora DBcluster.",
            "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.",
            "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.",
            "Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new file is uploaded. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON file in an Amazon Aurora DB cluster."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 95\n\nAn application allows users at a company's headquarters to access product data. The product data is stored in an Amazon RDS MySQL DB instance. The operations team\nhas isolated an application performance slowdown and wants to separate read traffic from write traffic. A solutions architect needs to optimize the application's\n\nperformance quickly.\nWhat should the solutions architect recommend?\n",
        "answers": [
            3
        ],
        "options": [
            "Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary Availability Zone.",
            "Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary Availability Zone.",
            "Create read replicas for the database. Configure the read replicas with half of the compute and storage resources as the source database.",
            "Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 96\n\nAn Amazon EC2 adminstrator created the fowlling policy associated with an IAM group containing several users:\n\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": \"ec2:terminateInstances\",\n\"Resource\": \"*\"\n\"Condition\": {\n\"IpAddress\": {\n\"aws:SourceIp\":\"10.100.100.0/24\"\n}\n}\n},{\n\"Effect\": \"Deny\",\n\"Action\": \"ec2:*\",\n\"Resource\": \"*\"\n\"Condition\": {\n\"StringNotEquals\":{\n \"ec2:Region\": \"us-east-1\", \n}\n}\n}\n]\n\nWhich combination of actions will the users be able to perform? \n",
        "answers": [
            2
        ],
        "options": [
            "Users can terminate an EC2 instance in any AWS Region except us-east-1.",
            "Users can terminate an EC2 instance iwth the IP address 10.100.100.1 in the us-east-1 Region.",
            "Users can terminate an EC2 instance in the us-east-1 Region when the user's souce IP is 10.100.100.254.",
            "Users can not terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 97\n\nA company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft Windows shared file storage. The company wants to migrate this\nworkload to the AWS Cloud and is considering various storage Mons. The storage solution must be highly available and integrated with Active Directory for access\ncontrol.\n\nWhich solution will satisfy these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Configure Amazon EFS storage and set the Active Directory domain for authentication.",
            "Create an SMB file share on an AWS Storage Gateway file gateway in two Availability Zones.",
            "Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume.",
            "Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 98\n\n An image-processing company has a web application that users use to upload images. The application uploads the images into an Amazon S3 bucket. The company has set up S3 event notifications is to publish the object creation events to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function that processes the images and sends the results to users through email.  Users report that they are receiving multiple email messages for every uploaded image. A solutions architect determines that SQS messages are invoking the Lambda function more than once, resulting in multiple email messages.  What should the solutions architect do to resolve this issue with the LEAST operational overhead? ",
        "answers": [
            2
        ],
        "options": [
            "Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds.",
            "Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard duplicate messages.",
            "Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout.",
            "Modify the Lambda function to delete each message from the SQS queue immediately after the message is read before processing."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 99\n\nA company is implementing a shared storage solution for a gaming application that is hosted in an on-premises data center. The company needs the ability to use Lustre\nclients to access data. The solution must be fully managed.\nWhich solution meets these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.",
            "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.",
            "Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect theapplication server to the file system.",
            "Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 100\n\nA company's containerized application runs on an Amazon EC2 instance. The application needs to download security certificates before it can communicate with other\nbusiness applications. The company wants a highly secure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in\nhighly available storage after the data is encrypted.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            2
        ],
        "options": [
            "Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as needed. Control access to the data by using fine-grained IAM access.",
            "Create an AWS Lambda function that uses the Python cryptography library to receive and perform encryption operations. Store the function in an Amazon S3 bucket.",
            "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon S3.",
            "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block Store (Amazon EBS) volumes."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 101\n\nAccompany is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message\nService (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.",
            "Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.",
            "Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.",
            "Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 102\n\nA company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored in the S3 bucket. Additionally, the encryption key must be\nautomatically rotated every year.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            0
        ],
        "options": [
            "Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3encryption keys.",
            "Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket.",
            "Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket's default encryption behavior to use the customer managed KMSkey. Move the data to the S3 bucket. Manually rotate the KMS key every year.",
            "Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS Key Management Service (AWS KMS) key without keymaterial. Import the customer key material into the KMS key. Enable automatic key rotation."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 103\n\nThe customers of a finance company request appointments with financial advisors by sending text messages. A web application that runs on Amazon EC2 instances\naccepts the appointment requests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another\napplication that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages to the customers. After successful scheduling, this\napplication stores the meeting information in an Amazon DynamoDB database.\n\nAs the company expands, customers report that their meeting invitations are taking longer to arrive.\n\nWhat should a solutions architect recommend to resolve this issue?\n",
        "answers": [
            3
        ],
        "options": [
            "Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.",
            "Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.",
            "Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the appointment requests.",
            "Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SQS queue."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 104\n\nAn online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers\nand stores this data in Amazon S3. Additional customer data is stored in Amazon RDS.\n\nThe company wants to make all the data available to various teams so that the teams can perform analytics. The solution must provide the ability to manage fine-grained\npermissions for the data and must minimize operational overhead.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.",
            "Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an AWS Glue crawler. Use Amazon Athena to query the data.Use S3 policies to limit access.",
            "Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use LakeFormation access controls to limit access.",
            "Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from Amazon S3 and Amazon RDS to Amazon Redshift. UseAmazon Redshift access controls to limit access."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 105\n\nA company hosts a marketing website in an on-premises data center. The website consists of static documents and runs on a single server. An administrator updates the\nwebsite content infrequently and uses an SFTP client to upload new documents.\n\nThe company decides to host its website on AWS and to use Amazon CloudFront. The company\u2019s solutions architect creates a CloudFront distribution. The solutions\narchitect must design the most cost-effective and resilient architecture for website hosting to serve as the CloudFront origin.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Create a virtual server by using Amazon Lightsail. Configure the web server in the Lightsail instance. Upload website content by using an SFTP client.",
            "Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer. Upload website content by using an SFTP client.",
            "Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAl). Upload website content by using the AWS CLI.",
            "Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload website content by using the SFTP client."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 106\n\nA company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where the AMIs were created. The company\n\nneeds to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 Createlmage API operation is called within the company's\naccount.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            2
        ],
        "options": [
            "Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a Createlmage API call is detected.",
            "Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on Createlmage when an API call is detected.",
            "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. Configure the target as an Amazon Simple Notification Service(Amazon SNS) topic to send an alert when a Createlmage API call is detected.",
            "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to anAmazon Simple Notification Service (Amazon SNS) topic when a Createlmage API call is detected."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 107\n\nA company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate microservice for\n\nprocessing. The company is using Amazon API Gateway to deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests\n\nbefore dispatching them to the processing microservices.\nThe company provisioned as much DynamoDB throughput as its budget allows, but the company is still experiencing availability issues and is losing user requests.\n\nWhat should a solutions architect do to address this issue without impacting existing users?\n",
        "answers": [
            3
        ],
        "options": [
            "Add throttling on the API Gateway with server-side throttling limits.",
            "Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.",
            "Create a secondary index in DynamoDB for the table with the user requests.",
            "Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 108\n\n A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket.  Which solution will meet these requirements? ",
        "answers": [
            1
        ],
        "options": [
            "Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2instance's IAM role for access.",
            "Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint.Attach a resource policy to the S3 bucket to only allow the EC2 instance\u2019s IAM role for access.",
            "Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket\u2019s service API endpoint. Create a route in the VPC route table toprovide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.",
            "Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket's service API endpoint. Create a route in the VPC routetable to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 109\n\nA solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The application will run on Amazon EC2 On-Demand Instances\nand will automatically scale across multiple Availability Zones. The EC2 instances will scale up and down frequently throughout the day. An Application Load Balancer\n(ALB) will handle the load distribution. The architecture needs to support distributed session data management. The company is willing to make changes to code if\nneeded.\n\nWhat should the solutions architect do to ensure that the architecture supports distributed session data management?\n",
        "answers": [
            0
        ],
        "options": [
            "Use Amazon ElastiCache to manage and store session data.",
            "Use session affinity (sticky sessions) of the ALB to manage session data.",
            "Use Session Manager from AWS Systems Manager to manage the session.",
            "Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage the session."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 110\n\n A company offers a food delivery service that is growing rapidly. Because of the growth, the company\u2019s order processing system is experiencing scaling problems during peak traffic hours. The current architecture includes the following: \n- A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the application.\n- Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders. \nThe order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event.  A solutions architect must ensure that the order collection process and the order fulfillment process can both scale properly during peak traffic hours. The solution must optimize utilization of the company\u2019s AWS resources.  Which solution meets these requirements? ",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure each Auto Scaling group's minimum capacityaccording to peak workload values.",
            "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure a CloudWatch alarm to invoke an Amazon SimpleNotification Service (Amazon SNS) topic that creates additional Auto Scaling groups on demand.",
            "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Scale the Auto Scaling groups based on notifications that the queues send.",
            "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 111\n\nA company hosts multiple production applications. One of the applications consists of resources from Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple\nNotification Service (Amazon SNS), and Amazon Simple Queue Service (Amazon SQS) across multiple AWS Regions. All company resources are tagged with a tag name\nof \u201capplication\u201d and a value that corresponds to each application. A solutions architect must provide the quickest solution for identifying all of the tagged components.\n\nWhich solution meets these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Use AWS CloudTrail to generate a list of resources with the application tag.",
            "Use the AWS CLI to query each service across all Regions to report the tagged components.",
            "Run a query in Amazon CloudWatch Logs Insights to report on the components with the application tag.",
            "Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 112\n\nA company needs to export its database once a day to Amazon S3 for other teams to access. The exported object size varies between 2 GB and 5 GB. The S3 access\npattern for the data is variable and changes rapidly. The data must be immediately available and must remain accessible for up to 3 months. The company needs the\nmost cost-effective solution that will not increase retrieval time.\n\nWhich S3 storage class should the company use to meet these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "S3 Intelligent-Tiering",
            "S3 Glacier Instant Retrieval",
            "S3 Standard",
            "S3 Standard-Infrequent Access (S3 Standard-IA)"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 113\n\nAccompany is developing a new mobile app. The company must implement proper traffic filtering to protect its Application Load Balancer (ALB) against common\napplication-level attacks, such as cross-site scripting or SQL injection. The company has minimal infrastructure and operational staff. The company needs to reduce its\nshare of the responsibility in managing, updating, and securing servers for its AWS environment.\n\nWhat should a solutions architect recommend to meet these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Configure AWS WAF rules and associate them with the ALB.",
            "Deploy the application using Amazon S3 with public hosting enabled.",
            "Deploy AWS Shield Advanced and add the ALB as a protected resource.",
            "Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall, which then passes the traffic to the current ALB."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 114\n\n A company\u2019s reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company must convert these files to Apache Parquet format and must store the files in a transformed data bucket.  Which solution will meet these requirements with the LEAST development effort? ",
        "answers": [
            1
        ],
        "options": [
            "Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the data. Use EMR File System (EMRFS) to write files to the transformed data bucket.",
            "Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed databucket in the output step.",
            "Use AWS Batch to create a job definition with Bash syntax to transform the data and output the data to the transformed data bucket. Use the job definition tosubmit a job. Specify an array job as the job type.",
            "Create an AWS Lambda function to transform the data and output the data to the transformed data bucket. Configure an event notification for the S3 bucket.Specify the Lambda function as the destination for the event notification."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 115\n\n A company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup data need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer.  What should a solutions architect do to migrate and store the data at the LOWEST cost? ",
        "answers": [
            0
        ],
        "options": [
            "Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.",
            "Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on premises to Amazon S3 Glacier.",
            "Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier DeepArchive.",
            "Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync task to copy files from the on-premises NAS storage toAmazon S3 Glacier."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 116\n\nA company has a serverless website with millions of objects in an Amazon S3 bucket. The company uses the S3 bucket as the origin for an Amazon CloudFront\ndistribution. The company did not set encryption on the S3 bucket before the objects were loaded. A solutions architect needs to enable encryption for all existing objects\nand for all objects that are added to the S3 bucket in the future.\n\nWhich solution will meet these requirements with the LEAST amount of effort?\n",
        "answers": [
            1
        ],
        "options": [
            "Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download all existing objects to temporary local storage. Upload the objectsto the new S3 bucket.",
            "Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 BatchOperations job that uses the copy command to encrypt those objects.",
            "Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the settings on the S3 bucket to use server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Turn on versioning for the S3 bucket.",
            "Navigate to Amazon S3 in the AWS Management Console. Browse the S3 bucket's objects. Sort by the encryption field. Select each unencrypted object. Use theModify button to apply default encryption settings to every unencrypted object in the S3 bucket. "
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 117\n\nA company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company\nneeds to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when\nthe primary infrastructure is healthy.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in asecond AWS Region.",
            "Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora Replica inthe second Region.",
            "Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora database that is restoredfrom the latest snapshot.",
            "Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-passive failover. Create an Aurora second primary instance in the second Region."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 118\n\nA company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2\ninstance. The default network ACL has been modified to block all traffic. A solutions architect needs to make the web server accessible from everywhere on port 443.\n\nWhich combination of steps will accomplish this task? \n",
        "answers": [
            0,
            4
        ],
        "options": [
            "Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.",
            "Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.",
            "Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.",
            "Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0.",
            "Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 119\n\nA company's application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used\nAWS CloudFormation to deploy infrastructure and used the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are reporting\ndelays when the users attempt to access the application.\n\nWhich solution will resolve these issues in the MOST operationally efficient way?\n",
        "answers": [
            3
        ],
        "options": [
            "Replace the EC2 instances with T3 EC2 instances that run in an Auto Scaling group. Make the changes by using the AWS Management Console.",
            "Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the desired capacity and the maximum capacity of the AutoScaling group manually when an increase is necessary.",
            "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.",
            "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 120\n\nA solutions architect is designing a new API using Amazon API Gateway that will receive requests from users. The volume of requests is highly variable; several hours can\npass without receiving a single request. The data processing will take place asynchronously, but should be completed within a few seconds after a request is made.\n\nWhich compute service should the solutions architect have the API invoke to deliver the requirements at the lowest cost?\n",
        "answers": [
            1
        ],
        "options": [
            "An AWS Glue job",
            "An AWS Lambda function",
            "A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS)",
            "A containerized service hosted in Amazon ECS with Amazon EC2."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 121\n\n A company runs an application on a group of Amazon Linux EC2 instances. For compliance reasons, the company must retain all application log files for 7 years. The log files will be analyzed by a reporting tool that must be able to access all the files concurrently.  Which storage solution meets these requirements MOST cost-effectively? ",
        "answers": [
            3
        ],
        "options": [
            "Amazon Elastic Block Store (Amazon EBS)",
            "Amazon Elastic File System (Amazon EFS)",
            "Amazon EC2 instance store",
            "Amazon S3"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 122\n\nA company has hired an external vendor to perform work in the company's AWS account. The vendor uses an automated tool that is hosted in an AWS account that the\nvendor owns. The vendor does not have IAM access to the company\u2019s AWS account.\n\nHow should a solutions architect grant this access to the vendor?\n",
        "answers": [
            0
        ],
        "options": [
            "Create an IAM role in the company\u2019s account to delegate access to the vendor's IAM role. Attach the appropriate IAM policies to the role for the permissions thatthe vendor requires.",
            "Create an IAM user in the company\u2019s account with a password that meets the password complexity requirements. Attach the appropriate IAM policies to the userfor the permissions that the vendor requires.",
            "Create an IAM group in the company's account. Add the tool's IAM user from the vendor account to the group. Attach the appropriate IAM policies to the group forthe permissions that the vendor requires.",
            "Create a new identity provider by choosing \u201cAWS account\u201d as the provider type in the IAM console. Supply the vendor's AWS account ID and user name. Attach theappropriate IAM policies to the new provider for the permissions that the vendor requires. 12"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 123\n\nA company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic Kubernetes Service (Amazon EKS) in private subnets. The application\nneeds to write data to an Amazon DynamoDB table. A solutions architect must ensure that the application can interact with the DynamoDB table without exposing traffic\nto the internet.\n\nWhich combination of steps should the solutions architect take to accomplish this goal? \n",
        "answers": [
            3,
            0
        ],
        "options": [
            "Attach an IAM role that has sufficient privileges to the EKS pod.",
            "Attach an IAM user that has sufficient privileges to the EKS pod.",
            "Allow outbound connectivity to the DynamoDB table through the private subnets\u2019 network ACLs.",
            "Create a VPC endpoint for DynamoDB.",
            "Embed the access keys in the Java Spring Boot code."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 124\n\nA company recently migrated its web application to AWS by rehosting the application on Amazon EC2 instances in a single AWS Region. The company wants to redesign\n\nits application architecture to be highly available and fault tolerant. Traffic must reach all running EC2 instances randomly.\n\nWhich combination of steps should the company take to meet these requirements? \n",
        "answers": [
            4,
            2
        ],
        "options": [
            "Create an Amazon Route 53 failover routing policy.",
            "Create an Amazon Route 53 weighted routing policy.",
            "Create an Amazon Route 53 multivalue answer routing policy.",
            "Launch three EC2 instances: two instances in one Availability Zone and one instance in another Availability Zone.",
            "Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 125\n\nA media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to\ngrow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new\ndata with SQL.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            1
        ],
        "options": [
            "Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon S3 bucket.",
            "Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.",
            "Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as the data arrives in the S3 bucket.",
            "Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones. Configure the service to forward data to an Amazon RDS Multi-AZ database."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 126\n\nA company collects data from thousands of remote devices by using a RESTful web services application that runs on an Amazon EC2 instance. The EC2 instance receives\nthe raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket. The number of remote devices will increase into the millions soon. The company\nneeds a highly scalable solution that minimizes operational overhead.\n\nWhich combination of steps should a solutions architect take to meet these requirements? \n",
        "answers": [
            0,
            4
        ],
        "options": [
            "Use AWS Glue to process the raw data in Amazon S3.",
            "Use Amazon Route 53 to route traffic to different EC2 instances.",
            "Add more EC2 instances to accommodate the increasing amount of incoming data.",
            "Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to process the data.",
            "Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 127\n\nA company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing CloudTrail across a set of AWS accounts by using AWS Organizations from the\nparent account. The CloudTrail target S3 bucket is configured with S3 Versioning enabled. An S3 Lifecycle policy is in place to delete current objects after 3 years.\n\nAfter the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of objects has continued to rise. However, the number of new CloudTrail logs\nthat are delivered to the S3 bucket has remained consistent.\n\nWhich solution will delete objects that are older than 3 years in the MOST cost-effective manner?\n",
        "answers": [
            1
        ],
        "options": [
            "Configure the organization's centralized CloudTrail trail to expire objects after 3 years.",
            "Configure the S3 Lifecycle policy to delete previous versions as well as current versions.",
            "Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older than 3 years.",
            "Configure the parent account as the owner of all objects that are delivered to the S3 bucket."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question 128\n\nA company has an API that receives real-time data from a fleet of monitoring devices. The API stores this data in an Amazon RDS DB instance for later analysis. The\namount of data that the monitoring devices send to the API fluctuates. During periods of heavy traffic, the API often returns timeout errors.\n\nAfter an inspection of the logs, the company determines that the database is not capable of processing the volume of write traffic that comes from the API. A solutions\narchitect must minimize the number of connections to the database and must ensure that data is not lost during periods of heavy traffic.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Increase the size of the DB instance to an instance type that has more available memory.",
            "Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all active RDS DB instances.",
            "Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database.",
            "Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an AWS Lambda function that Amazon SNS invokes towrite data from the topic to the database."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 129\n\nA company manages its own Amazon EC2 instances that run MySQL databases. The company is manually managing replication and scaling as demand increases or\ndecreases. The company needs a new solution that simplifies the process of adding or removing compute capacity to or from its database tier as needed. The solution\nalso must offer improved performance, scaling, and durability with minimal effort from operations.\n\nWhich solution meets these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.",
            "Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL.",
            "Combine the databases into one larger MySQL database. Run the larger database on larger EC2 instances.",
            "Create an EC2 Auto Scaling group for the database tier. Migrate the existing databases to the new environment. 19"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 130\n\nA company is concerned that two NAT instances in use will no longer be able to support the traffic needed for the company's application. A solutions architect wants to\n\nimplement a solution that is highly available, fault tolerant, and automatically scalable.\n\nWhat should the solutions architect recommend?\n",
        "answers": [
            2
        ],
        "options": [
            "Remove the two NAT instances and replace them with two NAT gateways in the same Availability Zone.",
            "Use Auto Scaling groups with Network Load Balancers for the NAT instances in different Availability Zones.",
            "Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones.",
            "Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a Network Load Balancer."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 131\n\nAn application runs on an Amazon EC2 instance that has an Elastic IP address in VPC A. The application requires access to a database in VPC B. Both VPCs are in the\nsame AWS account.\n\nWhich solution will provide the required access MOST securely?\n",
        "answers": [
            1
        ],
        "options": [
            "Create a DB instance security group that allows all traffic from the public IP address of the application server in VPC A.",
            "Configure a VPC peering connection between VPC A and VPC B.",
            "Make the DB instance publicly accessible. Assign a public IP address to the DB instance.",
            "Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the new EC2 instance."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 132\n\nA company runs demonstration environments for its customers on Amazon EC2 instances. Each environment is isolated in its own VPC. The company's operations team\nneeds to be notified when RDP or SSH access to an environment has been established.\n",
        "answers": [
            2
        ],
        "options": [
            "Configure Amazon CloudWatch Application Insights to create AWS Systems Manager Opsitems when RDP or SSH access is detected.",
            "Configure the EC2 instances with an IAM instance profile that has an IAM role with the AmazonSSMManagedinstanceCore policy attached.",
            "Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon CloudWatch metric alarm with a notification action for when the alarm is in the ALARM state.",
            "Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification. Configure an Amazon Simple Notification Service(Amazon SNS) topic as a target. Subscribe the operations team to the topic."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 133\n\nA solutions architect has created a new AWS account and must secure AWS account root user access.\n\nWhich combination of actions will accomplish this? \n",
        "answers": [
            1,
            0
        ],
        "options": [
            "Ensure the root user uses a strong password.",
            "Enable multi-factor authentication to the root user.",
            "Store root user access keys in an encrypted Amazon S3 bucket.",
            "Add the root user to a group containing administrative permissions.",
            "Apply the required permissions to the root user with an inline policy document.23"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 134\n\nAccompany is building a new web-based customer relationship management application. The application will use several Amazon EC2 instances that are backed by\nAmazon Elastic Block Store (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the\napplication must be encrypted at rest and in transit.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use AWS Certificate Manager (ACM) to encrypt the EBS volumesand Aurora database storage at rest.",
            "Use the AWS root account to log in to the AWS Management Console. Upload the company's encryption certificates. While in the root account, select the option toturn on encryption for all data at rest and in transit for the account.",
            "Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM)certificate to the ALB to encrypt data in transit.",
            "Use BitLocker to encrypt all data at rest. Import the company\u2019s TLS certificate keys to AWS Key Management Service (AWS KMS) Attach the KMS keys to the ALB toencrypt data in transit."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 135\n\n A company is moving its on-premises Oracle database to Amazon Aurora PostgreSQL. The database has several applications that write to the same tables. The  applications need to be migrated one by one with a month in between each migration. Management has expressed concerns that the database has a high number of reads and writes. The data must be kept in sync across both databases throughout the migration.  What should a solutions architect recommend? ",
        "answers": [
            2
        ],
        "options": [
            "Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task and a table mapping to select all tables.",
            "Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication taskand a table mapping to select all tables.",
            "Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.",
            "Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a compute optimized replication instance. Create a full load pluschange data capture (CDC) replication task and a table mapping to select the largest tables."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 136\n\n A company has a three-tier application for image sharing. The application uses an Amazon EC2 instance for the front-end layer, another EC2 instance for the application layer, and a third EC2 instance for a MySQL database. A solutions architect must design a scalable and highly available solution that requires the least amount of change to the application.  Which solution meets these requirements? ",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer. Move the database to an Amazon DynamoDB table. Use AmazonS3 to store and serve users\u2019 images.",
            "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS DBinstance with multiple read replicas to serve users\u2019 images.",
            "Use Amazon S3 to host the front-end layer. Use a fleet of EC2 instances in an Auto Scaling group for the application layer. Move the database to a memory optimized instance type to store and serve users\u2019 images.",
            "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZDB instance. Use Amazon S3 to store and serve users\u2019 images. "
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 137\n\n An application running on an Amazon EC2 instance in VPC-A needs to access files in another EC2 instance in VPC-B. Both VPCs are in separate AWS accounts. The  network administrator needs to design a solution to configure secure access to EC2 instance in VPC-B from VPC-A. The connectivity should not have a single point of failure or bandwidth concerns. Which solution will meet these requirements? ",
        "answers": [
            0
        ],
        "options": [
            "Set up a VPC peering connection between VPC-A and VPC-B.",
            "Set up VPC gateway endpoints for the EC2 instance running in VPC-B.",
            "Attach a virtual private gateway to VPC-B and set up routing from VPC-A.",
            "Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate routes from VPC-A."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 138\n\n A company wants to experiment with individual AWS accounts for its engineer team. The company wants to be notified as soon as the Amazon EC2 instance usage for a given month exceeds a specific threshold for each account.  What should a solutions architect do to meet this requirement MOST cost-effectively? ",
        "answers": [
            2
        ],
        "options": [
            "Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service(Amazon SES) notification when a threshold is exceeded.",
            "Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple EmailService (Amazon SES) notification when a threshold is exceeded.",
            "Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an alert threshold for the budget.Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.",
            "Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report data with Amazon Athena. Use Amazon EventBridge to schedule an Athena query. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 139\n\nA solutions architect needs to design a new microservice for a company\u2019s application. Clients must be able to call an HTTPS endpoint to reach the microservice. The\nmicroservice also must use AWS Identity and Access Management (IAM) to authenticate calls. The solutions architect will write the logic for this microservice by using a\n\nsingle AWS Lambda function that is written in Go 1.x.\nWhich solution will deploy the function in the MOST operationally efficient way?\n",
        "answers": [
            0
        ],
        "options": [
            "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function. Enable IAM authentication on the API.",
            "Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.",
            "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM authentication logic into the Lambda@Edge function.",
            "Create an Amazon CloudFront distribution. Deploy the function to CloudFront Functions. Specify AWS_IAM as the authentication type."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 140\n\n A company previously migrated its data warehouse solution to AWS. The company also has an AWS Direct Connect connection. Corporate office users query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 50 MB and each webpage sent by the visualization tool is approximately 500 KB. Result sets returned by the data warehouse are not cached.  Which solution provides the LOWEST data transfer egress cost for the company? ",
        "answers": [
            3
        ],
        "options": [
            "Host the visualization tool on premises and query the data warehouse directly over the internet.",
            "Host the visualization tool in the same AWS Region as the data warehouse. Access it over the internet.",
            "Host the visualization tool on premises and query the data warehouse directly over a Direct Connect connection at a location in the same AWS Region.",
            "Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct Connect connection at a location in the same Region."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 141\n\nAn online learning company is migrating to the AWS Cloud. The company maintains its student records in a PostgreSQL database. The company needs a solution in which\nits data is available and online across multiple AWS Regions at all times.\n\nWhich solution will meet these requirements with the LEAST amount of operational overhead?\n",
        "answers": [
            2
        ],
        "options": [
            "Migrate the PostgreSQL database to a PostgreSQL cluster on Amazon EC2 instances.",
            "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance with the Multi-AZ feature turned on.",
            "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a read replica in another Region.",
            "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Set up DB snapshots to be copied to another Region."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 142\n\n A company hosts its web application on AWS using seven Amazon EC2 instances. The company requires that the IP addresses of all healthy EC2 instances be returned in response to DNS queries. Which policy should be used to meet this requirement? ",
        "answers": [
            2
        ],
        "options": [
            "Simple routing policy",
            "Latency routing policy",
            "Multivalue routing policy",
            "Geolocation routing policy"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 143\n\nA medical research lab produces data that is related to a new study. The lab wants to make the data available with minimum latency to clinics across the country for their\non-premises, file-based applications. The data files are stored in an Amazon S3 bucket that has read-only permissions for each clinic.\n\nWhat should a solutions architect recommend to meet these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic",
            "Migrate the files to each clinic's on-premises applications by using AWS DataSync for processing.",
            "Deploy an AWS Storage Gateway volume gateway as a virtual machine (VM) on premises at each clinic.",
            "Attach an Amazon Elastic File System (Amazon EFS) file system to each clinic's on-premises servers. "
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 144 \n\n A company is using a content management system that runs on a single Amazon EC2 instance. The EC2 instance contains both the web server and the database software. The company must make its website platform highly available and must enable the website to scale to meet user demand.  What should a solutions architect recommend to meet these requirements? ",
        "answers": [
            2
        ],
        "options": [
            "Move the database to Amazon RDS, and enable automatic backups. Manually launch another EC2 instance in the same Availability Zone. Configure an ApplicationLoad Balancer in the Availability Zone, and set the two instances as targets.",
            "Migrate the database to an Amazon Aurora instance with a read replica in the same Availability Zone as the existing EC2 instance. Manually launch another EC2instance in the same Availability Zone. Configure an Application Load Balancer, and set the two EC2 instances as targets.",
            "Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Configure anApplication Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.",
            "Move the database to a separate EC2 instance, and schedule backups to Amazon S3. Create an Amazon Machine Image (AMI) from the original EC2 instance.Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 145\n\nA company is launching an application on AWS. The application uses an Application Load Balancer (ALB) to direct traffic to at least two Amazon EC2 instances in a single\ntarget group. The instances are in an Auto Scaling group for each environment. The company requires a development environment and a production environment. The\nproduction environment will have periods of high traffic.\n\nWhich solution will configure the development environment MOST cost-effectively?\n",
        "answers": [
            3
        ],
        "options": [
            "Reconfigure the target group in the development environment to have only one EC2 instance as a target.",
            "Change the ALB balancing algorithm to least outstanding requests.",
            "Reduce the size of the EC2 instances in both environments.",
            "Reduce the maximum number of EC2 instances in the development environments Auto Scaling group."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 146\n\nA company runs a web application on Amazon EC2 instances in multiple Availability Zones. The EC2 instances are in private subnets. A solutions architect implements an\ninternet-facing Application Load Balancer (ALB) and specifies the EC2 instances as the target group. However, the internet traffic is not reaching the EC2 instances.\n\nHow should the solutions architect reconfigure the architecture to resolve this issue?\n",
        "answers": [
            3
        ],
        "options": [
            "Replace the ALB with a Network Load Balancer. Configure a NAT gateway in a public subnet to allow internet traffic.",
            "Move the EC2 instances to public subnets. Add a rule to the EC2 instances\u2019 security groups to allow outbound traffic to 0.0.0.0/0.",
            "Update the route tables for the EC2 instances\u2019 subnets to send 0.0.0.0/0 traffic through the internet gateway route. Add a rule to the EC2 instances\u2019 security groups to allow outbound traffic to 0.0.0.0/0.",
            "Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the route tables for the public subnets with a route to the private subnets."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 147\n\n A company has deployed a database in Amazon RDS for MySQL. Due to increased transactions, the database support team is reporting slow reads against the DB instance and recommends adding a read replica.  Which combination of actions should a solutions architect take before implementing this change?  ",
        "answers": [
            2,
            4
        ],
        "options": [
            "Enable binlog replication on the RDS primary node.",
            "Choose a failover priority for the source DB instance.",
            "Allow long-running transactions to complete on the source DB instance.",
            "Create a global table and specify the AWS Regions where the table will be available.",
            "Enable automatic backups on the source instance by setting the backup retention period to a value other than 0."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 148\n\nA company runs analytics software on Amazon EC2 instances. The software accepts job requests from users to process data that has been uploaded to Amazon S3.\nUsers report that some submitted data is not being processed Amazon CloudWatch reveals that the EC2 instances have a consistent CPU utilization at or near 100%. The\ncompany wants to improve system performance and scale the system based on user load.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Create a copy of the instance. Place all instances behind an Application Load Balancer.",
            "Create an S3 VPC endpoint for Amazon S3. Update the software to reference the endpoint.",
            "Stop the EC2 instances. Modify the instance type to one with a more powerful CPU and more memory. Restart the instances.",
            "Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2 Auto Scaling group based on queue size. Update the software to read from the queue."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 149\n\nAcompany is implementing a shared storage solution for a media application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to\naccess data. The solution must be fully managed.\n\nWhich AWS solution meets these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Create an AWS Storage Gateway volume gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.",
            "Create an AWS Storage Gateway tape gateway. Configure tapes to use Amazon S3. Connect the application server to the tape gateway.",
            "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.",
            "Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 150\n\nA company's security team requests that network traffic be captured in VPC Flow Logs. The logs will be frequently accessed for 90 days and then accessed intermittently.\n\nWhat should a solutions architect do to meet these requirements when configuring the logs?\n",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 90 days",
            "Use Amazon Kinesis as the target. Configure the Kinesis stream to always retain the logs for 90 days.",
            "Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and enable S3 Intelligent-Tiering.",
            "Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 151\n\nAn Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound internet access, but the EC2 instance needs the ability to\ndownload monthly security updates from an outside vendor.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway as the default route.",
            "Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.",
            "Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as thedefault route.",
            "Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure theprivate subnet route table to use the internet gateway as the default route."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 152\n\nA solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time.\n\nThe files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy.\n\nWhich solution meets these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Amazon Elastic File System (Amazon EFS)",
            "Amazon Elastic Block Store (Amazon EBS)",
            "Amazon S3 Glacier Deep Archive",
            "AWS Backup"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 153\n\n  A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM user. \nPolicy1: {'Version': '2012-10-17', 'Statement': [{'Effect': 'Allow', 'Action': ['iam:Get*', 'iam:List*', 'kms:List*', 'ec2:*', 'ds:*', 'logs:Get*', 'logs:Describe*'], 'Resource': '*'}]}, Policy2: {'Version': '2012-10-17', 'Statement': [{'Effect': 'Deny', 'Action': 'ds:Delete*', 'Resource': '*'}]}",
        "answers": [
            2
        ],
        "options": [
            "Deleting IAM Users",
            "Deleting directories",
            "Deleting Amazon EC2 instances",
            "Deleting logs from Amazon CloudWatch Logs"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 154\n\nA company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to\nAmazon EC2 security group ingress and egress rules between the application tiers.\n\nWhat should a solutions architect do to correct this issue?\n",
        "answers": [
            1
        ],
        "options": [
            "Create security group rules using the instance ID as the source or destination.",
            "Create security group rules using the security group ID as the source or destination.",
            "Create security group rules using the VPC CIDR blocks as the source or destination.",
            "Create security group rules using the subnet CIDR blocks as the source or destination."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 155\n\nA company has an ecommerce checkout workflow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during\nthe checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction.\n\nHow should a solutions architect refactor this workflow to prevent the creation of multiple orders?\n",
        "answers": [
            3
        ],
        "options": [
            "Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis DataFirehose and process the order.",
            "Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call thepayment service, and pass in the order information.",
            "Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service topoll Amazon SNS, retrieve the message, and process the order.",
            "Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment  service to retrieve the message and process the order. Delete the message from the queue."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 156\n\nA solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The solution must prevent accidental deletion of the\ndocuments and ensure that all versions of the documents are available. Users must be able to download, modify, and upload documents.\n\nWhich combination of actions should be taken to meet these requirements? \n",
        "answers": [
            3,
            1
        ],
        "options": [
            "Enable a read-only bucket ACL.",
            "Enable versioning on the bucket.",
            "Attach an IAM policy to the bucket.",
            "Enable MFA Delete on the bucket.",
            "Encrypt the bucket using AWS KMS."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 157\n\nA company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless\nsolution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The\nsolution must not affect the speed of EC2 instance launches.\n\nHow should the company move the data to Amazon S3 to meet these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
            "Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
            "Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.",
            "Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status dataand send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0,
        "tags": []
    },
    {
        "question": "Question 158\n\nA company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket.\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            3
        ],
        "options": [
            "Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.",
            "Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each, each S3 PUT event to invoke the Spark job.",
            "Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodicallyuse Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.",
            "Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 159\n\nA company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum\nperiod of 2 years. The backups must be consistent and restorable.\n\nWhich solution should a solutions architect recommend to meet these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.",
            "Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use AmazonData Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.",
            "Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.",
            "Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task tostream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 160\n\n A company's compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on-premises Active Directory controls access to the files and folders.  The company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file system.  Which solution will meet these requirements? ",
        "answers": [
            3
        ],
        "options": [
            "Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory groups to IAM groups to restrict access.",
            "Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to IAM groups to restrict access.",
            "Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict access.",
            "Join the file system to the Active Directory to restrict access."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 161\n\nA company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load\nBalancer. The instances run in an Auto Scaling group across multiple Availability Zones.\n\nThe company wants to provide its customers with different versions of content based on the devices that the customers use to access the website.\n\nWhich combination of actions should a solutions architect take to meet these requirements? \n",
        "answers": [
            0,
            2
        ],
        "options": [
            "Configure Amazon CloudFront to cache multiple versions of the content.",
            "Configure a host header in a Network Load Balancer to forward traffic to different instances.",
            "Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.",
            "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances.",
            "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 162\n\nA company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a Cache VPC for the ElastiCache cluster and an App VPC for\nthe application's Amazon EC2 instances. Both VPCs are in the us-east-1 Region.\n\nThe solutions architect must implement a solution to provide the application's EC2 instances with access to the ElastiCache cluster.\n\nWhich solution will meet these requirements MOST cost-effectively?\n",
        "answers": [
            0
        ],
        "options": [
            "Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCachecluster's security group to allow inbound connection from the application's security group.",
            "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for theElastiCache cluster's security group to allow inbound connection from the application's security group.",
            "Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the peering connection's security group to allow inbound connection from the application's security group.",
            "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for theTransit VPC's security group to allow inbound connection from the application's security group."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 163\n\nAcompany is building an application that consists of several microservices. The company has decided to use container technologies to deploy its software on AWS. The\ncompany needs a solution that minimizes the amount of ongoing effort for maintenance and scaling. The company cannot manage additional infrastructure.\n\nWhich combination of actions should a solutions architect take to meet these requirements? \n",
        "answers": [
            3,
            0
        ],
        "options": [
            "Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.",
            "Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability Zones.",
            "Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch type. Specify a desired task number level of greater than or equalto2.",
            "Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.",
            "Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability Zones. Create a deployment that specifies two or more replicas for each microservice."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 164\n\n A company has a web application hosted over 10 Amazon EC2 instances with traffic directed by Amazon Route 53. The company occasionally experiences a timeout when attempting to browse the application. The networking team finds that some DNS queries return IP addresses of unhealthy insta.ls, resulting in the timeout error.  What should a solutions architect implement to overcome these timeout errors? ",
        "answers": [
            3
        ],
        "options": [
            "Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.",
            "Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.",
            "Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.",
            "Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 165\n\nA solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the\nedge as possible, with the least delivery time.\n\nWhich solution meets these requirements and is MOST secure?\n",
        "answers": [
            2
        ],
        "options": [
            "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.",
            "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.",
            "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.",
            "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 166\n\nA company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair\nadvantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind\nApplication Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy\nendpoints.\n\nWhich solution meets these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.",
            "Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambdafunctions to optimize the traffic.",
            "Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.",
            "Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 167\n\nA company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-\nreal time and must store the data in a centralized location in Apache Parquet format for further processing.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            3
        ],
        "options": [
            "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWSLambda function to send the data to the Kinesis Data Analytics application.",
            "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.",
            "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.",
            "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 168\n\nA gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application\nstores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The\ncompany wants to improve the user experience while minimizing changes to the application's architecture.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Use Amazon ElastiCache in front of the database.",
            "Use RDS Proxy between the application and the database.",
            "Migrate the application from EC2 instances to AWS Lambda.",
            "Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 169\n\nAn ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance degradation is attributed to an increase in the\nnumber of read-only SQL queries triggered by business analysts. A solutions architect needs to solve the problem with minimal changes to the existing web application.\n\nWhat should the solutions architect recommend?\n",
        "answers": [
            2
        ],
        "options": [
            "Export the data to Amazon DynamoDB and have the business analysts run their queries.",
            "Load the data into Amazon ElastiCache and have the business analysts run their queries.",
            "Create a read replica of the primary database and have the business analysts run their queries.",
            "Copy the data into an Amazon Redshift cluster and have the business analysts run their queries."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 170\n\n A company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions architect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The data also must be encrypted in transit.  Which solution meets these requirements? ",
        "answers": [
            0
        ],
        "options": [
            "Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
            "Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
            "Create bucket policies that require the use of server-side encryption with S3 managed encryption keys (SSE-S3) for S3 uploads.",
            "Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management Service (AWS KMS) key."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 171\n\n A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon EC2 capacity is reached. The peak capacity is the \u2018same every night and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the desired EC2 capacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete.  What should the solutions architect do to meet these requirements? ",
        "answers": [
            2
        ],
        "options": [
            "Increase the minimum capacity for the Auto Scaling group.",
            "Increase the maximum capacity for the Auto Scaling group.",
            "Configure scheduled scaling to scale up to the desired compute level.",
            "Change the scaling policy to add more EC2 instances during each scaling operation."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 172\n\nA company serves a dynamic website from a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages\nto serve customers around the world. The website's architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in\nother parts of the world.\n\nThe website needs to serve requests quickly and efficiently regardless of a user's location. However, the company does not want to recreate the existing architecture\nacross multiple Regions.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as theorigin. Set the cache behavior settings to cache based on the Accept-Language request header.",
            "Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.",
            "Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable theAPI cache based on the Accept-Language request header.",
            "Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind anAmazon Route 53 record set with a geolocation routing policy."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 173\n\nA rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must create a disaster recovery (DR) strategy that includes\na different AWS Region. The company wants its database to be up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region\nneeds to run at reduced capacity and must be able to scale up if necessary.\n\nWhich solution will meet these requirements with the LOWEST recovery time objective (RTO)?\n",
        "answers": [
            1
        ],
        "options": [
            "Use an Amazon Aurora global database with a pilot light deployment.",
            "Use an Amazon Aurora global database with a warm standby deployment.",
            "Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment.",
            "Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 174\n\nA company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery (DR) solution for the application. The DR solution needs\nto have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?\n",
        "answers": [
            1
        ],
        "options": [
            "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts.",
            "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.",
            "Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.",
            "Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 175\n\nA company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an\nAmazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances\novernight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning.\n\nHow should the scaling be changed to address the staff complaints and keep costs to a minimum?\n",
        "answers": [
            2
        ],
        "options": [
            "Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.",
            "Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.",
            "Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.",
            "Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 176\n\nA company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application has data\nlayer that uses Oracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the\nRDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company\npredicts that traffic will continue to increase at a steady but unpredictable rate before leveling off.\n\nWhat should a solutions architect do to ensure the system can automatically scale for the increased traffic? \n",
        "answers": [
            0,
            3
        ],
        "options": [
            "Configure storage Auto Scaling on the RDS for Oracle instance.",
            "Migrate the database to Amazon Aurora to use Auto Scaling storage.",
            "Configure an alarm on the RDS for Oracle instance for low free storage space.",
            "Configure the Auto Scaling group to use the average CPU as the scaling metric.",
            "Configure the Auto Scaling group to use the average free memory as the scaling metric."
        ],
        "total_times_question_attempted": 2,
        "correct_times_question_attempted": 1,
        "current_probability": 0.4,
        "tags": [
            "Scaling"
        ],
        "attempt_history": [
            true
        ]
    },
    {
        "question": "Question 177\n\nA company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File\nSystem (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the\npopularity of the service has grown over time, the storage costs have become too expensive.\n\nWhich storage solution is MOST cost-effective?\n",
        "answers": [
            3
        ],
        "options": [
            "Use AWS Storage Gateway for files to store and process the video content.",
            "Use AWS Storage Gateway for volumes to store and process the video content.",
            "Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).",
            "Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 178\n\n A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-traffic queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any financial information is present in the employee data.  Which combination of steps should a solutions architect take to meet these requirements?  ",
        "answers": [
            1,
            4
        ],
        "options": [
            "Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.",
            "Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.",
            "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.",
            "Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share thedashboards with users.",
            "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple NotificationService (Amazon SNS) subscription."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 179\n\nA company has an application that is backed by an Amazon DynamoDB table. The company's compliance requirements specify that database backups must be taken\nevery month, must be available for 6 months, and must be retained for 7 years.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            0
        ],
        "options": [
            "Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.",
            "Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.",
            "Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.",
            "Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each month with a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 180\n\nA company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company's\nAmazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.",
            "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.",
            "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.",
            "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 181\n\n A company runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine compliance check, the company sets a standard that requires a Tecovery point objective (RPO) of less than 1 second for all its production databases.  Which solution meets these requirements? ",
        "answers": [
            0
        ],
        "options": [
            "Enable a Multi-AZ deployment for the DB instance.",
            "Enable auto scaling for the DB instance in one Availability Zone.",
            "Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.",
            "Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question 182\n\nA company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the\npublic subnets directs web traffic to the EC2 instances. The company wants to implement new security measures to restrict inbound traffic from the ALB to the EC2\ninstances while preventing access from any other source inside or outside the private subnet of the EC2 instances.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Configure a route in a route table to direct traffic from the internet to the private IP addresses of the EC2 instances.",
            "Configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB.",
            "Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.",
            "Configure the security group for the ALB to allow any TCP traffic on any port."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 183\n\nA research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs\nintermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an\nSMB file system.\n\nThe company maintains two synchronized file systems. This strategy is causing data duplication and inefficient resource usage. The company needs to migrate the\napplications to AWS without making code changes to either application.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            3
        ],
        "options": [
            "Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.",
            "Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.",
            "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon SimpleQueue Service (Amazon SQS) to exchange data between the applications.",
            "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 184\n\nAs part of budget planning, management wants a report of AWS billed items listed by user. The data will be used to create department budgets. A solutions architect\nneeds to determine the most efficient way to obtain this report information.\n\nWhich solution meets these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Run a query with Amazon Athena to generate the report.",
            "Create a report in Cost Explorer and download the report.",
            "Access the bill details from the billing dashboard and download the bill.",
            "Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES)."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 185\n\n A company hosts it's static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each  month.  Which solution will meet these requirements MOST cost-effectively? ",
        "answers": [
            1
        ],
        "options": [
            "Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.",
            "Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).",
            "Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with AmazonWorkMail.",
            "Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 186\n\n A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website uses a database backend. The company notices that the website does not reflect updates that have been made in the website's Git repository. The company checks the continuous integration and continuous delivery (CI/CD)  pipeline between the Git repository and Amazon S3. The company verifies that the webhooks are configured properly and that the CI/CD pipeline is sending messages that indicate successful deployments.  A solutions architect needs to implement a solution that displays the updates on the website.  Which solution will meet these requirements? ",
        "answers": [
            2
        ],
        "options": [
            "Add an Application Load Balancer.",
            "Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.",
            "Invalidate the CloudFront cache.",
            "Use AWS Certificate Manager (ACM) to validate the website's SSL certificate."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 187\n\n A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specific features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers.  How should a solutions architect design the architecture to meet these requirements? ",
        "answers": [
            1
        ],
        "options": [
            "Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.",
            "Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.",
            "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.",
            "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (i02) Amazon ElasticBlock Store (Amazon EBS) volume for file sharing between the tiers."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 188\n\nA company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make\nany changes to the application.\n\nWhat should a solutions architect do to meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Create an Amazon S3 Standard bucket with access to the web servers.",
            "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.",
            "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.",
            "Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 189\n\nA company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account.\n\nWhich solution will meet these requirements in the MOST secure manner?\n",
        "answers": [
            1
        ],
        "options": [
            "Apply an S3 bucket policy that grants read access to the S3 bucket.",
            "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.",
            "Embed an access key and a secret key in the Lambda function's code to grant the required IAM permissions for read access to the S3 bucket.",
            "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all S3 buckets in the account."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 190\n\nA company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are in an Auto Scaling group that scales in response to user demand. The\ncompany wants to optimize cost savings without making a long-term commitment.\n\nWhich EC2 instance purchasing option should a solutions architect recommend to meet these requirements?\n",
        "answers": [
            2
        ],
        "options": [
            "Dedicated Instances only",
            "On-Demand Instances only",
            "A mix of On-Demand Instances and Spot Instances",
            "A mix of On-Demand Instances and Reserved Instances"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 191\n\n A media company uses Amazon CloudFront for its publicly available streaming video content. The company wants to secure the video content that is hosted in Amazon S3 by controlling who has access. Some of the company\u2019s users are using a custom HTTP client that does not support cookies. Some of the company\u2019s users are unable  to change the hard coded URLs that they are using for access.  Which services or methods will meet these requirements with the LEAST impact to the users?  ",
        "answers": [
            1,
            0
        ],
        "options": [
            "Signed cookies",
            "Signed URLs",
            "AWS AppSync",
            "JSON Web Token (JWT)",
            "AWS Secrets Manager"
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question 192\n\nA company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the\ndata to Amazon S3. The company needs the ability to use SQL to query the transformed data.\n\nWhich solutions will meet these requirements? \n",
        "answers": [
            0,
            1
        ],
        "options": [
            "Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
            "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3.Use Amazon Athena to query the transformed data from Amazon S3.",
            "Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
            "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.",
            "Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 193\n\n A company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred.  Which solution meets these requirements? ",
        "answers": [
            3
        ],
        "options": [
            "Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises systems to mount the Snowball S3 endpoint to providelocal access to the data.",
            "Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on-premises systems withlocal access to the data.",
            "Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of datato cache locally. Mount the gateway storage volumes to provide local access to the data.",
            "Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storagevolumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 194\n\nAn application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Traffic must not traverse the internet.\n\nHow should a solutions architect configure access to meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Create a private hosted zone by using Amazon Route 53.",
            "Set up a gateway VPC endpoint for Amazon S3 in the VPC.",
            "Configure the EC2 instances to use a NAT gateway to access the S3 bucket.",
            "Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 195\n\nAn ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the\ndata in three applications. Only one of the applications needs to process the Pll. The Pll must be removed before the other two applications process the data.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n",
        "answers": [
            1
        ],
        "options": [
            "Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.",
            "Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.",
            "Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.",
            "Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 196\n\n A development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC in the same account. The new VPC will be peered with the development VPC. The VPC CIDR block for the development VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC peering connection to the development VPC.  What is the SMALLEST CIDR block that meets these requirements? ",
        "answers": [
            3
        ],
        "options": [
            "10.0.1.0/32",
            "192.168.0.0/24",
            "192.168.1.0/32",
            "10.0.1.0/24"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 197\n\nA company deploys an application on five Amazon EC2 instances. An Application Load Balancer (ALB) distributes traffic to the instances by using a target group. The\naverage CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%.\n\nA solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must\nensure that the application has enough CPU resources when surges occur.\n\nWhich solution will meet these requirements?\n",
        "answers": [
            1
        ],
        "options": [
            "Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.",
            "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.",
            "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.",
            "Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPU Utilization metric is below 20%.Configure the second CloudWatch alarm to enter the ALARM state when the average CPU Utilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2instances that are running."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 198\n\n A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance.  The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone.  which solution will make the application highly available? ",
        "answers": [
            2
        ],
        "options": [
            "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DBinstance with connections to each network.",
            "Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones.Configure the DB instance with connections to each network.",
            "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DBinstance for Multi-AZ deployment.",
            "Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones.Configure the DB instance for Multi-AZ deployment."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 199\n\n A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds of Amazon EC2 instances that run Amazon Linux will distribute and process the data.  Which solution will meet the performance requirements? ",
        "answers": [
            1
        ],
        "options": [
            "Create an Amazon FSx for NetApp ONTAP file system. Sat each volume tiering policy to ALL. Import the raw data into the file system. Mount the file system on the EC2 instances.",
            "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import datafrom and export data to Amazon S3. Mount the file system on the EC2 instances.",
            "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import datafrom and export data to Amazon S3. Mount the file system on the EC2 instances.",
            "Create an Amazon FSx for NetApp ONTAP file system. Set each volume's tiering policy to NONE. Import the raw data into the file system. Mount the file system onthe EC2 instances."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question 200\n\nA company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24\nhours a day, 7 days a week. The application's database storage continues to grow over time.\n\nWhat should a solutions architect do to meet these requirements MOST cost-effectively?\n",
        "answers": [
            2
        ],
        "options": [
            "Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.",
            "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.",
            "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.",
            "Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0
    },
    {
        "question": "Question #1\n\nA solutions architect is designing a solution where users will be directed to a backup static error page if the primary website is unavailable. The primary website'sDNS records are hosted in Amazon Route 53 where their domain is pointing to an Application Load Balancer (ALB).Which configuration should the solutions architect use to meet the company's needs while minimizing changes and infrastructure overhead?",
        "answers": [
            1
        ],
        "options": [
            "Point a Route 53 alias record to an Amazon CloudFront distribution with the ALB as one of its origins. Then, create custom error pages for the distribution.",
            "Set up a Route 53 active-passive failover configuration. Direct traffic to a static error page hosted within an Amazon S3 bucket when Route 53 health checks determine that the ALB endpoint is unhealthy.",
            "Update the Route 53 record to use a latency-based routing policy. Add the backup static error page hosted within an Amazon S3 bucket to the record so the traffic is sent to the most responsive endpoints.",
            "Set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance hosting a static error page as endpoints. Route 53 will only send requests to the instance if the health checks fail for the ALB."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Active-passive failover -Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes only the healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response toDNS queries.To create an active-passive failover configuration with one primary record and one secondary record, you just create the records and specify Failover for the routing policy. When the primary resource is healthy, Route 53 responds to DNS queries using the primary record. When the primary resource is unhealthy, Route53 responds to DNS queries using the secondary record.How Amazon Route 53 averts cascading failuresAs a first defense against cascading failures, each request routing algorithm (such as weighted and failover) has a mode of last resort. In this special mode, when all records are considered unhealthy, the Route 53 algorithm reverts to considering all records healthy.For example, if all instances of an application, on several hosts, are rejecting health check requests, Route 53 DNS servers will choose an answer anyway and return it rather than returning no DNS answer or returning an NXDOMAIN (non-existent domain) response. An application can respond to users but still fail health checks, so this provides some protection against misconfiguration.Similarly, if an application is overloaded, and one out of three endpoints fails its health checks, so that it's excluded from Route 53 DNS responses, Route 53 distributes responses between the two remaining endpoints. If the remaining endpoints are unable to handle the additional load and they fail, Route 53 reverts to distributing requests to all three endpoints.Reference:https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-problems.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #2\n\nA solutions architect is designing a high performance computing (HPC) workload on Amazon EC2. The EC2 instances need to communicate to each other frequently and require network performance with low latency and high throughput.Which EC2 configuration meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Launch the EC2 instances in a cluster placement group in one Availability Zone.",
            "Launch the EC2 instances in a spread placement group in one Availability Zone.",
            "Launch the EC2 instances in an Auto Scaling group in two Regions and peer the VPCs.",
            "Launch the EC2 instances in an Auto Scaling group spanning multiple Availability Zones."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Placement groups -When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload.Cluster \u05d2\u20ac\" packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.Reference:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #3\n\nA company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world.Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance.What should a solutions architect do to accomplish this?",
        "answers": [
            2
        ],
        "options": [
            "Use Amazon S3 with Transfer Acceleration to host the application.",
            "Use Amazon S3 with CacheControl headers to host the application.",
            "Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.",
            "Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/ec2/autoscaling/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #4\n\nA company is migrating from an on-premises infrastructure to the AWS Cloud. One of the company's applications stores files on a Windows file server farm that uses Distributed File System Replication (DFSR) to keep data in sync. A solutions architect needs to replace the file server farm.Which service should the solutions architect use?",
        "answers": [
            1
        ],
        "options": [
            "Amazon EFS",
            "Amazon FSx",
            "Amazon S3",
            "AWS Storage Gateway"
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question #5\n\nA company has a legacy application that processes data in two parts. The second part of the process takes longer than the first, so the company has decided to rewrite the application as two microservices running on Amazon ECS that can scale independently.How should a solutions architect integrate the microservices?",
        "answers": [
            3
        ],
        "options": [
            "Implement code in microservice 1 to send data to an Amazon S3 bucket. Use S3 event notifications to invoke microservice 2.",
            "Implement code in microservice 1 to publish data to an Amazon SNS topic. Implement code in microservice 2 to subscribe to this topic.",
            "Implement code in microservice 1 to send data to Amazon Kinesis Data Firehose. Implement code in microservice 2 to read from Kinesis Data Firehose.",
            "Implement code in microservice 1 to send data to an Amazon SQS queue. Implement code in microservice 2 to process messages from the queue."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #6\n\nA company captures clickstream data from multiple websites and analyzes it using batch processing. The data is loaded nightly into Amazon Redshift and is consumed by business analysts. The company wants to move towards near-real-time data processing for timely insights. The solution should process the streaming data with minimal effort and operational overhead.Which combination of AWS services are MOST cost-effective for this solution? ",
        "answers": [
            1,
            3
        ],
        "options": [
            "Amazon EC2",
            "AWS Lambda",
            "Amazon Kinesis Data Streams",
            "Amazon Kinesis Data Firehose",
            "Amazon Kinesis Data Analytics"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0,
        "tags": [
            "Firehose"
        ]
    },
    {
        "question": "Question #7\n\nA company's application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end financial calculation batch executes. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application.What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?",
        "answers": [
            2
        ],
        "options": [
            "Configure an Amazon CloudFront distribution in front of the ALB.",
            "Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.",
            "Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.",
            "Configure Amazon ElastiCache to remove some of the workload from the EC2 instances."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Scheduled Scaling for Amazon EC2 Auto ScalingScheduled scaling allows you to set your own scaling schedule. For example, let's say that every week the traffic to your web application starts to increase onWednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date.Reference:https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #8\n\nA company runs a multi-tier web application that hosts news content. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones and use an Amazon Aurora database. A solutions architect needs to make the application more resilient to periodic increases in request rates.Which architecture should the solutions architect implement? ",
        "answers": [
            3,
            4
        ],
        "options": [
            "Add AWS Shield.",
            "Add Aurora Replica.",
            "Add AWS Direct Connect.",
            "Add AWS Global Accelerator.",
            "Add an Amazon CloudFront distribution in front of the Application Load Balancer."
        ],
        "explination": "Correct Answer: DE \ud83d\uddf3\ufe0f                                                AWS Global Accelerator -Acceleration for latency-sensitive applicationsMany applications, especially in areas such as gaming, media, mobile apps, and financials, require very low latency for a great user experience. To improve the user experience, Global Accelerator directs user traffic to the application endpoint that is nearest to the client, which reduces internet latency and jitter. GlobalAccelerator routes traffic to the closest edge location by using Anycast, and then routes it to the closest regional endpoint over the AWS global network. GlobalAccelerator quickly reacts to changes in network performance to improve your users' application performance.Amazon CloudFront -Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.Reference:https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-benefits-of-migrating.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #9\n\nAn application running on AWS uses an Amazon Aurora Multi-AZ deployment for its database. When evaluating performance metrics, a solutions architect discovered that the database reads are causing high I/O and adding latency to the write requests against the database.What should the solutions architect do to separate the read requests from the write requests?",
        "answers": [
            2
        ],
        "options": [
            "Enable read-through caching on the Amazon Aurora database.",
            "Update the application to read from the Multi-AZ standby instance.",
            "Create a read replica and modify the application to use the appropriate endpoint.",
            "Create a second Amazon Aurora database and link it to the primary database as a read replica."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Amazon RDS Read Replicas -Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well asAmazon Aurora.For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the sourceDB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance. The read replica operates as a DB instance that allows only read-only connections; applications can connect to a read replica just as they would to any DB instance.Amazon RDS replicates all databases in the source DB instance.Amazon Aurora further extends the benefits of read replicas by employing an SSD-backed virtualized storage layer purpose-built for database workloads. AmazonAurora replicas share the same underlying storage as the source instance, lowering costs and avoiding the need to copy data to the replica nodes. For more information about replication with Amazon Aurora, see the online documentation.Reference:https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html https://aws.amazon.com/rds/features/read-replicas/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #10\n\nA recently acquired company is required to build its own infrastructure on AWS and migrate multiple applications to the cloud within a month. Each application has approximately 50 TB of data to be transferred. After the migration is complete, this company and its parent company will both require secure network connectivity with consistent throughput from their data centers to the applications. A solutions architect must ensure one-time data migration and ongoing network connectivity.Which solution will meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "AWS Direct Connect for both the initial transfer and ongoing connectivity.",
            "AWS Site-to-Site VPN for both the initial transfer and ongoing connectivity.",
            "AWS Snowball for the initial transfer and AWS Direct Connect for ongoing connectivity.",
            "AWS Snowball for the initial transfer and AWS Site-to-Site VPN for ongoing connectivity."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html https://aws.amazon.com/directconnect/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #91\n\nA company is seeing access requests by some suspicious IP addresses. The security team discovers the requests are from different IP addresses under the same CIDR range.What should a solutions architect recommend to the team?",
        "answers": [
            2
        ],
        "options": [
            "Add a rule in the inbound table of the security to deny the traffic from that CIDR range.",
            "Add a rule in the outbound table of the security group to deny the traffic from that CIDR range.",
            "Add a deny rule in the inbound table of the network ACL with a lower number than other rules.",
            "Add a deny rule in the outbound table of the network ACL with a lower rule number than other rules."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #92\n\nA company recently expanded globally and wants to make its application accessible to users in those geographic locations. The application is deployed onAmazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. The company needs the ability to shift traffic from resources in one region to another.What should a solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "Configure an Amazon Route 53 latency routing policy.",
            "Configure an Amazon Route 53 geolocation routing policy.",
            "Configure an Amazon Route 53 geoproximity routing policy.",
            "Configure an Amazon Route 53 multivalue answer routing policy."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #93\n\nA company wants to replicate its data to AWS to recover in the event of a disaster. Today, a system administrator has scripts that copy data to a NFS share.Individual backup files need to be accessed with low latency by application administrators to deal with errors in processing.What should a solutions architect recommend to meet these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Modify the script to copy data to an Amazon S3 bucket instead of the on-premises NFS share.",
            "Modify the script to copy data to an Amazon S3 Glacier Archive instead of the on-premises NFS share.",
            "Modify the script to copy data to an Amazon Elastic File System (Amazon EFS) volume instead of the on-premises NFS share.",
            "Modify the script to copy data to an AWS Storage Gateway for File Gateway virtual appliance instead of the on-premises NFS share."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #94\n\nAn application requires a development environment (DEV) and production environment (PROD) for several years. The DEV instances will run for 10 hours each day during normal business hours, while the PROD instances will run 24 hours each day. A solutions architect needs to determine a compute instance purchase strategy to minimize costs.Which solution is the MOST cost-effective?",
        "answers": [
            2
        ],
        "options": [
            "DEV with Spot Instances and PROD with On-Demand Instances",
            "DEV with On-Demand Instances and PROD with Spot Instances",
            "DEV with Scheduled Reserved Instances and PROD with Reserved Instances",
            "DEV with On-Demand Instances and PROD with Scheduled Reserved Instances"
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #95\n\nA company runs multiple Amazon EC2 Linux instances in a VPC across two Availability Zones. The instances host applications that use a hierarchical directory structure. The applications need to read and write rapidly and concurrently to shared storage.What should a solutions architect do to meet these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system from each EC2 instance.",
            "Create an Amazon S3 bucket. Allow access from all the EC2 instances in the VPC.",
            "Create a file system on a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume. Attach the EBS volume to all the EC2 instances.",
            "Create file systems on Amazon Elastic Block Store (Amazon EBS) volumes that are attached to each EC2 instance. Synchronize the EBS volumes across the different EC2 instances."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #96\n\nA solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon EC2 capacity is reached. The peak capacity is the same every night and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the desired EC2 capacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete.What should the solutions architect do to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Increase the minimum capacity for the Auto Scaling group.",
            "Increase the maximum capacity for the Auto Scaling group.",
            "Configure scheduled scaling to scale up to the desired compute level.",
            "Change the scaling policy to add more EC2 instances during each scaling operation."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #97\n\nA Solutions Architect must design a web application that will be hosted on AWS, allowing users to purchase access to premium, shared content that is stored in anS3 bucket. Upon payment, content will be available for download for 14 days before the user is denied access.Which of the following would be the LEAST complicated implementation?",
        "answers": [
            2
        ],
        "options": [
            "Use an Amazon CloudFront distribution with an origin access identity (OAI). Configure the distribution with an Amazon S3 origin to provide access to the file through signed URLs. Design a Lambda function to remove data that is older than 14 days.",
            "Use an S3 bucket and provide direct access to the file. Design the application to track purchases in a DynamoDB table. Configure a Lambda function to remove data that is older than 14 days based on a query to Amazon DynamoDB.",
            "Use an Amazon CloudFront distribution with an OAI. Configure the distribution with an Amazon S3 origin to provide access to the file through signed URLs. Design the application to set an expiration of 14 days for the URL.",
            "Use an Amazon CloudFront distribution with an OAI. Configure the distribution with an Amazon S3 origin to provide access to the file through signed URLs. Design the application to set an expiration of 60 minutes for the URL and recreate the URL as necessary."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #98\n\nA solutions architect is designing a mission-critical web application. It will consist of Amazon EC2 instances behind an Application Load Balancer and a relational database. The database should be highly available and fault tolerant.Which database implementations will meet these requirements? ",
        "answers": [
            3,
            4
        ],
        "options": [
            "Amazon Redshift",
            "Amazon DynamoDB",
            "Amazon RDS for MySQL",
            "MySQL-compatible Amazon Aurora Multi-AZ",
            "Amazon RDS for SQL Server Standard Edition Multi-AZ"
        ],
        "explination": "Correct Answer: DE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #99\n\nA company's web application is running on Amazon EC2 instances behind an Application Load Balancer. The company recently changed its policy, which now requires the application to be accessed from one specific country only.Which configuration will meet this requirement?",
        "answers": [
            2
        ],
        "options": [
            "Configure the security group for the EC2 instances.",
            "Configure the security group on the Application Load Balancer.",
            "Configure AWS WAF on the Application Load Balancer in a VPC.",
            "Configure the network ACL for the subnet that contains the EC2 instances."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/es/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #100\n\nA solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group.A cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer be able to perform?",
        "answers": [
            2
        ],
        "options": [
            "Deleting IAM users",
            "Deleting directories",
            "Deleting Amazon EC2 instances",
            "Deleting logs from Amazon CloudWatch Logs"
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #101\n\nA company has an Amazon EC2 instance running on a private subnet that needs to access a public website to download patches and updates. The company does not want external websites to see the EC2 instance IP address or initiate connections to it.How can a solutions architect achieve this objective?",
        "answers": [
            1
        ],
        "options": [
            "Create a site-to-site VPN connection between the private subnet and the network in which the public site is deployed.",
            "Create a NAT gateway in a public subnet. Route outbound traffic from the private subnet through the NAT gateway.",
            "Create a network ACL for the private subnet where the EC2 instance deployed only allows access from the IP address range of the public website.",
            "Create a security group that only allows connections from the IP address range of the public website. Attach the security group to the EC2 instance."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #102\n\nA company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company's network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. What should a solutions architect do to meet these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Use AWS Snowball.",
            "Use AWS DataSync.",
            "Use a secure VPN connection.",
            "Use Amazon S3 Transfer Acceleration."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #103\n\nA company has a website running on Amazon EC2 instances across two Availability Zones. The company is expecting spikes in traffic on specific holidays, and wants to provide a consistent user experience. How can a solutions architect meet this requirement?",
        "answers": [
            3
        ],
        "options": [
            "Use step scaling.",
            "Use simple scaling.",
            "Use lifecycle hooks.",
            "Use scheduled scaling."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #104\n\nAn ecommerce company is running a multi-tier application on AWS. The front-end and backend tiers both run on Amazon EC2, and the database runs on AmazonRDS for MySQL. The backend tier communicates with the RDS instance. There are frequent calls to return identical datasets from the database that are causing performance slowdowns.Which action should be taken to improve the performance of the backend?",
        "answers": [
            1
        ],
        "options": [
            "Implement Amazon SNS to store the database calls.",
            "Implement Amazon ElastiCache to cache the large datasets.",
            "Implement an RDS for MySQL read replica to cache database calls.",
            "Implement Amazon Kinesis Data Firehose to stream the calls to the database."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #105\n\nA company has an on-premises data center that is running out of storage capacity. The company wants to migrate its storage infrastructure to AWS while minimizing bandwidth costs. The solution must allow for immediate retrieval of data at no additional cost.How can these requirements be met?",
        "answers": [
            1
        ],
        "options": [
            "Deploy Amazon S3 Glacier Vault and enable expedited retrieval. Enable provisioned retrieval capacity for the workload.",
            "Deploy AWS Storage Gateway using cached volumes. Use Storage Gateway to store data in Amazon S3 while retaining copies of frequently accessed data subsets locally.",
            "Deploy AWS Storage Gateway using stored volumes to store data locally. Use Storage Gateway to asynchronously back up point-in-time snapshots of the data to Amazon S3.",
            "Deploy AWS Direct Connect to connect with the on-premises data center. Configure AWS Storage Gateway to store data locally. Use Storage Gateway to asynchronously back up point-in-time snapshots of the data to Amazon S3."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #106\n\nA company is processing data on a daily basis. The results of the operations are stored in an Amazon S3 bucket, analyzed daily for one week, and then must remain immediately accessible for occasional analysis.What is the MOST cost-effective storage solution alternative to the current configuration?",
        "answers": [
            3
        ],
        "options": [
            "Configure a lifecycle policy to delete the objects after 30 days.",
            "Configure a lifecycle policy to transition the objects to Amazon S3 Glacier after 30 days.",
            "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
            "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #107\n\nA company delivers files in Amazon S3 to certain users who do not have AWS credentials. These users must be given access for a limited time. What should a solutions architect do to securely meet these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Enable public access on an Amazon S3 bucket.",
            "Generate a presigned URL to share with the users.",
            "Encrypt files using AWS KMS and provide keys to the users.",
            "Create and assign IAM roles that will grant GetObject permissions to the users."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #108\n\nA company wants to run a hybrid workload for data processing. The data needs to be accessed by on-premises applications for local data processing using anNFS protocol, and must also be accessible from the AWS Cloud for further analytics and batch processing.Which solution will meet these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Use an AWS Storage Gateway file gateway to provide file storage to AWS, then perform analytics on this data in the AWS Cloud.",
            "Use an AWS Storage Gateway tape gateway to copy the backup of the local data to AWS, then perform analytics on this data in the AWS cloud.",
            "Use an AWS Storage Gateway volume gateway in a stored volume configuration to regularly take snapshots of the local data, then copy the data to AWS.",
            "Use an AWS Storage Gateway volume gateway in a cached volume configuration to back up all the local storage in the AWS cloud, then perform analytics on this data in the cloud."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/storagegateway/file/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #109\n\nA company plans to store sensitive user data on Amazon S3. Internal security compliance requirement mandate encryption of data before sending it to AmazonS3.What should a solutions architect recommend to satisfy these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Server-side encryption with customer-provided encryption keys",
            "Client-side encryption with Amazon S3 managed encryption keys",
            "Server-side encryption with keys stored in AWS key Management Service (AWS KMS)",
            "Client-side encryption with a master key stored in AWS Key Management Service (AWS KMS)"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #110\n\nA solutions architect is moving the static content from a public website hosted on Amazon EC2 instances to an Amazon S3 bucket. An Amazon CloudFront distribution will be used to deliver the static assets. The security group used by the EC2 instances restricts access to a limited set of IP ranges. Access to the static content should be similarly restricted.Which combination of steps will meet these requirements? ",
        "answers": [
            0,
            1
        ],
        "options": [
            "Create an origin access identity (OAI) and associate it with the distribution. Change the permissions in the bucket policy so that only the OAI can read the objects.",
            "Create an AWS WAF web ACL that includes the same IP restrictions that exist in the EC2 security group. Associate this new web ACL with the CloudFront distribution.",
            "Create a new security group that includes the same IP restrictions that exist in the current EC2 security group. Associate this new security group with the CloudFront distribution.",
            "Create a new security group that includes the same IP restrictions that exist in the current EC2 security group. Associate this new security group with the S3 bucket hosting the static content.",
            "Create a new IAM role and associate the role with the distribution. Change the permissions either on the S3 bucket or on the files within the S3 bucket so that only the newly created IAM role has read and download permissions."
        ],
        "explination": "Correct Answer: AB \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #111\n\nA company is investigating potential solutions that would collect, process, and store users' service usage data. The business objective is to create an analytics capability that will enable the company to gather operational insights quickly using standard SQL queries. The solution should be highly available and ensureAtomicity, Consistency, Isolation, and Durability (ACID) compliance in the data tier.Which solution should a solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "Use an Amazon Timestream database.",
            "Use an Amazon Neptune database in a Multi-AZ design.",
            "Use a fully managed Amazon RDS for MySQL database in a Multi-AZ design.",
            "Deploy PostgreSQL on an Amazon EC2 instance that uses Amazon Elastic Block Store (Amazon EBS) Throughput Optimized HDD (st1) storage."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #112\n\nA company recently launched its website to serve content to its global user base. The company wants to store and accelerate the delivery of static content to its users by leveraging Amazon CloudFront with an Amazon EC2 instance attached as its origin.How should a solutions architect optimize high availability for the application?",
        "answers": [
            0
        ],
        "options": [
            "Use Lambda@Edge for CloudFront.",
            "Use Amazon S3 Transfer Acceleration for CloudFront.",
            "Configure another EC2 instance in a different Availability Zone as part of the origin group.",
            "Configure another EC2 instance as part of the origin server cluster in the same Availability Zone."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #113\n\nAn application running on an Amazon EC2 instance in VPC-A needs to access files in another EC2 instance in VPC-B. Both are in separate AWS accounts. The network administrator needs to design a solution to configure secure access to EC2 instance in VPC-B from VPC-A. The connectivity should not have a single point of failure or bandwidth concerns.Which solution will meet these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Set up a VPC peering connection between VPC-A and VPC-B.",
            "Set up VPC gateway endpoints for the EC2 instance running in VPC-B.",
            "Attach a virtual private gateway to VPC-B and set up routing from VPC-A.",
            "Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate routes from VPC-A."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #114\n\nA company currently stores symmetric encryption keys in a hardware security module (HSM). A solutions architect must design a solution to migrate key management to AWS. The solution should allow for key rotation and support the use of customer provided keys.Where should the key material be stored to meet these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Amazon S3",
            "AWS Secrets Manager",
            "AWS Systems Manager Parameter store",
            "AWS Key Management Service (AWS KMS)"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #115\n\nA recent analysis of a company's IT expenses highlights the need to reduce backup costs. The company's chief information officer wants to simplify the on- premises backup infrastructure and reduce costs by eliminating the use of physical backup tapes. The company must preserve the existing investment in the on- premises backup applications and workflows.What should a solutions architect recommend?",
        "answers": [
            3
        ],
        "options": [
            "Set up AWS Storage Gateway to connect with the backup applications using the NFS interface.",
            "Set up an Amazon EFS file system that connects with the backup applications using the NFS interface.",
            "Set up an Amazon EFS file system that connects with the backup applications using the iSCSI interface.",
            "Set up AWS Storage Gateway to connect with the backup applications using the iSCSI-virtual tape library (VTL) interface."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #116\n\nA company hosts an application on an Amazon EC2 instance that requires a maximum of 200 GB storage space. The application is used infrequently, with peaks during mornings and evenings. Disk I/O varies, but peaks at 3,000 IOPS. The chief financial officer of the company is concerned about costs and has asked a solutions architect to recommend the most cost-effective storage option that does not sacrifice performance.Which solution should the solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Amazon Elastic Block Store (Amazon EBS) Cold HDD (sc1)",
            "Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2)",
            "Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS SSD (io1)",
            "Amazon Elastic Block Store (Amazon EBS) Throughput Optimized HDD (st1)"
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #117\n\nA company's application hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Due to data sensitivity, traffic cannot traverse the internet.How should a solutions architect configure access?",
        "answers": [
            1
        ],
        "options": [
            "Create a private hosted zone using Amazon Route 53.",
            "Configure a VPC gateway endpoint for Amazon S3 in the VPC.",
            "Configure AWS PrivateLink between the EC2 instance and the S3 bucket.",
            "Set up a site-to-site VPN connection between the VPC and the S3 bucket."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #118\n\nA company has two applications it wants to migrate to AWS. Both applications process a large set of files by accessing the same files at the same time. Both applications need to read the files with low latency.Which architecture should a solutions architect recommend for this situation?",
        "answers": [
            3
        ],
        "options": [
            "Configure two AWS Lambda functions to run the applications. Create an Amazon EC2 instance with an instance store volume to store the data.",
            "Configure two AWS Lambda functions to run the applications. Create an Amazon EC2 instance with an Amazon Elastic Block Store (Amazon EBS) volume to store the data.",
            "Configure one memory optimized Amazon EC2 instance to run both applications simultaneously. Create an Amazon Elastic Block Store (Amazon EBS) volume with Provisioned IOPS to store the data.",
            "Configure two Amazon EC2 instances to run both applications. Configure Amazon Elastic File System (Amazon EFS) with General Purpose performance mode and Bursting Throughput mode to store the data."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #119\n\nAn ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance degradation is attributed to an increase in the number of read-only SQL queries triggered by business analysts. A solutions architect needs to solve the problem with minimal changes to the existing web application.What should the solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "Export the data to Amazon DynamoDB and have the business analysts run their queries.",
            "Load the data into Amazon ElastiCache and have the business analysts run their queries.",
            "Create a read replica of the primary database and have the business analysts run their queries.",
            "Copy the data into an Amazon Redshift cluster and have the business analysts run their queries."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #120\n\nA company is running a highly sensitive application on Amazon EC2 backed by an Amazon RDS database. Compliance regulations mandate that all personally identifiable information (PII) be encrypted at rest.Which solution should a solutions architect recommend to meet this requirement with the LEAST amount of changes to the infrastructure?",
        "answers": [
            3
        ],
        "options": [
            "Deploy AWS Certificate Manager to generate certificates. Use the certificates to encrypt the database volume.",
            "Deploy AWS CloudHSM, generate encryption keys, and use the keys to encrypt database volumes.",
            "Configure SSL encryption using AWS Key Management Service (AWS KMS) to encrypt database volumes.",
            "Configure Amazon Elastic Block Store (Amazon EBS) encryption and Amazon RDS encryption with AWS Key Management Service (AWS KMS) keys to encrypt instance and database volumes."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #121\n\nA company running an on-premises application is migrating the application to AWS to increase its elasticity and availability. The current architecture uses aMicrosoft SQL Server database with heavy read activity. The company wants to explore alternate database options and migrate database engines, if needed.Every 4 hours, the development team does a full copy of the production database to populate a test database. During this period, users experience latency.What should a solutions architect recommend as replacement database?",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon Aurora with Multi-AZ Aurora Replicas and restore from mysqldump for the test database.",
            "Use Amazon Aurora with Multi-AZ Aurora Replicas and restore snapshots from Amazon RDS for the test database.",
            "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas, and use the standby instance for the test database.",
            "Use Amazon RDS for SQL Server with a Multi-AZ deployment and read replicas, and restore snapshots from RDS for the test database."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #122\n\nA company has enabled AWS CloudTrail logs to deliver log files to an Amazon S3 bucket for each of its developer accounts. The company has created a centralAWS account for streamlining management and audit reviews. An internal auditor needs to access the CloudTrail logs, yet access needs to be restricted for all developer account users. The solution must be secure and optimized.How should a solutions architect meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Configure an AWS Lambda function in each developer account to copy the log files to the central account. Create an IAM role in the central account for the auditor. Attach an IAM policy providing read-only permissions to the bucket.",
            "Configure CloudTrail from each developer account to deliver the log files to an S3 bucket in the central account. Create an IAM user in the central account for the auditor. Attach an IAM policy providing full permissions to the bucket.",
            "Configure CloudTrail from each developer account to deliver the log files to an S3 bucket in the central account. Create an IAM role in the central account for the auditor. Attach an IAM policy providing read-only permissions to the bucket.",
            "Configure an AWS Lambda function in the central account to copy the log files from the S3 bucket in each developer account. Create an IAM user in the central account for the auditor. Attach an IAM policy providing full permissions to the bucket."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #123\n\nA company has several business systems that require access to data stored in a file share. The business systems will access the file share using the ServerMessage Block (SMB) protocol. The file share solution should be accessible from both of the company's legacy on-premises environments and with AWS.Which services meet the business requirements? ",
        "answers": [
            2,
            4
        ],
        "options": [
            "Amazon Elastic Block Store (Amazon EBS)",
            "Amazon Elastic File System (Amazon EFS)",
            "Amazon FSx for Windows",
            "Amazon S3",
            "AWS Storage Gateway file gateway"
        ],
        "explination": "Correct Answer: CE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #124\n\nA company is using Amazon EC2 to run its big data analytics workloads. These variable workloads run each night, and it is critical they finish by the start of business the following day. A solutions architect has been tasked with designing the MOST cost-effective solution.Which solution will accomplish this?",
        "answers": [
            0
        ],
        "options": [
            "Spot Fleet",
            "Spot Instances",
            "Reserved Instances",
            "On-Demand Instances"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #125\n\nA company has a Microsoft Windows-based application that must be migrated to AWS. This application requires the use of a shared Windows file system attached to multiple Amazon EC2 Windows instances.What should a solutions architect do to accomplish this?",
        "answers": [
            2
        ],
        "options": [
            "Configure a volume using Amazon Elastic File System (Amazon EFS). Mount the EFS volume to each Windows instance.",
            "Configure AWS Storage Gateway in Volume Gateway mode. Mount the volume to each Windows instance.",
            "Configure Amazon FSx for Windows File Server. Mount the Amazon FSx volume to each Windows instance.",
            "Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size. Attach each EC2 instance to the volume. Mount the file system within the volume to each Windows instance."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #126\n\nA company has created an isolated backup of its environment in another Region. The application is running in warm standby mode and is fronted by anApplication Load Balancer (ALB). The current failover process is manual and requires updating a DNS alias record to point to the secondary ALB in anotherRegion.What should a solutions architect do to automate the failover process?",
        "answers": [
            2
        ],
        "options": [
            "Enable an ALB health check",
            "Enable an Amazon Route 53 health check.",
            "Crate an CNAME record on Amazon Route 53 pointing to the ALB endpoint.",
            "Create conditional forwarding rules on Amazon Route 53 pointing to an internal BIND DNS server."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #127\n\nA company has a mobile chat application with a data store based in Amazon DynamoDB. Users would like new messages to be read with as little latency as possible. A solutions architect needs to design an optimal solution that requires minimal application changes.Which method should the solutions architect select?",
        "answers": [
            0
        ],
        "options": [
            "Configure Amazon DynamoDB Accelerator (DAX) for the new messages table. Update the code to use the DAX endpoint.",
            "Add DynamoDB read replicas to handle the increased read load. Update the application to point to the read endpoint for the read replicas.",
            "Double the number of read capacity units for the new messages table in DynamoDB. Continue to use the existing DynamoDB endpoint.",
            "Add an Amazon ElastiCache for Redis cache to the application stack. Update the application to point to the Redis cache endpoint instead of DynamoDB."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/blogs/aws/amazon-dynamodb-accelerator-dax-in-memory-caching-for-read-intensive-workloads/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #128\n\nA company is creating an architecture for a mobile app that requires minimal latency for its users. The company's architecture consists of Amazon EC2 instances behind an Application Load Balancer running in an Auto Scaling group. The EC2 instances connect to Amazon RDS. Application beta testing showed there was a slowdown when reading the data. However, the metrics indicate that the EC2 instances do not cross any CPU utilization thresholds.How can this issue be addressed?",
        "answers": [
            2
        ],
        "options": [
            "Reduce the threshold for CPU utilization in the Auto Scaling group.",
            "Replace the Application Load Balancer with a Network Load Balancer.",
            "Add read replicas for the RDS instances and direct read traffic to the replica.",
            "Add Multi-AZ support to the RDS instances and direct read traffic to the new EC2 instance."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0,
        "tags": [
            "SQS"
        ],
        "attempt_history": []
    },
    {
        "question": "Question #130\n\nA company hosts its website on Amazon S3. The website serves petabytes of outbound traffic monthly, which accounts for most of the company's AWS costs.What should a solutions architect do to reduce costs?",
        "answers": [
            0
        ],
        "options": [
            "Configure Amazon CloudFront with the existing website as the origin.",
            "Move the website to Amazon EC2 with Amazon Elastic Block Store (Amazon EBS) volumes for storage.",
            "Use AWS Global Accelerator and specify the existing website as the endpoint.",
            "Rearchitect the website to run on a combination of Amazon API Gateway and AWS Lambda."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #131\n\nA company runs a website on Amazon EC2 instances behind an ELB Application Load Balancer. Amazon Route 53 is used for the DNS. The company wants to set up a backup website with a message including a phone number and email address that users can reach if the primary website is down.How should the company deploy this solution?",
        "answers": [
            0
        ],
        "options": [
            "Use Amazon S3 website hosting for the backup website and Route 53 failover routing policy.",
            "Use Amazon S3 website hosting for the backup website and Route 53 latency routing policy.",
            "Deploy the application in another AWS Region and use ELB health checks for failover routing.",
            "Deploy the application in another AWS Region and use server-side redirection on the primary website."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #132\n\nA media company is evaluating the possibility of moving its systems to the AWS Cloud. The company needs at least 10 TB of storage with the maximum possibleI/O performance for video processing, 300 TB of very durable storage for storing media content, and 900 TB of storage to meet requirements for archival media that is not in use anymore.Which set of services should a solutions architect recommend to meet these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Amazon Elastic Block Store (Amazon EBS) for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage",
            "Amazon Elastic Block Store (Amazon EBS) for maximum performance, Amazon Elastic File System (Amazon EFS) for durable data storage, and Amazon S3 Glacier for archival storage",
            "Amazon EC2 instance store for maximum performance, Amazon Elastic File System (Amazon EFS) for durable data storage, and Amazon S3 for archival storage",
            "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #133\n\nA company uses Amazon S3 as its object storage solution. The company has thousands of S3 buckets it uses to store data. Some of the S3 buckets have data that is accessed less frequently than others. A solutions architect found that lifecycle policies are not consistently implemented or are implemented partially, resulting in data being stored in high-cost storage.Which solution will lower costs without compromising the availability of objects?",
        "answers": [
            2
        ],
        "options": [
            "Use S3 ACLs.",
            "Use Amazon Elastic Block Store (Amazon EBS) automated snapshots.",
            "Use S3 Intelligent-Tiering storage.",
            "Use S3 One Zone-Infrequent Access (S3 One Zone-IA)."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #134\n\nAn application is running on Amazon EC2 instances. Sensitive information required for the application is stored in an Amazon S3 bucket. The bucket needs to be protected from internet access while only allowing services within the VPC access to the bucket.Which combination of actions should solutions archived take to accomplish this? ",
        "answers": [
            0,
            2
        ],
        "options": [
            "Create a VPC endpoint for Amazon S3.",
            "Enable server access logging on the bucket.",
            "Apply a bucket policy to restrict access to the S3 endpoint.",
            "Add an S3 ACL to the bucket that has sensitive information.",
            "Restrict users using the IAM policy to use the specific bucket."
        ],
        "explination": "Correct Answer: AC \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #135\n\nA web application runs on Amazon EC2 instances behind an Application Load Balancer. The application allows users to create custom reports of historical weather data. Generating a report can take up to 5 minutes. These long-running requests use many of the available incoming connections, making the system unresponsive to other users.How can a solutions architect make the system more responsive?",
        "answers": [
            0
        ],
        "options": [
            "Use Amazon SQS with AWS Lambda to generate reports.",
            "Increase the idle timeout on the Application Load Balancer to 5 minutes.",
            "Update the client-side application code to increase its request timeout to 5 minutes.",
            "Publish the reports to Amazon S3 and use Amazon CloudFront for downloading to the user."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #136\n\nA solutions architect must create a highly available bastion host architecture. The solution needs to be resilient within a single AWS Region and should require only minimal effort to maintain.What should the solutions architect do to meet these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Create a Network Load Balancer backed by an Auto Scaling group with a UDP listener.",
            "Create a Network Load Balancer backed by a Spot Fleet with instances in a partition placement group.",
            "Create a Network Load Balancer backed by the existing servers in different Availability Zones as the target.",
            "Create a Network Load Balancer backed by an Auto Scaling group with instances in multiple Availability Zones as the target."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #137\n\nA three-tier web application processes orders from customers. The web tier consists of Amazon EC2 instances behind an Application Load Balancer, a middle tier of three EC2 instances decoupled from the web tier using Amazon SQS, and an Amazon DynamoDB backend. At peak times, customers who submit orders using the site have to wait much longer than normal to receive confirmations due to lengthy processing times. A solutions architect needs to reduce these processing times.Which action will be MOST effective in accomplishing this?",
        "answers": [
            3
        ],
        "options": [
            "Replace the SQS queue with Amazon Kinesis Data Firehose.",
            "Use Amazon ElastiCache for Redis in front of the DynamoDB backend tier.",
            "Add an Amazon CloudFront distribution to cache the responses for the web tier.",
            "Use Amazon EC2 Auto Scaling to scale out the middle tier instances based on the SQS queue depth."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0,
        "tags": [
            "SQS"
        ],
        "attempt_history": [
            true
        ]
    },
    {
        "question": "Question #138\n\nA company relies on an application that needs at least 4 Amazon EC2 instances during regular traffic and must scale up to 12 EC2 instances during peak loads.The application is critical to the business and must be highly available.Which solution will meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Deploy the EC2 instances in an Auto Scaling group. Set the minimum to 4 and the maximum to 12, with 2 in Availability Zone A and 2 in Availability Zone B.",
            "Deploy the EC2 instances in an Auto Scaling group. Set the minimum to 4 and the maximum to 12, with all 4 in Availability Zone A.",
            "Deploy the EC2 instances in an Auto Scaling group. Set the minimum to 8 and the maximum to 12, with 4 in Availability Zone A and 4 in Availability Zone B.",
            "Deploy the EC2 instances in an Auto Scaling group. Set the minimum to 8 and the maximum to 12, with all 8 in Availability Zone A."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #139\n\nA solutions architect must design a solution for a persistent database that is being migrated from on-premises to AWS. The database requires 64,000 IOPS according to the database administrator. If possible, the database administrator wants to use a single Amazon Elastic Block Store (Amazon EBS) volume to host the database instance.Which solution effectively meets the database administrator's criteria?",
        "answers": [
            1
        ],
        "options": [
            "Use an instance from the I3 I/O optimized family and leverage local ephemeral storage to achieve the IOPS requirement.",
            "Create a Nitro-based Amazon EC2 instance with an Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS SSD (io1) volume attached. Configure the volume to have 64,000 IOPS.",
            "Create and map an Amazon Elastic File System (Amazon EFS) volume to the database instance and use the volume to achieve the required IOPS for the database.",
            "Provision two volumes and assign 32,000 IOPS to each. Create a logical volume at the operating system level that aggregates both volumes to achieve the IOPS requirements."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #140\n\nA solutions architect is designing an architecture for a new application that requires low network latency and high network throughput between Amazon EC2 instances. Which component should be included in the architectural design?",
        "answers": [
            1
        ],
        "options": [
            "An Auto Scaling group with Spot Instance types.",
            "A placement group using a cluster placement strategy.",
            "A placement group using a partition placement strategy.",
            "An Auto Scaling group with On-Demand instance types."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #141\n\nA company has global users accessing an application deployed in different AWS Regions, exposing public static IP addresses. The users are experiencing poor performance when accessing the application over the internet.What should a solutions architect recommend to reduce internet latency?",
        "answers": [
            0
        ],
        "options": [
            "Set up AWS Global Accelerator and add endpoints.",
            "Set up AWS Direct Connect locations in multiple Regions.",
            "Set up an Amazon CloudFront distribution to access an application.",
            "Set up an Amazon Route 53 geoproximity routing policy to route traffic."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #142\n\nA company wants to migrate a workload to AWS. The chief information security officer requires that all data be encrypted at rest when stored in the cloud. The company wants complete control of encryption key lifecycle management.The company must be able to immediately remove the key material and audit key usage independently of AWS CloudTrail. The chosen services should integrate with other storage services that will be used on AWS.Which services satisfies these security requirements?",
        "answers": [
            0
        ],
        "options": [
            "AWS CloudHSM with the CloudHSM client",
            "AWS Key Management Service (AWS KMS) with AWS CloudHSM",
            "AWS Key Management Service (AWS KMS) with an external key material origin",
            "AWS Key Management Service (AWS KMS) with AWS managed customer master keys (CMKs)"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #143\n\nA company recently deployed a two-tier application in two Availability Zones in the us-east-1 Region. The databases are deployed in a private subnet while the web servers are deployed in a public subnet. An internet gateway is attached to the VPC. The application and database run on Amazon EC2 instances. The database servers are unable to access patches on the internet. A solutions architect needs to design a solution that maintains database security with the least operational overhead.Which solution meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Deploy a NAT gateway inside the public subnet for each Availability Zone and associate it with an Elastic IP address. Update the routing table of the private subnet to use it as the default route.",
            "Deploy a NAT gateway inside the private subnet for each Availability Zone and associate it with an Elastic IP address. Update the routing table of the private subnet to use it as the default route.",
            "Deploy two NAT instances inside the public subnet for each Availability Zone and associate them with Elastic IP addresses. Update the routing table of the private subnet to use it as the default route.",
            "Deploy two NAT instances inside the private subnet for each Availability Zone and associate them with Elastic IP addresses. Update the routing table of the private subnet to use it as the default route."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #144\n\nA company has an application with a REST-based interface that allows data to be received in near-real time from a third-party vendor. Once received, the application processes and stores the data for further analysis. The application is running on Amazon EC2 instances.The third-party vendor has received many 503 Service Unavailable Errors when sending data to the application. When the data volume spikes, the compute capacity reaches its maximum limit and the application is unable to process all requests.Which design should a solutions architect recommend to provide a more scalable solution?",
        "answers": [
            0
        ],
        "options": [
            "Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.",
            "Use Amazon API Gateway on top of the existing application. Create a usage plan with a quota limit for the third-party vendor.",
            "Use Amazon Simple Notification Service (Amazon SNS) to ingest the data. Put the EC2 instances in an Auto Scaling group behind an Application Load Balancer.",
            "Repackage the application as a container. Deploy the application using Amazon Elastic Container Service (Amazon ECS) using the EC2 launch type with an Auto Scaling group."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #145\n\nA solutions architect needs to design a low-latency solution for a static single-page application accessed by users utilizing a custom domain name. The solution must be serverless, encrypted in transit, and cost-effective.Which combination of AWS services and features should the solutions architect use? ",
        "answers": [
            0,
            3
        ],
        "options": [
            "Amazon S3",
            "Amazon EC2",
            "AWS Fargate",
            "Amazon CloudFront",
            "Elastic Load Balancer"
        ],
        "explination": "Correct Answer: AD \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #146\n\nA company is migrating to the AWS Cloud. A file server is the first workload to migrate. Users must be able to access the file share using the Server MessageBlock (SMB) protocol. Which AWS managed service meets these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Amazon Elastic Block Store (Amazon EBS)",
            "Amazon EC2",
            "Amazon FSx",
            "Amazon S3"
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #147\n\nA solutions architect is designing a customer-facing application. The application is expected to have a variable amount of reads and writes depending on the time of year and clearly defined access patterns throughout the year. Management requires that database auditing and scaling be managed in the AWS Cloud. TheRecovery Point Objective (RPO) must be less than 5 hours.Which solutions can accomplish this? ",
        "answers": [
            0,
            1
        ],
        "options": [
            "Use Amazon DynamoDB with auto scaling. Use on-demand backups and AWS CloudTrail.",
            "Use Amazon DynamoDB with auto scaling. Use on-demand backups and Amazon DynamoDB Streams.",
            "Use Amazon Redshift Configure concurrency scaling. Enable audit logging. Perform database snapshots every 4 hours.",
            "Use Amazon RDS with Provisioned IOPS. Enable the database auditing parameter. Perform database snapshots every 5 hours.",
            "Use Amazon RDS with auto scaling. Enable the database auditing parameter. Configure the backup retention period to at least 1 day."
        ],
        "explination": "Correct Answer: AB \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #148\n\nA company has migrated an on-premises Oracle database to an Amazon RDS for Oracle Multi-AZ DB instance in the us-east-l Region. A solutions architect is designing a disaster recovery strategy to have the database provisioned in the us-west-2 Region in case the database becomes unavailable in the us-east-1Region. The design must ensure the database is provisioned in the us-west-2 Region in a maximum of 2 hours, with a data loss window of no more than 3 hours.How can these requirements be met?",
        "answers": [
            1
        ],
        "options": [
            "Edit the DB instance and create a read replica in us-west-2. Promote the read replica to master in us-west-2 in case the disaster recovery environment needs to be activated.",
            "Select the multi-Region option to provision a standby instance in us-west-2. The standby instance will be automatically promoted to master in us-west-2 in case the disaster recovery environment needs to be created.",
            "Take automated snapshots of the database instance and copy them to us-west-2 every 3 hours. Restore the latest snapshot to provision another database instance in us-west-2 in case the disaster recovery environment needs to be activated.",
            "Create a multimaster read/write instances across multiple AWS Regions. Select VPCs in us-east-1 and us-west-2 to make that deployment. Keep the master read/write instance in us-west-2 available to avoid having to activate a disaster recovery environment."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #149\n\nA monolithic application was recently migrated to AWS and is now running on a single Amazon EC2 instance. Due to application limitations, it is not possible to use automatic scaling to scale out the application. The chief technology officer (CTO) wants an automated solution to restore the EC2 instance in the unlikely event the underlying hardware fails.What would allow for automatic recovery of the EC2 instance as quickly as possible?",
        "answers": [
            0
        ],
        "options": [
            "Configure an Amazon CloudWatch alarm that triggers the recovery of the EC2 instance if it becomes impaired.",
            "Configure an Amazon CloudWatch alarm to trigger an SNS message that alerts the CTO when the EC2 instance is impaired.",
            "Configure AWS CloudTrail to monitor the health of the EC2 instance, and if it becomes impaired, trigger instance recovery.",
            "Configure an Amazon EventBridge event to trigger an AWS Lambda function once an hour that checks the health of the EC2 instance and triggers instance recovery if the EC2 instance is unhealthy."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #150\n\nA solutions architect is working on optimizing a legacy document management application running on Microsoft Windows Server in an on-premises data center.The application stores a large number of files on a network file share. The chief information officer wants to reduce the on-premises data center footprint and minimize storage costs by moving on-premises storage to AWS.What should the solutions architect do to meet these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Set up an AWS Storage Gateway file gateway.",
            "Set up Amazon Elastic File System (Amazon EFS)",
            "Set up AWS Storage Gateway as a volume gateway",
            "Set up an Amazon Elastic Block Store (Amazon EBS) volume."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #151\n\nA solutions architect is designing a hybrid application using the AWS cloud. The network between the on-premises data center and AWS will use an AWS DirectConnect (DX) connection. The application connectivity between AWS and the on-premises data center must be highly resilient.Which DX configuration should be implemented to meet these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Configure a DX connection with a VPN on top of it.",
            "Configure DX connections at multiple DX locations.",
            "Configure a DX connection using the most reliable DX partner.",
            "Configure multiple virtual interfaces on top of a DX connection."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #152\n\nA company runs an application on Amazon EC2 instances. The application is deployed in private subnets in three Availability Zones of the us-east-1 Region. The instances must be able to connect to the internet to download files. The company wants a design that is highly available across the Region.Which solution should be implemented to ensure that there are no disruptions to internet connectivity?",
        "answers": [
            1
        ],
        "options": [
            "Deploy a NAT instance in a private subnet of each Availability Zone.",
            "Deploy a NAT gateway in a public subnet of each Availability Zone.",
            "Deploy a transit gateway in a private subnet of each Availability Zone.",
            "Deploy an internet gateway in a public subnet of each Availability Zone."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #153\n\nApplication developers have noticed that a production application is very slow when business reporting users run large production reports against the AmazonRDS instance backing the application. The CPU and memory utilization metrics for the RDS instance do not exceed 60% while the reporting queries are running.The business reporting users must be able to generate reports without affecting the application's performance.Which action will accomplish this?",
        "answers": [
            3
        ],
        "options": [
            "Increase the size of the RDS instance.",
            "Create a read replica and connect the application to it.",
            "Enable multiple Availability Zones on the RDS instance.",
            "Create a read replica and connect the business reports to it."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #154\n\nA company is running a two-tier ecommerce website using AWS services. The current architect uses a publish-facing Elastic Load Balancer that sends traffic toAmazon EC2 instances in a private subnet. The static content is hosted on EC2 instances, and the dynamic content is retrieved from a MySQL database. The application is running in the United States. The company recently started selling to users in Europe and Australia. A solutions architect needs to design solution so their international users have an improved browsing experience.Which solution is MOST cost-effective?",
        "answers": [
            1
        ],
        "options": [
            "Host the entire website on Amazon S3.",
            "Use Amazon CloudFront and Amazon S3 to host static images.",
            "Increase the number of public load balancers and EC2 instances.",
            "Deploy the two-tier website in AWS Regions in Europe and Australia."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #155\n\nA company's website provides users with downloadable historical performance reports. The website needs a solution that will scale to meet the company's website demands globally. The solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time.Which combination should a solutions architect recommend to meet these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Amazon CloudFront and Amazon S3",
            "AWS Lambda and Amazon DynamoDB",
            "Application Load Balancer with Amazon EC2 Auto Scaling",
            "Amazon Route 53 with internal Application Load Balancers"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #156\n\nA company wants to deploy a shared file system for its .NET application servers and Microsoft SQL Server databases running on Amazon EC2 instances withWindows Server 2016. The solution must be able to be integrated into the corporate Active Directory domain, be highly durable, be managed by AWS, and provide high levels of throughput and IOPS.Which solution meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Use Amazon FSx for Windows File Server.",
            "Use Amazon Elastic File System (Amazon EFS).",
            "Use AWS Storage Gateway in file gateway mode.",
            "Deploy a Windows file server on two On Demand instances across two Availability Zones."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #157\n\nA company that develops web applications has launched hundreds of Application Load Balancers (ALBs) in multiple Regions. The company wants to create an allow list for the IPs of all the load balancers on its firewall device. A solutions architect is looking for a one-time, highly available solution to address this request, which will also help reduce the number of IPs that need to be allowed by the firewall.What should the solutions architect recommend to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Create a AWS Lambda function to keep track of the IPs for all the ALBs in different Regions. Keep refreshing this list.",
            "Set up a Network Load Balancer (NLB) with Elastic IPs. Register the private IPs of all the ALBs as targets to this NLB.",
            "Launch AWS Global Accelerator and create endpoints for all the Regions. Register all the ALBs in different Regions to the corresponding endpoints.",
            "Set up an Amazon EC2 instance, assign an Elastic IP to this EC2 instance, and configure the instance as a proxy to forward traffic to all the ALBs."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #158\n\nA company runs an application using Amazon ECS. The application creates resized versions of an original image and then makes Amazon S3 API calls to store the resized images in Amazon S3. How can a solutions architect ensure that the application has permission to access Amazon S3?",
        "answers": [
            1
        ],
        "options": [
            "Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch the container.",
            "Create an IAM role with S3 permissions, and then specify that role as the taskRoleArn in the task definition.",
            "Create a security group that allows access from Amazon ECS to Amazon S3, and update the launch configuration used by the ECS cluster.",
            "Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the ECS cluster while logged in as this account."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #159\n\nA company is planning to migrate its virtual server-based workloads to AWS. The company has internet-facing load balancers backed by application servers. The application servers rely on patches from an internet-hosted repository.Which services should a solutions architect recommend be hosted on the public subnet? ",
        "answers": [
            0,
            2
        ],
        "options": [
            "NAT gateway",
            "Amazon RDS DB instances",
            "Application Load Balancers",
            "Amazon EC2 application servers",
            "Amazon Elastic File System (Amazon EFS) volumes"
        ],
        "explination": "Correct Answer: AC \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #160\n\nA company has established a new AWS account. The account is newly provisioned and no changed have been made to the default settings. The company is concerned about the security of the AWS account root user.What should be done to secure the root user?",
        "answers": [
            1
        ],
        "options": [
            "Create IAM users for daily administrative tasks. Disable the root user.",
            "Create IAM users for daily administrative tasks. Enable multi-factor authentication on the root user.",
            "Generate an access key for the root user. Use the access key for daily administration tasks instead of the AWS Management Console.",
            "Provide the root user credentials to the most senior solutions architect. Have the solutions architect use the root user for daily administration tasks."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #161\n\nA company is using a tape backup solution to store its key application data offsite. The daily data volume is around 50 TB. The company needs to retain the backups for 7 years for regulatory purposes. The backups are rarely accessed, and a week's notice is typically given if a backup needs to be restored.The company is now considering a cloud-based option to reduce the storage costs and operational burden of managing tapes. The company also wants to make sure that the transition from tape backups to the cloud minimizes disruptions.Which storage solution is MOST cost-effective?",
        "answers": [
            0
        ],
        "options": [
            "Use Amazon Storage Gateway to back up to Amazon Glacier Deep Archive.",
            "Use AWS Snowball Edge to directly integrate the backups with Amazon S3 Glacier.",
            "Copy the backup data to Amazon S3 and create a lifecycle policy to move the data to Amazon S3 Glacier.",
            "Use Amazon Storage Gateway to back up to Amazon S3 and create a lifecycle policy to move the backup to Amazon S3 Glacier."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #162\n\nA company requires a durable backup storage solution for its on-premises database servers while ensuring on-premises applications maintain access to these backups for quick recovery. The company will use AWS storage services as the destination for these backups. A solutions architect is designing a solution with minimal operational overhead.Which solution should the solutions architect implement?",
        "answers": [
            0
        ],
        "options": [
            "Deploy an AWS Storage Gateway file gateway on-premises and associate it with an Amazon S3 bucket.",
            "Back up the databases to an AWS Storage Gateway volume gateway and access it using the Amazon S3 API.",
            "Transfer the database backup files to an Amazon Elastic Block Store (Amazon EBS) volume attached to an Amazon EC2 instance.",
            "Back up the database directly to an AWS Snowball device and use lifecycle rules to move the data to Amazon S3 Glacier Deep Archive."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #163\n\nA company decides to migrate its three-tier web application from on-premises to the AWS Cloud. The new database must be capable of dynamically scaling storage capacity and performing table joins.Which AWS service meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Amazon Aurora",
            "Amazon RDS for SqlServer",
            "Amazon DynamoDB Streams",
            "Amazon DynamoDB on-demand"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #164\n\nA company mandates that an Amazon S3 gateway endpoint must allow traffic to trusted buckets only.Which method should a solutions architect implement to meet this requirement?",
        "answers": [
            3
        ],
        "options": [
            "Create a bucket policy for each of the company's trusted S3 buckets that allows traffic only from the company's trusted VPCs.",
            "Create a bucket policy for each of the company's trusted S3 buckets that allows traffic only from the company's S3 gateway endpoint IDs.",
            "Create an S3 endpoint policy for each of the company's S3 gateway endpoints that blocks access from any VPC other than the company's trusted VPCs.",
            "Create an S3 endpoint policy for each of the company's S3 gateway endpoints that provides access to the Amazon Resource Name (ARN) of the trusted S3 buckets."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #165\n\nA company is using a VPC peering strategy to connect its VPCs in a single Region to allow for cross-communication. A recent increase in account creations andVPCs has made it difficult to maintain the VPC peering strategy, and the company expects to grow to hundreds of VPCs. There are also new requests to create site-to-site VPNs with some of the VPCs. A solutions architect has been tasked with creating a centrally managed networking setup for multiple accounts, VPCs, and VPNs.Which networking solution meets these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Configure shared VPCs and VPNs and share to each other.",
            "Configure a hub-and-spoke VPC and route all traffic through VPC peering.",
            "Configure an AWS Direct Connect connection between all VPCs and VPNs.",
            "Configure a transit gateway with AWS Transit Gateway and connect all VPCs and VPNs."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #166\n\nA solutions architect is helping a developer design a new ecommerce shopping cart application using AWS services. The developer is unsure of the current database schema and expects to make changes as the ecommerce site grows. The solution needs to be highly resilient and capable of automatically scaling read and write capacity.Which database solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Amazon Aurora PostgreSQL",
            "Amazon DynamoDB with on-demand enabled",
            "Amazon DynamoDB with DynamoDB Streams enabled",
            "Amazon SQS and Amazon Aurora PostgreSQL"
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #167\n\nA solutions architect must migrate a Windows internet information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The solutions architected has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and configuring an Elastic Load Balancer attached to the instances.Which replacement to the on-premises file share is MOST resilient and durable?",
        "answers": [
            2
        ],
        "options": [
            "Migrate the file Share to Amazon RDS.",
            "Migrate the file Share to AWS Storage Gateway",
            "Migrate the file Share to Amazon FSx for Windows File Server.",
            "Migrate the file share to Amazon Elastic File System (Amazon EFS)"
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #168\n\nA company needs to implement a relational database with a multi-Region disaster recovery Recovery Point Objective (RPO) of 1 second and a Recovery TimeObjective (RTO) of 1 minute.Which AWS solution can achieve this?",
        "answers": [
            0
        ],
        "options": [
            "Amazon Aurora Global Database",
            "Amazon DynamoDB global tables",
            "Amazon RDS for MySQL with Multi-AZ enabled",
            "Amazon RDS for MySQL with a cross-Region snapshot copy"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #169\n\nA company runs a web service on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across two Availability Zones. The company needs a minimum of four instances at all times to meet the required service level agreement (SLA) while keeping costs low.If an Availability Zone fails, how can the company remain compliant with the SLA?",
        "answers": [
            0
        ],
        "options": [
            "Add a target tracking scaling policy with a short cooldown period.",
            "Change the Auto Scaling group launch configuration to use a larger instance type.",
            "Change the Auto Scaling group to use six servers across three Availability Zones.",
            "Change the Auto Scaling group to use eight servers across two Availability Zones."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #170\n\nA company is reviewing its AWS Cloud deployment to ensure its data is not accessed by anyone without appropriate authorization. A solutions architect is tasked with identifying all open Amazon S3 buckets and recording any S3 bucket configuration changes.What should the solutions architect do to accomplish this?",
        "answers": [
            0
        ],
        "options": [
            "Enable AWS Config service with the appropriate rules",
            "Enable AWS Trusted Advisor with the appropriate checks.",
            "Write a script using an AWS SDK to generate a bucket report",
            "Enable Amazon S3 server access logging and configure Amazon CloudWatch Events."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #171\n\nA company is planning to build a new web application on AWS. The company expects predictable traffic most of the year and very high traffic on occasion. The web application needs to be highly available and fault tolerant with minimal latency.What should a solutions architect recommend to meet these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Use an Amazon Route 53 routing policy to distribute requests to two AWS Regions, each with one Amazon EC2 instance.",
            "Use Amazon EC2 instances in an Auto Scaling group with an Application Load Balancer across multiple Availability Zones.",
            "Use Amazon EC2 instances in a cluster placement group with an Application Load Balancer across multiple Availability Zones.",
            "Use Amazon EC2 instances in a cluster placement group and include the cluster placement group within a new Auto Scaling group."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #172\n\nA company is designing a web application using AWS that processes insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type must be responded to within 24 hours, and must not be lost. The solution should be simple to set up and maintain.Which solution meets these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to pool messages from its own data stream using the Kinesis Client Library (KCL).",
            "Create multiple Amazon Simple Notification Service (Amazon SNS) topics and register Amazon SQS queues to their own SNS topic based on the quote type. Configure the web application to publish messages to the SNS topic queue. Configure each backend application server to work its own SQS queue.",
            "Create a single Amazon Simple Notification Service (Amazon SNS) topic and subscribe the Amazon SQS queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to work its own SQS queue.",
            "Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an Amazon Elasticsearch Service (Amazon ES) cluster. Configure the web application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from Amazon ES and process them accordingly."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #173\n\nA solutions architect has configured the following IAM policy.Which action will be allowed by the policy?",
        "answers": [
            2
        ],
        "options": [
            "An AWS Lambda function can be deleted from any network.",
            "An AWS Lambda function can be created from any network.",
            "An AWS Lambda function can be deleted from the 100.220.0.0/20 network.",
            "An AWS Lambda function can be deleted from the 220.100.16.0/20 network."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #174\n\nA solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. The media files must be resilient to the loss of anAvailability Zone. Some files are accessed frequently while other files are rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of storing and retrieving the media files.Which storage option meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "S3 Standard",
            "S3 Intelligent-Tiering",
            "S3 Standard-Infrequent Access (S3 Standard-IA)",
            "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #175\n\nA company is running a three-tier web application to process credit card payments. The front-end user interface consists of static webpages. The application tier can have long-running processes. The database tier uses MySQL.The application is currently running on a single, general purpose large Amazon EC2 instance. A solutions architect needs to decouple the services to make the web application highly available.Which solution would provide the HIGHEST availability?",
        "answers": [
            0
        ],
        "options": [
            "Move static assets to Amazon CloudFront. Leave the application in EC2 in an Auto Scaling group. Move the database to Amazon RDS to deploy Multi-AZ.",
            "Move static assets and the application into a medium EC2 instance. Leave the database on the large instance. Place both instances in an Auto Scaling group.",
            "Move static assets to Amazon S3. Move the application to AWS Lambda with the concurrency limit set. Move the database to Amazon DynamoDB with on- demand enabled.",
            "Move static assets to Amazon S3. Move the application to Amazon Elastic Container Service (Amazon ECS) containers with Auto Scaling enabled. Move the database to Amazon RDS to deploy Multi-AZ."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #176\n\nA media company stores video content in an Amazon Elastic Block Store (Amazon EBS) volume. A certain video file has become popular and a large number of users across the world are accessing this content. This has resulted in a cost increase.Which action will DECREASE cost without compromising user accessibility?",
        "answers": [
            1
        ],
        "options": [
            "Change the EBS volume to Provisioned IOPS (PIOPS).",
            "Store the video in an Amazon S3 bucket and create an Amazon CloudFront distribution.",
            "Split the video into multiple, smaller segments so users are routed to the requested video segments only.",
            "Clear an Amazon S3 bucket in each Region and upload the videos so users are routed to the nearest S3 bucket."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #177\n\nA solutions architect is designing the cloud architecture for a new application being deployed to AWS. The application allows users to interactively download and upload files. Files older than 2 years will be accessed less frequently. The solutions architect needs to ensure that the application can scale to any number of files while maintaining high availability and durability.Which scalable solutions should the solutions architect recommend? ",
        "answers": [
            0,
            2
        ],
        "options": [
            "Store the files on Amazon S3 with a lifecycle policy that moves objects older than 2 years to S3 Glacier.",
            "Store the files on Amazon S3 with a lifecycle policy that moves objects older than 2 years to S3 Standard-Infrequent Access (S3 Standard-IA)",
            "Store the files on Amazon Elastic File System (Amazon EFS) with a lifecycle policy that moves objects older than 2 years to EFS Infrequent Access (EFS IA).",
            "Store the files in Amazon Elastic Block Store (Amazon EBS) volumes. Schedule snapshots of the volumes. Use the snapshots to archive data older than 2 years.",
            "Store the files in RAID-striped Amazon Elastic Block Store (Amazon EBS) volumes. Schedule snapshots of the volumes. Use the snapshots to archive data older than 2 years."
        ],
        "explination": "Correct Answer: AC \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #178\n\nA company has recently updated its internal security standards. The company must now ensure all Amazon S3 buckets and Amazon Elastic Block Store (AmazonEBS) volumes are encrypted with keys created and periodically rotated by internal security specialists. The company is looking for a native, software-based AWS service to accomplish this goal.What should a solutions architect recommend as a solution?",
        "answers": [
            0
        ],
        "options": [
            "Use AWS Secrets Manager with customer master keys (CMKs) to store master key material and apply a routine to create a new CMK periodically and replace it in AWS Secrets Manager.",
            "Use AWS Key Management Service (AWS KMS) with customer master keys (CMKs) to store master key material and apply a routine to re-create a new key periodically and replace it in AWS KMS.",
            "Use an AWS CloudHSM cluster with customer master keys (CMKs) to store master key material and apply a routine to re-create a new key periodically and replace it in the CloudHSM cluster nodes.",
            "Use AWS Systems Manager Parameter Store with customer master keys (CMKs) to store master key material and apply a routine to re-create a new key periodically and replace it in the Parameter Store."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #179\n\nA company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed.What should the solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.",
            "Move the website to Amazon S3. Use cross-Region replication between Regions.",
            "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
            "Use an Amazon Route 53 geo-proximity routing policy pointing to on-premises servers."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #180\n\nA development team needs to host a website that will be accessed by other teams. The website contents consist of HTML, CSS, client-side JavaScript, and images.Which method is the MOST cost-effective for hosting the website?",
        "answers": [
            1
        ],
        "options": [
            "Containerize the website and host it in AWS Fargate.",
            "Create an Amazon S3 bucket and host the website there.",
            "Deploy a web server on an Amazon EC2 instance to host the website.",
            "Configure an Application Load Balancer with an AWS Lambda target that uses the Express.js framework."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #181\n\nA company is hosting multiple websites for several lines of business under its registered parent domain. Users accessing these websites will be routed to appropriate backend Amazon EC2 instances based on the subdomain. The websites host static webpages, images, and server-side scripts like PHP andJavaScript.Some of the websites experience peak access during the first two hours of business with constant usage throughout the rest of the day. A solutions architect needs to design a solution that will automatically adjust capacity to these traffic patterns while keeping costs low.Which combination of AWS services or features will meet these requirements? ",
        "answers": [
            2,
            3
        ],
        "options": [
            "AWS Batch",
            "Network Load Balancer",
            "Application Load Balancer",
            "Amazon EC2 Auto Scaling",
            "Amazon S3 website hosting"
        ],
        "explination": "Correct Answer: CD \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #182\n\nA company uses an Amazon S3 bucket to store static images for its website. The company configured permissions to allow access to Amazon S3 objects by privileged users only.What should a solutions architect do to protect against data loss? ",
        "answers": [
            0,
            4
        ],
        "options": [
            "Enable versioning on the S3 bucket.",
            "Enable access logging on the S3 bucket.",
            "Enable server-side encryption on the S3 bucket.",
            "Configure an S3 lifecycle rule to transition objects to Amazon S3 Glacier.",
            "Use MFA Delete to require multi-factor authentication to delete an object."
        ],
        "explination": "Correct Answer: AE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #183\n\nAn operations team has a standard that states IAM policies should not be applied directly to users. Some new team members have not been following this standard. The operations manager needs a way to easily identify the users with attached policies.What should a solutions architect do to accomplish this?",
        "answers": [
            2
        ],
        "options": [
            "Monitor using AWS CloudTrail.",
            "Create an AWS Config rule to run daily.",
            "Publish IAM user changes to Amazon SNS.",
            "Run AWS Lambda when a user is modified."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #184\n\nA company wants to use an AWS Region as a disaster recovery location for its on-premises infrastructure. The company has 10 TB of existing data, and the on- premise data center has a 1 Gbps internet connection. A solutions architect must find a solution so the company can have its existing data on AWS in 72 hours without transmitting it using an unencrypted channel.Which solution should the solutions architect select?",
        "answers": [
            2
        ],
        "options": [
            "Send the initial 10 TB of data to AWS using FTP.",
            "Send the initial 10 TB of data to AWS using AWS Snowball.",
            "Establish a VPN connection between Amazon VPC and the company's data center.",
            "Establish an AWS Direct Connect connection between Amazon VPC and the company's data center."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #185\n\nA company is building applications in containers. The company wants to migrate its on-premises development and operations services from its on-premises data center to AWS. Management states that production systems must be cloud agnostic and use the same configuration and administrator tools across production systems. A solutions architect needs to design a managed solution that will align open-source software.Which solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Launch the containers on Amazon EC2 with EC2 instance worker nodes.",
            "Launch the containers on Amazon Elastic Kubernetes Service (Amazon EKS) and EKS worker nodes.",
            "Launch the containers on Amazon Elastic Containers service (Amazon ECS) with AWS Fargate instances.",
            "Launch the containers on Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 instance worker nodes."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #186\n\nA company hosts its website on AWS. To address the highly variable demand, the company has implemented Amazon EC2 Auto Scaling. Management is concerned that the company is over-provisioning its infrastructure, especially at the front end of the three-tier application. A solutions architect needs to ensure costs are optimized without impacting performance.What should the solutions architect do to accomplish this?",
        "answers": [
            3
        ],
        "options": [
            "Use Auto Scaling with Reserved Instances.",
            "Use Auto Scaling with a scheduled scaling policy.",
            "Use Auto Scaling with the suspend-resume feature.",
            "Use Auto Scaling with a target tracking scaling policy."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question #187\n\nA solutions architect is performing a security review of a recently migrated workload. The workload is a web application that consists of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The solutions architect must improve the security posture and minimize the impact of a DDoS attack on resources.Which solution is MOST effective?",
        "answers": [
            1
        ],
        "options": [
            "Configure an AWS WAF ACL with rate-based rules. Create an Amazon CloudFront distribution that points to the Application Load Balancer. Enable the WAF ACL on the CloudFront distribution.",
            "Create a custom AWS Lambda function that adds identified attacks into a common vulnerability pool to capture a potential DDoS attack. Use the identified information to modify a network ACL to block access.",
            "Enable VPC Flow Logs and store then in Amazon S3. Create a custom AWS Lambda functions that parses the logs looking for a DDoS attack. Modify a network ACL to block identified source IP addresses.",
            "Enable Amazon GuardDuty and configure findings written to Amazon CloudWatch. Create an event with CloudWatch Events for DDoS alerts that triggers Amazon Simple Notification Service (Amazon SNS). Have Amazon SNS invoke a custom AWS Lambda function that parses the logs, looking for a DDoS attack. Modify a network ACL to block identified source IP addresses."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #188\n\nA company has multiple AWS accounts for various departments. One of the departments wants to share an Amazon S3 bucket with all other department.Which solution will require the LEAST amount of effort?",
        "answers": [
            2
        ],
        "options": [
            "Enable cross-account S3 replication for the bucket.",
            "Create a pre-signed URL for the bucket and share it with other departments.",
            "Set the S3 bucket policy to allow cross-account access to other departments.",
            "Create IAM users for each of the departments and configure a read-only IAM policy."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example2.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #189\n\nA company needs to share an Amazon S3 bucket with an external vendor. The bucket owner must be able to access all objects.Which action should be taken to share the S3 bucket?",
        "answers": [
            2
        ],
        "options": [
            "Update the bucket to be a Requester Pays bucket.",
            "Update the bucket to enable cross-origin resource sharing (CORS).",
            "Create a bucket policy to require users to grant bucket-owner-full-control when uploading objects.",
            "Create an IAM policy to require users to grant bucket-owner-full-control when uploading objects."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                By default, an S3 object is owned by the AWS account that uploaded it. This is true even when the bucket is owned by another account. To get access to the object, the object owner must explicitly grant you (the bucket owner) access. The object owner can grant the bucket owner full control of the object by updating the access control list (ACL) of the object. The object owner can update the ACL either during a put or copy operation, or after the object is added to the bucket.Similar:https://aws.amazon.com/it/premiumsupport/knowledge-center/s3-require-object-ownership/Resolution Add a bucket policy that grants users access to put objects in your bucket only when they grant you (the bucket owner) full control of the object.Reference:https://aws.amazon.com/it/premiumsupport/knowledge-center/s3-bucket-owner-access/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #190\n\nA company is developing a real-time multiplier game that uses UDP for communications between client and servers in an Auto Scaling group. Spikes in demand are anticipated during the day, so the game server platform must adapt accordingly. Developers want to store gamer scores and other non-relational data in a database solution that will scale without intervention.Which solution should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Use Amazon Route 53 for traffic distribution and Amazon Aurora Serverless for data storage.",
            "Use a Network Load Balancer for traffic distribution and Amazon DynamoDB on-demand for data storage.",
            "Use a Network Load Balancer for traffic distribution and Amazon Aurora Global Database for data storage.",
            "Use an Application Load Balancer for traffic distribution and Amazon DynamoDB global tables for data storage."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #11\n\nA company serves content to its subscribers across the world using an application running on AWS. The application has several Amazon EC2 instances in a private subnet behind an Application Load Balancer (ALB). Due to a recent change in copyright restrictions, the chief information officer (CIO) wants to block access for certain countries.Which action will meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Modify the ALB security group to deny incoming traffic from blocked countries.",
            "Modify the security group for EC2 instances to deny incoming traffic from blocked countries.",
            "Use Amazon CloudFront to serve the application and deny access to blocked countries.",
            "Use ALB listener rules to return access denied responses to incoming traffic from blocked countries."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                \"block access for certain countries.\" You can use geo restriction, also known as geo blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront web distribution.Reference:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #12\n\nA company is creating a new application that will store a large amount of data. The data will be analyzed hourly and modified by several Amazon EC2 Linux instances that are deployed across multiple Availability Zones. The application team believes the amount of space needed will continue to grow for the next 6 months.Which set of actions should a solutions architect take to support these needs?",
        "answers": [
            1
        ],
        "options": [
            "Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on the application instances.",
            "Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system on the application instances.",
            "Store the data in Amazon S3 Glacier. Update the S3 Glacier vault policy to allow access to the application instances.",
            "Store the data in an Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS volume shared between the application instances."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Amazon Elastic File System -Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.Amazon EFS is designed to provide massively parallel shared access to thousands of Amazon EC2 instances, enabling your applications to achieve high levels of aggregate throughput and IOPS with consistent low latencies.Amazon EFS is well suited to support a broad spectrum of use cases from home directories to business-critical applications. Customers can use EFS to lift-and- shift existing enterprise applications to the AWS Cloud. Other use cases include: big data analytics, web serving and content management, application development and testing, media and entertainment workflows, database backups, and container storage.Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability. Amazon EC2 instances can access your file system across AZs, regions, and VPCs, while on-premises servers can access using AWS Direct Connect or AWS VPN.Reference:https://aws.amazon.com/efs/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #13\n\nA company is migrating a three-tier application to AWS. The application requires a MySQL database. In the past, the application users reported poor application performance when creating new entries. These performance issues were caused by users generating different real-time reports from the application during working hours.Which solution will improve the performance of the application when it is moved to AWS?",
        "answers": [
            2
        ],
        "options": [
            "Import the data into an Amazon DynamoDB table with provisioned capacity. Refactor the application to use DynamoDB for reports.",
            "Create the database on a compute optimized Amazon EC2 instance. Ensure compute resources exceed the on-premises database.",
            "Create an Amazon Aurora MySQL Multi-AZ DB cluster with multiple read replicas. Configure the application to use the reader endpoint for reports.",
            "Create an Amazon Aurora MySQL Multi-AZ DB cluster. Configure the application to use the backup instance of the cluster as an endpoint for the reports."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Amazon RDS Read Replicas Now Support Multi-AZ DeploymentsStarting today, Amazon RDS Read Replicas for MySQL and MariaDB now support Multi-AZ deployments. Combining Read Replicas with Multi-AZ enables you to build a resilient disaster recovery strategy and simplify your database engine upgrade process.Amazon RDS Read Replicas enable you to create one or more read-only copies of your database instance within the same AWS Region or in a different AWSRegion. Updates made to the source database are then asynchronously copied to your Read Replicas. In addition to providing scalability for read-heavy workloads, Read Replicas can be promoted to become a standalone database instance when needed.Amazon RDS Multi-AZ deployments provide enhanced availability for database instances within a single AWS Region. With Multi-AZ, your data is synchronously replicated to a standby in a different Availability Zone (AZ). In the event of an infrastructure failure, Amazon RDS performs an automatic failover to the standby, minimizing disruption to your applications.You can now use Read Replicas with Multi-AZ as part of a disaster recovery (DR) strategy for your production databases. A well-designed and tested DR plan is critical for maintaining business continuity after a disaster. A Read Replica in a different region than the source database can be used as a standby database and promoted to become the new production database in case of a regional disruption.You can also combine Read Replicas with Multi-AZ for your database engine upgrade process. You can create a Read Replica of your production database instance and upgrade it to a new database engine version. When the upgrade is complete, you can stop applications, promote the Read Replica to a standalone database instance, and switch over your applications. Since the database instance is already a Multi-AZ deployment, no additional steps are needed.Overview of Amazon RDS Read ReplicasDeploying one or more read replicas for a given source DB instance might make sense in a variety of scenarios, including the following:Scaling beyond the compute or I/O capacity of a single DB instance for read-heavy database workloads. You can direct this excess read traffic to one or more read replicas.Serving read traffic while the source DB instance is unavailable. In some cases, your source DB instance might not be able to take I/O requests, for example due to I/O suspension for backups or scheduled maintenance. In these cases, you can direct read traffic to your read replicas. For this use case, keep in mind that the data on the read replica might be \"stale\" because the source DB instance is unavailable.Business reporting or data warehousing scenarios where you might want business reporting queries to run against a read replica, rather than your primary, production DB instance.Implementing disaster recovery. You can promote a read replica to a standalone instance as a disaster recovery solution if the source DB instance fails.Reference:https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/ https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #14\n\nA solutions architect is deploying a distributed database on multiple Amazon EC2 instances. The database stores all data on multiple instances so it can withstand the loss of an instance. The database requires block storage with latency and throughput to support several million transactions per second per server.Which storage solution should the solutions architect use?",
        "answers": [
            1
        ],
        "options": [
            "EBS Amazon Elastic Block Store (Amazon EBS)",
            "Amazon EC2 instance store",
            "Amazon Elastic File System (Amazon EFS)",
            "Amazon S3"
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #15\n\nOrganizers for a global event want to put daily reports online as static HTML pages. The pages are expected to generate millions of views from users around the world. The files are stored in an Amazon S3 bucket. A solutions architect has been asked to design an efficient and effective solution.Which action should the solutions architect take to accomplish this?",
        "answers": [
            3
        ],
        "options": [
            "Generate presigned URLs for the files.",
            "Use cross-Region replication to all Regions.",
            "Use the geoproximity feature of Amazon Route 53.",
            "Use Amazon CloudFront with the S3 bucket as its origin."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Using Amazon S3 Origins, MediaPackage Channels, and Custom Origins for Web DistributionsUsing Amazon S3 Buckets for Your OriginWhen you use Amazon S3 as an origin for your distribution, you place any objects that you want CloudFront to deliver in an Amazon S3 bucket. You can use any method that is supported by Amazon S3 to get your objects into Amazon S3, for example, the Amazon S3 console or API, or a third-party tool. You can create a hierarchy in your bucket to store the objects, just as you would with any other Amazon S3 bucket.Using an existing Amazon S3 bucket as your CloudFront origin server doesn't change the bucket in any way; you can still use it as you normally would to store and access Amazon S3 objects at the standard Amazon S3 price. You incur regular Amazon S3 charges for storing the objects in the bucket.Using Amazon S3 Buckets Configured as Website Endpoints for Your OriginYou can set up an Amazon S3 bucket that is configured as a website endpoint as custom origin with CloudFront.When you configure your CloudFront distribution, for the origin, enter the Amazon S3 static website hosting endpoint for your bucket. This value appears in theAmazon S3 console, on the Properties tab, in the Static website hosting pane. For example: http://bucket-name.s3-website-region.amazonaws.comFor more information about specifying Amazon S3 static website endpoints, see Website endpoints in the Amazon Simple Storage Service Developer Guide.When you specify the bucket name in this format as your origin, you can use Amazon S3 redirects and Amazon S3 custom error documents. For more information about Amazon S3 features, see the Amazon S3 documentation.Using an Amazon S3 bucket as your CloudFront origin server doesn't change it in any way. You can still use it as you normally would and you incur regularAmazon S3 charges.Reference:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #16\n\nA solutions architect is designing a new service behind Amazon API Gateway. The request patterns for the service will be unpredictable and can change suddenly from 0 requests to over 500 per second. The total size of the data that needs to be persisted in a backend database is currently less than 1 GB with unpredictable future growth. Data can be queried using simple key-value requests.Which combination of AWS services would meet these requirements? ",
        "answers": [
            1,
            2
        ],
        "options": [
            "AWS Fargate",
            "AWS Lambda",
            "Amazon DynamoDB",
            "Amazon EC2 Auto Scaling",
            "MySQL-compatible Amazon Aurora"
        ],
        "explination": "Correct Answer: BC \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-api-gateway-supports-endpoint-integrations-with-private-vpcs",
        "topic": "Topic 1"
    },
    {
        "question": "Question #17\n\nA start-up company has a web application based in the us-east-1 Region with multiple Amazon EC2 instances running behind an Application Load Balancer across multiple Availability Zones. As the company's user base grows in the us-west-1 Region, it needs a solution with low latency and high availability.What should a solutions architect do to accomplish this?",
        "answers": [
            2
        ],
        "options": [
            "Provision EC2 instances in us-west-1. Switch the Application Load Balancer to a Network Load Balancer to achieve cross-Region load balancing.",
            "Provision EC2 instances and an Application Load Balancer in us-west-1. Make the load balancer distribute the traffic based on the location of the request.",
            "Provision EC2 instances and configure an Application Load Balancer in us-west-1. Create an accelerator in AWS Global Accelerator that uses an endpoint group that includes the load balancer endpoints in both Regions.",
            "Provision EC2 instances and configure an Application Load Balancer in us-west-1. Configure Amazon Route 53 with a weighted routing policy. Create alias records in Route 53 that point to the Application Load Balancer."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Register endpoints for endpoint groups: You register one or more regional resources, such as Application Load Balancers, Network Load Balancers, EC2Instances, or Elastic IP addresses, in each endpoint group. Then you can set weights to choose how much traffic is routed to each endpoint.Endpoints in AWS Global AcceleratorEndpoints in AWS Global Accelerator can be Network Load Balancers, Application Load Balancers, Amazon EC2 instances, or Elastic IP addresses. A static IP address serves as a single point of contact for clients, and Global Accelerator then distributes incoming traffic across healthy endpoints. Global Accelerator directs traffic to endpoints by using the port (or port range) that you specify for the listener that the endpoint group for the endpoint belongs to.Each endpoint group can have multiple endpoints. You can add each endpoint to multiple endpoint groups, but the endpoint groups must be associated with different listeners.Global Accelerator continually monitors the health of all endpoints that are included in an endpoint group. It routes traffic only to the active endpoints that are healthy. If Global Accelerator doesn't have any healthy endpoints to route traffic to, it routes traffic to all endpoints.Reference:https://docs.aws.amazon.com/global-accelerator/latest/dg/about-endpoints.html https://aws.amazon.com/global-accelerator/faqs/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #18\n\nA solutions architect is designing a solution to access a catalog of images and provide users with the ability to submit requests to customize images. Image customization parameters will be in any request sent to an AWS API Gateway API. The customized image will be generated on demand, and users will receive a link they can click to view or download their customized image. The solution must be highly available for viewing and customizing images.What is the MOST cost-effective solution to meet these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Use Amazon EC2 instances to manipulate the original image into the requested customizations. Store the original and manipulated images in Amazon S3. Configure an Elastic Load Balancer in front of the EC2 instances.",
            "Use AWS Lambda to manipulate the original image to the requested customizations. Store the original and manipulated images in Amazon S3. Configure an Amazon CloudFront distribution with the S3 bucket as the origin.",
            "Use AWS Lambda to manipulate the original image to the requested customizations. Store the original images in Amazon S3 and the manipulated images in Amazon DynamoDB. Configure an Elastic Load Balancer in front of the Amazon EC2 instances.",
            "Use Amazon EC2 instances to manipulate the original image into the requested customizations. Store the original images in Amazon S3 and the manipulated images in Amazon DynamoDB. Configure an Amazon CloudFront distribution with the S3 bucket as the origin."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                AWS Lambda is a compute service that lets you run code without provisioning or managing servers. AWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second. You pay only for the compute time you consume \u05d2\u20ac\" there is no charge when your code is not running. With AWS Lambda, you can run code for virtually any type of application or backend service \u05d2\u20ac\" all with zero administration. AWS Lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, code monitoring and logging. All you need to do is supply your code in one of the languages that AWSLambda supports.Storing your static content with S3 provides a lot of advantages. But to help optimize your application's performance and security while effectively managing cost, we recommend that you also set up Amazon CloudFront to work with your S3 bucket to serve and protect the content. CloudFront is a content delivery network(CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out ofCloudFront can be more cost effective than delivering it from S3 directly to your users.CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located. CloudFront has edge servers in locations all around the world.Reference:https://docs.aws.amazon.com/lambda/latest/dg/welcome.htmlhttps://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #19\n\nA company is planning to migrate a business-critical dataset to Amazon S3. The current solution design uses a single S3 bucket in the us-east-1 Region with versioning enabled to store the dataset. The company's disaster recovery policy states that all data multiple AWS Regions.How should a solutions architect design the S3 solution?",
        "answers": [
            2
        ],
        "options": [
            "Create an additional S3 bucket in another Region and configure cross-Region replication.",
            "Create an additional S3 bucket in another Region and configure cross-origin resource sharing (CORS).",
            "Create an additional S3 bucket with versioning in another Region and configure cross-Region replication.",
            "Create an additional S3 bucket with versioning in another Region and configure cross-origin resource (CORS)."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Reference:https://medium.com/@KerrySheldon/s3-exercise-2-4-adding-objects-to-an-s3-bucket-with-cross-region-replication-a78b332b7697",
        "topic": "Topic 1"
    },
    {
        "question": "Question #20\n\nA company has application running on Amazon EC2 instances in a VPC. One of the applications needs to call an Amazon S3 API to store and read objects. The company's security policies restrict any internet-bound traffic from the applications.Which action will fulfill these requirements and maintain security?",
        "answers": [
            1
        ],
        "options": [
            "Configure an S3 interface endpoint.",
            "Configure an S3 gateway endpoint.",
            "Create an S3 bucket in a private subnet.",
            "Create an S3 bucket in the same Region as the EC2 instance."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #191\n\nA company collects temperature, humidity, and atmospheric pressure data in cities across multiple continents. The average volume of data collected per site each day is 500 GB. Each site has a high-speed internet connection. The company's weather forecasting applications are based in a single Region and analyze the data daily.What is the FASTEST way to aggregate data from all of these global sites?",
        "answers": [
            0
        ],
        "options": [
            "Enable Amazon S3 Transfer Acceleration on the destination bucket. Use multipart uploads to directly upload site data to the destination bucket.",
            "Upload site data to an Amazon S3 bucket in the closest AWS Region. Use S3 cross-Region replication to copy objects to the destination bucket.",
            "Schedule AWS Snowball jobs daily to transfer data to the closest AWS Region. Use S3 cross-Region replication to copy objects to the destination bucket.",
            "Upload the data to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Once a day take an EBS snapshot and copy it to the centralized Region. Restore the EBS volume in the centralized Region and run an analysis on the data daily."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Step-1: To transfer to S3 from global sites: Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. S3 Transfer Acceleration leverages Amazon CloudFront's globally distributed AWS Edge Locations. Used to accelerate object uploads to S3 over long distances (latency). Transfer acceleration is as secure as a direct upload to S3.Step-2: When the application analyze/aggregate the data from S3 and then again upload the results - Multipart uploadReference:http://lavnish.blogspot.com/2017/06/aws-s3-cross-region-replication.html https://aws.amazon.com/s3/transfer-acceleration/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #192\n\nA company has a custom application running on an Amazon EC instance that:* Reads a large amount of data from Amazon S3* Performs a multi-stage analysis* Writes the results to Amazon DynamoDBThe application writes a significant number of large, temporary files during the multi-stage analysis. The process performance depends on the temporary storage performance.What would be the fastest storage option for holding the temporary files?",
        "answers": [
            0
        ],
        "options": [
            "Multiple Amazon S3 buckets with Transfer Acceleration for storage.",
            "Multiple Amazon Elastic Block Store (Amazon EBS) drives with Provisioned IOPS and EBS optimization.",
            "Multiple Amazon Elastic File System (Amazon EFS) volumes using the Network File System version 4.1 (NFSv4.1) protocol.",
            "Multiple instance store volumes with software RAID 0."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #193\n\nA leasing company generates and emails PDF statements every month for all its customers. Each statement is about 400 KB in size. Customers can download their statements from the website for up to 30 days from when the statements were generated. At the end of their 3-year lease, the customers are emailed a ZIP file that contains all the statements.What is the MOST cost-effective storage solution for this situation?",
        "answers": [
            1
        ],
        "options": [
            "Store the statements using the Amazon S3 Standard storage class. Create a lifecycle policy to move the statements to Amazon S3 Glacier storage after 1 day.",
            "Store the statements using the Amazon S3 Glacier storage class. Create a lifecycle policy to move the statements to Amazon S3 Glacier Deep Archive storage after 30 days.",
            "Store the statements using the Amazon S3 Standard storage class. Create a lifecycle policy to move the statements to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) storage after 30 days.",
            "Store the statements using the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create a lifecycle policy to move the statements to Amazon S3 Glacier storage after 30 days."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #194\n\nA company recently released a new type of internet-connected sensor. The company is expecting to sell thousands of sensors, which are designed to stream high volumes of data each second to a central location. A solutions architect must design a solution that ingests and stores data so that engineering teams can analyze it in near-real time with millisecond responsiveness.Which solution should the solutions architect recommend?",
        "answers": [
            3
        ],
        "options": [
            "Use an Amazon SQS queue to ingest the data. Consume the data with an AWS Lambda function, which then stores the data in Amazon Redshift.",
            "Use an Amazon SQS queue to ingest the data. Consume the data with an AWS Lambda function, which then stores the data in Amazon DynamoDB.",
            "Use Amazon Kinesis Data Streams to ingest the data. Consume the data with an AWS Lambda function, which then stores the data in Amazon Redshift.",
            "Use Amazon Kinesis Data Streams to ingest the data. Consume the data with an AWS Lambda function, which then stores the data in Amazon DynamoDB."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #195\n\nA website runs a web application that receives a burst of traffic each day at noon. The users upload new pictures and content daily, but have been complaining of timeouts. The architecture uses Amazon EC2 Auto Scaling groups, and the custom application consistently takes 1 minute to initiate upon boot up before responding to user requests.How should a solutions architect redesign the architecture to better respond to changing traffic?",
        "answers": [
            2
        ],
        "options": [
            "Configure a Network Load Balancer with a slow start configuration.",
            "Configure AWS ElastiCache for Redis to offload direct requests to the servers.",
            "Configure an Auto Scaling step scaling policy with an instance warmup condition.",
            "Configure Amazon CloudFront to use an Application Load Balancer as the origin."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #196\n\nA company is concerned that two NAT instances in use will no longer be able to support the traffic needed for the company's application. A solutions architect wants to implement a solution that is highly available fault tolerant, and automatically scalable.What should the solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "Remove the two NAT instances and replace them with two NAT gateways in the same Availability Zone.",
            "Use Auto Scaling groups with Network Load Balancers for the NAT instances in different Availability Zones.",
            "Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones.",
            "Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a Network Load Balancer."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #197\n\nA company operates a website on Amazon EC2 Linux instances. Some of the instances are failing. Troubleshooting points to insufficient swap space on the failed instances. The operations team lead needs a solution to monitor this.What should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Configure an Amazon CloudWatch SwapUsage metric dimension. Monitor the SwapUsage dimension in the EC2 metrics in CloudWatch.",
            "Use EC2 metadata to collect information, then publish it to Amazon CloudWatch custom metrics. Monitor SwapUsage metrics in CloudWatch.",
            "Install an Amazon CloudWatch agent on the instances. Run an appropriate script on a set schedule. Monitor SwapUtilization metrics in CloudWatch.",
            "Enable detailed monitoring in the EC2 console. Create an Amazon CloudWatch SwapUtilization custom metric. Monitor SwapUtilization metrics in CloudWatch."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #198\n\nA company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2 instance. The default network ACL has been modified to block all traffic. A solutions architect needs to make the web server accessible from everywhere on port443.Which combination of steps will accomplish this task? ",
        "answers": [
            0,
            4
        ],
        "options": [
            "Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.",
            "Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.",
            "Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.",
            "Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0.",
            "Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0."
        ],
        "explination": "Correct Answer: AE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #199\n\nA company must re-evaluate its need for the Amazon EC2 instances it currently has provisioned in an Auto Scaling group. At present, the Auto Scaling group is configured for a minimum of two instances and a maximum of four instances across two Availability Zones. A Solutions architect reviewed Amazon CloudWatch metrics and found that CPU utilization is consistently low for all the EC2 instances.What should the solutions architect recommend to maximize utilization while ensuring the application remains fault tolerant?",
        "answers": [
            3
        ],
        "options": [
            "Remove some EC2 instances to increase the utilization of remaining instances.",
            "Increase the Amazon Elastic Block Store (Amazon EBS) capacity of instances with less CPU utilization.",
            "Modify the Auto Scaling group scaling policy to scale in and out based on a higher CPU utilization metric.",
            "Create a new launch configuration that uses smaller instance types. Update the existing Auto Scaling group."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                As the Launch Configuration can't be modified once created, only way to update the Launch Configuration for an Auto Scaling group is to create a new one and associate it with the Auto Scaling group",
        "topic": "Topic 1"
    },
    {
        "question": "Question #200\n\nA company has an application that posts messages to Amazon SQS. Another application polls the queue and processes the messages in an I/O-intensive operation. The company has a service level agreement (SLA) that specifies the maximum amount of time that can elapse between receiving the messages and responding to the users. Due to an increase in the number of messages, the company has difficulty meeting its SLA consistently.What should a solutions architect do to help improve the application's processing time and ensure it can handle the load at any level?",
        "answers": [
            3
        ],
        "options": [
            "Create an Amazon Machine Image (AMI) from the instance used for processing. Terminate the instance and replace it with a larger size.",
            "Create an Amazon Machine Image (AMI) from the instance used for processing. Terminate the instance and replace it with an Amazon EC2 Dedicated Instance.",
            "Create an Amazon Machine image (AMI) from the instance used for processing. Create an Auto Scaling group using this image in its launch configuration. Configure the group with a target tracking policy to keep its aggregate CPU utilization below 70%.",
            "Create an Amazon Machine Image (AMI) from the instance used for processing. Create an Auto Scaling group using this image in its launch configuration. Configure the group with a target tracking policy based on the age of the oldest message in the SQS queue."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0,
        "tags": [
            "AMI"
        ]
    },
    {
        "question": "Question #201\n\nA company is designing a new web service that will run on Amazon EC2 instances behind an Elastic Load Balancer. However, many of the web service clients can only reach IP addresses whitelisted on their firewalls.What should a solutions architect recommend to meet the clients' needs?",
        "answers": [
            0
        ],
        "options": [
            "A Network Load Balancer with an associated Elastic IP address.",
            "An Application Load Balancer with an associated Elastic IP address",
            "An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address",
            "An EC2 instance with a public IP address running as a proxy in front of the load balancer"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #202\n\nA company wants to host a web application on AWS that will communicate to a database within a VPC. The application should be highly available.What should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Create two Amazon EC2 instances to host the web servers behind a load balancer, and then deploy the database on a large instance.",
            "Deploy a load balancer in multiple Availability Zones with an Auto Scaling group for the web servers, and then deploy Amazon RDS in multiple Availability Zones.",
            "Deploy a load balancer in the public subnet with an Auto Scaling group for the web servers, and then deploy the database on an Amazon EC2 instance in the private subnet.",
            "Deploy two web servers with an Auto Scaling group, configure a domain that points to the two web servers, and then deploy a database architecture in multiple Availability Zones."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #203\n\nA company's packaged application dynamically creates and returns single-use text files in response to user requests. The company is using Amazon CloudFront for distribution, but wants to further reduce data transfer costs. The company cannot modify the application's source code.What should a solutions architect do to reduce costs?",
        "answers": [
            0
        ],
        "options": [
            "Use Lambda@Edge to compress the files as they are sent to users.",
            "Enable Amazon S3 Transfer Acceleration to reduce the response times.",
            "Enable caching on the CloudFront distribution to store generated files at the edge.",
            "Use Amazon S3 multipart uploads to move the files to Amazon S3 before returning them to users."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #204\n\nA database is on an Amazon RDS MySQL 5.6 Multi-AZ DB instance that experiences highly dynamic reads. Application developers notice a significant slowdown when testing read performance from a secondary AWS Region. The developers want a solution that provides less than 1 second of read replication latency.What should the solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Install MySQL on Amazon EC2 in the secondary Region.",
            "Migrate the database to Amazon Aurora with cross-Region replicas.",
            "Create another RDS for MySQL read replica in the secondary Region.",
            "Implement Amazon ElastiCache to improve database query performance."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/rds/aurora/global-database/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #205\n\nA company is planning to deploy an Amazon RDS DB instance running Amazon Aurora. The company has a backup retention policy requirement of 90 days.Which solution should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Set the backup retention period to 90 days when creating the RDS DB instance.",
            "Configure RDS to copy automated snapshots to a user-managed Amazon S3 bucket with a lifecycle policy set to delete after 90 days.",
            "Create an AWS Backup plan to perform a daily snapshot of the RDS database with the retention set to 90 days. Create an AWS Backup job to schedule the execution of the backup plan daily.",
            "Use a daily scheduled event with Amazon CloudWatch Events to execute a custom AWS Lambda function that makes a copy of the RDS automated snapshot. Purge snapshots older than 90 days."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #206\n\nA company currently has 250 TB of backup files stored in Amazon S3 in a vendor's proprietary format. Using a Linux-based software application provided by the vendor, the company wants to retrieve files from Amazon S3, transform the files to an industry-standard format, and re-upload them to Amazon S3. The company wants to minimize the data transfer charges associated with this conversation.What should a solutions architect do to accomplish this?",
        "answers": [
            3
        ],
        "options": [
            "Install the conversion software as an Amazon S3 batch operation so the data is transformed without leaving Amazon S3.",
            "Install the conversion software onto an on-premises virtual machine. Perform the transformation and re-upload the files to Amazon S3 from the virtual machine.",
            "Use AWS Snowball Edge devices to export the data and install the conversion software onto the devices. Perform the data transformation and re-upload the files to Amazon S3 from the Snowball Edge devices.",
            "Launch an Amazon EC2 instance in the same Region as Amazon S3 and install the conversion software onto the instance. Perform the transformation and re- upload the files to Amazon S3 from the EC2 instance."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #207\n\nA company is migrating a NoSQL database cluster to Amazon EC2. The database automatically replicates data to maintain at least three copies of the data. I/O throughput of the servers is the highest priority. Which instance type should a solutions architect recommend for the migration?",
        "answers": [
            0
        ],
        "options": [
            "Storage optimized instances with instance store",
            "Burstable general purpose instances with an Amazon Elastic Block Store (Amazon EBS) volume",
            "Memory optimized instances with Amazon Elastic Block Store (Amazon EBS) optimization enabled",
            "Compute optimized instances with Amazon Elastic Block Store (Amazon EBS) optimization enabled"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #208\n\nA company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft Windows shared file storage. The company wants to migrate this workload to the AWS Cloud and is considering various storage options. The storage solution must be highly available and integrated with ActiveDirectory for access control.Which solution will satisfy these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Configure Amazon EFS Amazon Elastic File System (Amazon EFS) storage and set the Active Directory domain for authentication.",
            "Create an SMB file share on an AWS Storage Gateway file gateway in two Availability Zones.",
            "Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume.",
            "Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/fsx/windows/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #209\n\nA company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost-effective database platform that will not require database modifications.Which solution will meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Amazon DynamoDB",
            "Amazon RDS for MySQL",
            "MySQL-compatible Amazon Aurora Serverless",
            "MySQL deployed on Amazon EC2 in an Auto Scaling group"
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #210\n\nA solutions architect is designing the storage architecture for a new web application used for storing and viewing engineering drawings. All application components will be deployed on the AWS infrastructure.The application design must support caching to minimize the amount of time that users wait for the engineering drawings to load. The application must be able to store petabytes of data. Which combination of storage and caching should the solutions architect use?",
        "answers": [
            1
        ],
        "options": [
            "Amazon S3 with Amazon CloudFront",
            "Amazon S3 Glacier with Amazon ElastiCache",
            "Amazon Elastic Block Store (Amazon EBS) volumes with Amazon CloudFront",
            "AWS Storage Gateway with Amazon ElastiCache"
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #211\n\nA solutions architect is creating an application that will handle batch processing of large amounts of data. The input data will be held in Amazon S3 and the output data will be stored in a different S3 bucket. For processing, the application will transfer the data over the network between multiple Amazon EC2 instances.What should the solutions architect do to reduce the overall data transfer costs?",
        "answers": [
            2
        ],
        "options": [
            "Place all the EC2 instances in an Auto Scaling group.",
            "Place all the EC2 instances in the same AWS Region.",
            "Place all the EC2 instances in the same Availability Zone.",
            "Place all the EC2 instances in private subnets in multiple Availability Zones."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #212\n\nA company hosts its core network services, including directory services and DNS, in its on-premises data center. The data center is connected to the AWS Cloud using AWS Direct Connect (DX). Additional AWS accounts are planned that will require quick, cost-effective, and consistent access to these network services.What should a solutions architect implement to meet these requirements with the LEAST amount of operational overhead?",
        "answers": [
            3
        ],
        "options": [
            "Create a DX connection in each new account. Route the network traffic to the on-premises servers.",
            "Configure VPC endpoints in the DX VPC for all required services. Route the network traffic to the on-premises servers.",
            "Create a VPN connection between each new account and the DX VPC. Route the network traffic to the on-premises servers.",
            "Configure AWS Transit Gateway between the accounts. Assign DX to the transit gateway and route network traffic to the on-premises servers."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #213\n\nA company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate users.What should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Deploy Amazon Inspector and associate it with the ALB.",
            "Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.",
            "Deploy rules to the network ACLs associated with the ALB to block the incoming traffic.",
            "Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/blogs/aws/protect-web-sites-services-using-rate-based-rules-for-aws-waf/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #214\n\nA company receives structured and semi-structured data from various sources once every day. A solutions architect needs to design a solution that leverages big data processing frameworks. The data should be accessible using SQL queries and business intelligence tools.What should the solutions architect recommend to build the MOST high-performing solution?",
        "answers": [
            1
        ],
        "options": [
            "Use AWS Glue to process data and Amazon S3 to store data.",
            "Use Amazon EMR to process data and Amazon Redshift to store data.",
            "Use Amazon EC2 to process data and Amazon Elastic Block Store (Amazon EBS) to store data.",
            "Use Amazon Kinesis Data Analytics to process data and Amazon Elastic File System (Amazon EFS) to store data."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/redshift/features/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #215\n\nA company is hosting an election reporting website on AWS for users around the world. The website uses Amazon EC2 instances for the web and application tiers in an Auto Scaling group with Application Load Balancers. The database tier uses an Amazon RDS for MySQL database. The website is updated with election results once an hour and has historically observed hundreds of users accessing the reports.The company is expecting a significant increase in demand because of upcoming elections in different countries. A solutions architect must improve the website's ability to handle additional demand while minimizing the need for additional EC2 instances.Which solution will meet these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Launch an Amazon ElastiCache cluster to cache common database queries.",
            "Launch an Amazon CloudFront web distribution to cache commonly requested website content.",
            "Enable disk-based caching on the EC2 instances to cache commonly requested website content.",
            "Deploy a reverse proxy into the design using an EC2 instance with caching enabled for commonly requested website content."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #216\n\nA company is building a website that relies on reading and writing to an Amazon DynamoDB database. The traffic associated with the website predictably peaks during business hours on weekdays and declines overnight and during weekends. A solutions architect needs to design a cost-effective solution that can handle the load.What should the solutions architect do to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Enable DynamoDB Accelerator (DAX) to cache the data.",
            "Enable Multi-AZ replication for the DynamoDB database.",
            "Enable DynamoDB auto scaling when creating the tables.",
            "Enable DynamoDB On-Demand capacity allocation when creating the tables."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #217\n\nA company uses Amazon Redshift for its data warehouse. The company wants to ensure high durability for its data in case of any component failure.What should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Enable concurrency scaling.",
            "Enable cross-Region snapshots.",
            "Increase the data retention period.",
            "Deploy Amazon Redshift in Multi-AZ."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #218\n\nA company has data stored in an on-premises data center that is used by several on-premises applications. The company wants to maintain its existing application environment and be able to use AWS services for data analytics and future visualizations.Which storage service should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Amazon Redshift",
            "AWS Storage Gateway for files",
            "Amazon Elastic Block Store (Amazon EBS)",
            "Amazon Elastic File System (Amazon EFS)"
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #219\n\nA solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 origin to store a static website. The company's security policy requires that all website traffic be inspected by AWS WAF.How should the solutions architect comply with these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Configure an S3 bucket policy to accept requests coming from the AWS WAF Amazon Resource Name (ARN) only.",
            "Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin.",
            "Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only. Associate AWS WAF to CloudFront.",
            "Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the distribution."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #220\n\nA company has a 143 TB MySQL database that it wants to migrate to AWS. The plan is to use Amazon Aurora MySQL as the platform going forward. The company has a 100 Mbps AWS Direct Connect connection to Amazon VPC.Which solution meets the company's needs and takes the LEAST amount of time?",
        "answers": [
            3
        ],
        "options": [
            "Use a gateway endpoint for Amazon S3. Migrate the data to Amazon S3. Import the data into Aurora.",
            "Upgrade the Direct Connect link to 500 Mbps. Copy the data to Amazon S3. Import the data into Aurora.",
            "Order an AWS Snowmobile and copy the database backup to it. Have AWS import the data into Amazon S3. Import the backup into Aurora.",
            "Order four 50-TB AWS Snowball devices and copy the database backup onto them. Have AWS import the data into Amazon S3. Import the data into Aurora."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #221\n\nA company hosts an online shopping application that stores all orders in an Amazon RDS for PostgreSQL Single-AZ DB instance. Management wants to eliminate single points of failure and has asked a solutions architect to recommend an approach to minimize database downtime without requiring any changes to the application code.Which solution meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Convert the existing database instance to a Multi-AZ deployment by modifying the database instance and specifying the Multi-AZ option.",
            "Create a new RDS Multi-AZ deployment. Take a snapshot of the current RDS instance and restore the new Multi-AZ deployment with the snapshot.",
            "Create a read-only replica of the PostgreSQL database in another Availability Zone. Use Amazon Route 53 weighted record sets to distribute requests across the databases.",
            "Place the RDS for PostgreSQL database in an Amazon EC2 Auto Scaling group with a minimum group size of two. Use Amazon Route 53 weighted record sets to distribute requests across instances."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #222\n\nA company has a 10 Gbps AWS Direct Connect connection from its on-premises servers to AWS. The workloads using the connection are critical. The company requires a disaster recovery strategy with maximum resiliency that maintains the current connection bandwidth at a minimum.What should a solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "Set up a new Direct Connect connection in another AWS Region.",
            "Set up a new AWS managed VPN connection in another AWS Region.",
            "Set up two new Direct Connect connections: one in the current AWS Region and one in another Region.",
            "Set up two new AWS managed VPN connections: one in the current AWS Region and one in another Region."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #223\n\nA solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates.What should the solutions architect do to enable internet access for the private subnets?",
        "answers": [
            0
        ],
        "options": [
            "Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT gateway in its AZ.",
            "Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT instance in its AZ.",
            "Create a second internet gateway on one of the private subnets. Update the route table for the private subnets that forward non-VPC traffic to the private internet gateway.",
            "Create an egress-only internet gateway on one of the public subnets. Update the route table for the private subnets that forward non-VPC traffic to the egress- only internet gateway."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #224\n\nAs part of budget planning, management wants a report of AWS billed items listed by user. The data will be used to create department budgets. A solutions architect needs to determine the most efficient way to obtain this report information.Which solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Run a query with Amazon Athena to generate the report.",
            "Create a report in Cost Explorer and download the report.",
            "Access the bill details from the billing dashboard and download the bill.",
            "Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES)."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #225\n\nA company with facilities in North America, Europe, and Asia is designing new distributed application to optimize its global supply chain and manufacturing process. The orders booked on one continent should be visible to all Regions in a second or less. The database should be able to support failover with a shortRecovery Time Objective (RTO). The uptime of the application is important to ensure that manufacturing is not impacted.What should a solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Use Amazon DynamoDB global tables.",
            "Use Amazon Aurora Global Database.",
            "Use Amazon RDS for MySQL with a cross-Region read replica.",
            "Use Amazon RDS for PostgreSQL with a cross-Region read replica."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #226\n\nA company's near-real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance.Which combination of steps should the solutions architect take? ",
        "answers": [
            0,
            4
        ],
        "options": [
            "Use Amazon Kinesis Data Firehose to ingest the data.",
            "Use AWS Lambda with AWS Step Functions to process the data.",
            "Use AWS Database Migration Service (AWS DMS) to ingest the data.",
            "Use Amazon EC2 instances in an Auto Scaling group to process the data.",
            "Use AWS Fargate with Amazon Elastic Container Service (Amazon ECS) to process the data."
        ],
        "explination": "Correct Answer: AE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #227\n\nAn application running on an Amazon EC2 instance needs to access an Amazon DynamoDB table. Both the EC2 instance and the DynamoDB table are in the same AWS account. A solutions architect must configure the necessary permissions.Which solution will allow least privilege access to the DynamoDB table from the EC2 instance?",
        "answers": [
            0
        ],
        "options": [
            "Create an IAM role with the appropriate policy to allow access to the DynamoDB table. Create an instance profile to assign this IAM role to the EC2 instance.",
            "Create an IAM role with the appropriate policy to allow access to the DynamoDB table. Add the EC2 instance to the trust relationship policy document to allow it to assume the role.",
            "Create an IAM user with the appropriate policy to allow access to the DynamoDB table. Store the credentials in an Amazon S3 bucket and read them from within the application code directly.",
            "Create an IAM user with the appropriate policy to allow access to the DynamoDB table. Ensure that the application stores the IAM credentials securely on local storage and uses them to make the DynamoDB calls."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #228\n\nA solutions architect is designing a solution that involves orchestrating a series of Amazon Elastic Container Service (Amazon ECS) task types running onAmazon EC2 instances that are part of an ECS cluster. The output and state data for all tasks needs to be stored. The amount of data output by each task is approximately 10 MB, and there could be hundreds of tasks running at a time. The system should be optimized for high-frequency reading and writing. As old outputs are archived and deleted, the storage size is not expected to exceed 1 TB.Which storage solution should the solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "An Amazon DynamoDB table accessible by all ECS cluster instances.",
            "An Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode.",
            "An Amazon Elastic File System (Amazon EFS) file system with Bursting Throughput mode.",
            "An Amazon Elastic Block Store (Amazon EBS) volume mounted to the ECS cluster instances."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #229\n\nAn online photo application lets users upload photos and perform image editing operations. The application offers two classes of service: free and paid. Photos submitted by paid users are processed before those submitted by free users. Photos are uploaded to Amazon S3 and the job information is sent to Amazon SQS.Which configuration should a solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Use one SQS FIFO queue. Assign a higher priority to the paid photos so they are processed first.",
            "Use two SQS FIFO queues: one for paid and one for free. Set the free queue to use short polling and the paid queue to use long polling.",
            "Use two SQS standard queues: one for paid and one for free. Configure Amazon EC2 instances to prioritize polling for the paid queue over the free queue.",
            "Use one SQS standard queue. Set the visibility timeout of the paid photos to zero. Configure Amazon EC2 instances to prioritize visibility settings so paid photos are processed first."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #230\n\nA company wants to migrate its MySQL database from on premises to AWS. The company recently experienced a database outage that significantly impacted the business. To ensure this does not happen again, the company wants a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes.Which solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones.",
            "Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data.",
            "Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data.",
            "Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #231\n\nA company stores user data in AWS. The data is used continuously with peak usage during business hours. Access patterns vary, with some data not being used for months at a time. A solutions architect must choose a cost-effective solution that maintains the highest level of durability while maintaining high availability.Which storage solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Amazon S3 Standard",
            "Amazon S3 Intelligent-Tiering",
            "Amazon S3 Glacier Deep Archive",
            "Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)"
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #232\n\nA company receives inconsistent service from its data center provider because the company is headquartered in an area affected by natural disasters. The company is not ready to fully migrate to the AWS Cloud, but it wants a failure environment on AWS in case the on-premises data center fails.The company runs web servers that connect to external vendors. The data available on AWS and on premises must be uniform.Which solution should a solutions architect recommend that has the LEAST amount of downtime?",
        "answers": [
            0
        ],
        "options": [
            "Configure an Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3.",
            "Configure an Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to create Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3.",
            "Configure an Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer.",
            "Configure an Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #233\n\nA company has three VPCs named Development, Testing, and Production in the us-east-1 Region. The three VPCs need to be connected to an on-premises data center and are designed to be separate to maintain security and prevent any resource sharing. A solutions architect needs to find a scalable and secure solution.What should the solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Create an AWS Direct Connect connection and a VPN connection for each VPC to connect back to the data center.",
            "Create VPC peers from all the VPCs to the Production VPC. Use an AWS Direct Connect connection from the Production VPC back to the data center.",
            "Connect VPN connections from all the VPCs to a VPN in the Production VPC. Use a VPN connection from the Production VPC back to the data center.",
            "Create a new VPC called Network. Within the Network VPC, create an AWS Transit Gateway with an AWS Direct Connect connection back to the data center. Attach all the other VPCs to the Network VPC."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #234\n\nWhat should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?",
        "answers": [
            3
        ],
        "options": [
            "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.",
            "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private.",
            "Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true.",
            "Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #235\n\nA company needs a secure connection between its on-premises environment and AWS. This connection does not need high bandwidth and will handle a small amount of traffic. The connection should be set up quickly.What is the MOST cost-effective method to establish this type of connection?",
        "answers": [
            3
        ],
        "options": [
            "Implement a client VPN.",
            "Implement AWS Direct Connect.",
            "Implement a bastion host on Amazon EC2.",
            "Implement an AWS Site-to-Site VPN connection."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #236\n\nA company uses Application Load Balancers (ALBs) in different AWS Regions. The ALBs receive inconsistent traffic that can spike and drop throughout the year.The company's networking team needs to allow the IP addresses of the ALBs in the on-premises firewall to enable connectivity.Which solution is the MOST scalable with minimal configuration changes?",
        "answers": [
            2
        ],
        "options": [
            "Write an AWS Lambda script to get the IP addresses of the ALBs in different Regions. Update the on-premises firewall's rule to allow the IP addresses of the ALBs.",
            "Migrate all ALBs in different Regions to the Network Load Balancer (NLBs). Update the on-premises firewall's rule to allow the Elastic IP addresses of all the NLBs.",
            "Launch AWS Global Accelerator. Register the ALBs in different Regions to the accelerator. Update the on-premises firewall's rule to allow static IP addresses associated with the accelerator.",
            "Launch a Network Load Balancer (NLB) in one Region. Register the private IP addresses of the ALBs in different Regions with the NLB. Update the on- premises firewall's rule to allow the Elastic IP address attached to the NLB."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #237\n\nA company runs a high performance computing (HPC) workload on AWS. The workload required low-latency network performance and high network throughput with tightly coupled node-to-node communication. The Amazon EC2 instances are properly sized for compute and storage capacity, and are launched using default options.What should a solutions architect propose to improve the performance of the workload?",
        "answers": [
            0
        ],
        "options": [
            "Choose a cluster placement group while launching Amazon EC2 instances.",
            "Choose dedicated instance tenancy while launching Amazon EC2 instances.",
            "Choose an Elastic Inference accelerator while launching Amazon EC2 instances.",
            "Choose the required capacity reservation while launching Amazon EC2 instances."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #238\n\nA company uses a legacy on-premises analytics application that operates on gigabytes of .csv files and represents months of data. The legacy application cannot handle the growing size of .csv files. New .csv files are added daily from various data sources to a central on-premises storage location. The company wants to continue to support the legacy application while users learn AWS analytics services. To achieve this, a solutions architect wants to maintain two synchronized copies of all the .csv files on-premises and in Amazon S3.Which solution should the solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Deploy AWS DataSync on-premises. Configure DataSync to continuously replicate the .csv files between the company's on-premises storage and the company's S3 bucket.",
            "Deploy an on-premises file gateway. Configure data sources to write the .csv files to the file gateway. Point the legacy analytics application to the file gateway. The file gateway should replicate the .csv files to Amazon S3.",
            "Deploy an on-premises volume gateway. Configure data sources to write the .csv files to the volume gateway. Point the legacy analytics application to the volume gateway. The volume gateway should replicate data to Amazon S3.",
            "Deploy AWS DataSync on-premises. Configure DataSync to continuously replicate the .csv files between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon Elastic File System (Amazon EFS) to the company's S3 bucket."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #239\n\nA company has media and application files that need to be shared internally. Users currently are authenticated using Active Directory and access files from aMicrosoft Windows platform. The chief executive officer wants to keep the same user permissions, but wants the company to improve the process as the company is reaching its storage capacity limit.What should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Set up a corporate Amazon S3 bucket and move all media and application files.",
            "Configure Amazon FSx for Windows File Server and move all the media and application files.",
            "Configure Amazon Elastic File System (Amazon EFS) and move all media and application files.",
            "Set up Amazon EC2 on Windows, attach multiple Amazon Elastic Block Store (Amazon EBS) volumes, and move all media and application files."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/fsx/windows/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #240\n\nA company is deploying a web portal. The company wants to ensure that only the web portion of the application is publicly accessible. To accomplish this, theVPC was designed with two public subnets and two private subnets. The application will run on several Amazon EC2 instances in an Auto Scaling group. SSL termination must be offloaded from the EC2 instances.What should a solutions architect do to ensure these requirements are met?",
        "answers": [
            2
        ],
        "options": [
            "Configure the Network Load Balancer in the public subnets. Configure the Auto Scaling group in the private subnets and associate it with the Application Load Balancer.",
            "Configure the Network Load Balancer in the public subnets. Configure the Auto Scaling group in the public subnets and associate it with the Application Load Balancer.",
            "Configure the Application Load Balancer in the public subnets. Configure the Auto Scaling group in the private subnets and associate it with the Application Load Balancer.",
            "Configure the Application Load Balancer in the private subnets. Configure the Auto Scaling group in the private subnets and associate it with the Application Load Balancer."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #241\n\nA company is experiencing growth as demand for its product has increased. The company's existing purchasing application is slow when traffic spikes. The application is a monolithic three-tier application that uses synchronous transactions and sometimes sees bottlenecks in the application tier. A solutions architect needs to design a solution that can meet required application response times while accounting for traffic volume spikes.Which solution will meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Vertically scale the application instance using a larger Amazon EC2 instance size.",
            "Scale the application's persistence layer horizontally by introducing Oracle RAC on AWS.",
            "Scale the web and application tiers horizontally using Auto Scaling groups and an Application Load Balancer.",
            "Decouple the application and data tiers using Amazon Simple Queue Service (Amazon SQS) with asynchronous AWS Lambda calls."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #242\n\nA company hosts an application used to upload files to an Amazon S3 bucket. Once uploaded, the files are processed to extract metadata, which takes less than5 seconds. The volume and frequency of the uploads varies from a few files each hour to hundreds of concurrent uploads. The company has asked a solutions architect to design a cost-effective architecture that will meet these requirements.What should the solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Configure AWS CloudTrail trails to log S3 API calls. Use AWS AppSync to process the files.",
            "Configure an object-created event notification within the S3 bucket to invoke an AWS Lambda function to process the files.",
            "Configure Amazon Kinesis Data Streams to process and send data to Amazon S3. Invoke an AWS Lambda function to process the files.",
            "Configure an Amazon Simple Notification Service (Amazon SNS) topic to process the files uploaded to Amazon S3. Invoke an AWS Lambda function to process the files."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #243\n\nA company has copied 1 PB of data from a colocation facility to an Amazon S3 bucket in the us-east-1 Region using an AWS Direct Connect link. The company now wants to copy the data to another S3 bucket in the us-west-2 Region. The colocation facility does not allow the use of AWS Snowball.What should a solutions architect recommend to accomplish this?",
        "answers": [
            3
        ],
        "options": [
            "Order a Snowball Edge device to copy the data from one Region to another Region.",
            "Transfer contents from the source S3 bucket to a target S3 bucket using the S3 console.",
            "Use the aws S3 sync command to copy data from the source bucket to the destination bucket.",
            "Add a cross-Region replication configuration to copy objects across S3 buckets in different Regions."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #244\n\nA company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-flight is lost. The company's data science team wants to query ingested data in near-real time.Which solution provides near-real-time data querying that is scalable with minimal data loss?",
        "answers": [
            0
        ],
        "options": [
            "Publish data to Amazon Kinesis Data Streams. Use Kinesis Data Analytics to query the data.",
            "Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.",
            "Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.",
            "Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #245\n\nA company is deploying a multi-instance application within AWS that requires minimal latency between the instances.What should a solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Use an Auto Scaling group with a cluster placement group.",
            "Use an Auto Scaling group with single Availability Zone in the same AWS Region.",
            "Use an Auto Scaling group with multiple Availability Zones in the same AWS Region.",
            "Use a Network Load Balancer with multiple Amazon EC2 Dedicated Hosts as the targets."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #246\n\nA company is developing a mobile game that streams score updates to a backend processor and then posts results on a leaderboard. A solutions architect needs to design a solution that can handle large traffic spikes, process the mobile game updates in order of receipt, and store the processed updates in a highly available database. The company also wants to minimize the management overhead required to maintain the solution.What should the solutions architect do to meet these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Push score updates to Amazon Kinesis Data Streams. Process the updates in Kinesis Data Streams with AWS Lambda. Store the processed updates in Amazon DynamoDB.",
            "Push score updates to Amazon Kinesis Data Streams. Process the updates with a fleet of Amazon EC2 instances set up for Auto Scaling. Store the processed updates in Amazon Redshift.",
            "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to process the updates. Store the processed updates in a SQL database running on Amazon EC2.",
            "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue. Use a fleet of Amazon EC2 instances with Auto Scaling to process the updates in the SQS queue. Store the processed updates in an Amazon RDS Multi-AZ DB instance."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #247\n\nA company is building a document storage application on AWS. The application runs on Amazon EC2 instances in multiple Availability Zones. The company requires the document store to be highly available. The documents need to be returned immediately when requested. The lead engineer has configured the application to use Amazon Elastic Block Store (Amazon EBS) to store the documents, but is willing to consider other options to meet the availability requirement.What should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Snapshot the EBS volumes regularly and build new volumes using those snapshots in additional Availability Zones.",
            "Use Amazon Elastic Block Store (Amazon EBS) for the EC2 instance root volumes. Configure the application to build the document store on Amazon S3.",
            "Use Amazon Elastic Block Store (Amazon EBS) for the EC2 instance root volumes. Configure the application to build the document store on Amazon S3 Glacier.",
            "Use at least three Provisioned IOPS EBS volumes for EC2 instances. Mount the volumes to the EC2 instances in a RAID 5 configuration."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #249\n\nA solutions architect is designing a security solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Because the individual developers will have AWS account root user-level access to their own accounts, the solutions architect wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified.Which action meets these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Create an IAM policy that prohibits changes to CloudTrail, and attach it to the root user.",
            "Create a new trail in CloudTrail from within the developer accounts with the organization trails option enabled.",
            "Create a service control policy (SCP) the prohibits changes to CloudTrail, and attach it the developer accounts.",
            "Create a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the management account."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #250\n\nA company wants to share forensic accounting data that is stored in an Amazon RDS DB instance with an external auditor. The auditor has its own AWS account and requires its own copy of the database.How should the company securely share the database with the auditor?",
        "answers": [
            0
        ],
        "options": [
            "Create a read replica of the database and configure IAM standard database authentication to grant the auditor access.",
            "Copy a snapshot of the database to Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket.",
            "Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket.",
            "Make an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #251\n\nA company has an automobile sales website that stores its listings in a database on Amazon RDS. When an automobile is sold, the listing needs to be removed from the website and the data must be sent to multiple target systems.Which design should a solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue for the targets to consume.",
            "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) FIFO queue for the targets to consume.",
            "Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS) queue fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS Lambda functions to update the targets.",
            "Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #252\n\nA company is building a media sharing application and decides to use Amazon S3 for storage. When a media file is uploaded, the company starts a multi-step process to create thumbnails, identify objects in the images, transcode videos into standard formats and resolutions, and extract and store the metadata to anAmazon DynamoDB table. The metadata is used for searching and navigation.The amount of traffic is variable. The solution must be able to scale to handle spikes in load without unnecessary expenses.What should a solutions architect recommend to support this workload?",
        "answers": [
            2
        ],
        "options": [
            "Build the processing into the website or mobile app used to upload the content to Amazon S3. Save the required data to the DynamoDB table when the objects are uploaded.",
            "Trigger AWS Step Functions when an object is stored in the S3 bucket. Have the Step Functions perform the steps needed to process the object and then write the metadata to the DynamoDB table.",
            "Trigger an AWS Lambda function when an object is stored in the S3 bucket. Have the Lambda function start AWS Batch to perform the steps to process the object. Place the object data in the DynamoDB table when complete.",
            "Trigger an AWS Lambda function to store an initial entry in the DynamoDB table when an object is uploaded to Amazon S3. Use a program running on an Amazon EC2 instance in an Auto Scaling group to poll the index for unprocessed items, and use the program to perform the processing."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #253\n\nA company provides an API to its users that automates inquiries for tax computations based on item prices. The company experiences a larger number of inquiries during the holiday season only that cause slower response times. A solutions architect needs to design a solution that is scalable and elastic.What should the solutions architect do to accomplish this?",
        "answers": [
            1
        ],
        "options": [
            "Provide an API hosted on an Amazon EC2 instance. The EC2 instance performs the required computations when the API request is made.",
            "Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item names to AWS Lambda for tax computations.",
            "Create an Application Load Balancer that has two Amazon EC2 instances behind it. The EC2 instances will compute the tax on the received item names.",
            "Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2 instance. API Gateway accepts and passes the item names to the EC2 instance for tax computations."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #254\n\nAn application is running on an Amazon EC2 instance and must have millisecond latency when running the workload. The application makes many small reads and writes to the file system, but the file system itself is small.Which Amazon Elastic Block Store (Amazon EBS) volume type should a solutions architect attach to their EC2 instance?",
        "answers": [
            1
        ],
        "options": [
            "Cold HDD (sc1)",
            "General Purpose SSD (gp2)",
            "Provisioned IOPS SSD (io1)",
            "Throughput Optimized HDD (st1)"
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/blogs/database/best-storage-practices-for-running-production-workloads-on-hosted-databases-with-amazon-rds-or-amazon- ec2/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #255\n\nA solutions architect is designing a multi-Region disaster recovery solution for an application that will provide public API access. The application will use AmazonEC2 instances with a userdata script to load application code and an Amazon RDS for MySQL database. The Recovery Time Objective (RTO) is 3 hours and theRecovery Point Objective (RPO) is 24 hours.Which architecture would meet these requirements at the LOWEST cost?",
        "answers": [
            3
        ],
        "options": [
            "Use an Application Load Balancer for Region failover. Deploy new EC2 instances with the userdata script. Deploy separate RDS instances in each Region.",
            "Use Amazon Route 53 for Region failover. Deploy new EC2 instances with the userdata script. Create a read replica of the RDS instance in a backup Region.",
            "Use Amazon API Gateway for the public APIs and Region failover. Deploy new EC2 instances with the userdata script. Create a MySQL read replica of the RDS instance in a backup Region.",
            "Use Amazon Route 53 for Region failover. Deploy new EC2 instances with the userdata script for APIs, and create a snapshot of the RDS instance daily for a backup. Replicate the snapshot to a backup Region."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #256\n\nA solutions architect needs to ensure that all Amazon Elastic Block Store (Amazon EBS) volumes restored from unencrypted EBC snapshots are encrypted.What should the solutions architect do to accomplish this?",
        "answers": [
            2
        ],
        "options": [
            "Enable EBS encryption by default for the AWS Region.",
            "Enable EBS encryption by default for the specific volumes.",
            "Create a new volume and specify the symmetric customer master key (CMK) to use for encryption.",
            "Create a new volume and specify the asymmetric customer master key (CMK) to use for encryption."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#volume-account-off",
        "topic": "Topic 1"
    },
    {
        "question": "Question #257\n\nA company runs a static website through its on-premises data center. The company has multiple servers that handle all of its traffic, but on busy days, services are interrupted and the website becomes unavailable. The company wants to expand its presence globally and plans to triple its website traffic.What should a solutions architect recommend to meet these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Migrate the website content to Amazon S3 and host the website on Amazon CloudFront.",
            "Migrate the website content to Amazon EC2 instances with public Elastic IP addresses in multiple AWS Regions.",
            "Migrate the website content to Amazon EC2 instances and vertically scale as the load increases.",
            "Use Amazon Route 53 to distribute the loads across multiple Amazon CloudFront distributions for each AWS Region that exists globally."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #258\n\nA company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it. The job is stateless in nature, can be started and stopped at any given time with no negative impact, and typically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design a scalable and cost-effective solution that meets the requirements of the job.What should the solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Implement EC2 Spot Instances.",
            "Purchase EC2 Reserved Instances.",
            "Implement EC2 On-Demand Instances.",
            "Implement the processing on AWS Lambda."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #259\n\nA company is hosting its static website in an Amazon S3 bucket, which is the origin for Amazon CloudFront. The company has users in the United States, Canada, and Europe and wants to reduce costs.What should a solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "Adjust the CloudFront caching time to live (TTL) from the default to a longer timeframe.",
            "Implement CloudFront events with Lambda@Edge to run the website's data processing.",
            "Modify the CloudFront price class to include only the locations of the countries that are served.",
            "Implement a CloudFront Secure Sockets Layer (SSL) certificate to push security closer to the locations of the countries that are served."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #260\n\nA company is planning to migrate a commercial off-the-shelf application from its on-premises data center to AWS. The software has a software licensing model using sockets and cores with predictable capacity and uptime requirements. The company wants to use its existing licenses, which were purchased earlier this year.Which Amazon EC2 pricing option is the MOST cost-effective?",
        "answers": [
            0
        ],
        "options": [
            "Dedicated Reserved Hosts",
            "Dedicated On-Demand Hosts",
            "Dedicated Reserved Instances",
            "Dedicated On-Demand Instances"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #261\n\nA company is designing a website that uses an Amazon S3 bucket to store static images. The company wants all future requests to have faster response times while reducing both latency and cost.Which service configuration should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Deploy a NAT server in front of Amazon S3.",
            "Deploy Amazon CloudFront in front of Amazon S3.",
            "Deploy a Network Load Balancer in front of Amazon S3.",
            "Configure Auto Scaling to automatically adjust the capacity of the website."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/getting-started/hands-on/deliver-content-faster/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #262\n\nA company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The sales team requires the database to have minimal downtime. A database administrator wants to migrate this database to AWS without selecting a particular instance type in anticipation of more users in the future.Which service should a solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Amazon Aurora MySQL",
            "Amazon Aurora Serverless for MySQL",
            "Amazon Redshift Spectrum",
            "Amazon RDS for MySQL"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/premiumsupport/knowledge-center/migrate-mysql-rds-dms/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #263\n\nA company needs to comply with a regulatory requirement that states all emails must be stored and archived externally for 7 years. An administrator has created compressed email files on premises and wants a managed service to transfer the files to AWS storage.Which managed service should a solutions architect recommend?",
        "answers": [
            3
        ],
        "options": [
            "Amazon Elastic File System (Amazon EFS)",
            "Amazon S3 Glacier",
            "AWS Backup",
            "AWS Storage Gateway"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/storagegateway/faqs/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #265\n\nA company that hosts its web application on AWS wants to ensure all Amazon EC2 instances, Amazon RDS DB instances, and Amazon Redshift clusters are configured with tags. The company wants to minimize the effort of configuring and operating this check.What should a solutions architect do to accomplish this?",
        "answers": [
            0
        ],
        "options": [
            "Use AWS Config rules to define and detect resources that are not properly tagged.",
            "Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.",
            "Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 instance.",
            "Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Reference:https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf",
        "topic": "Topic 1"
    },
    {
        "question": "Question #266\n\nA company has a live chat application running on its on-premises servers that use WebSockets. The company wants to migrate the application to AWS.Application traffic is inconsistent, and the company expects there to be more traffic with sharp spikes in the future.The company wants a highly scalable solution with no server maintenance nor advanced capacity planning.Which solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Use Amazon API Gateway and AWS Lambda with an Amazon DynamoDB table as the data store. Configure the DynamoDB table for provisioned capacity.",
            "Use Amazon API Gateway and AWS Lambda with an Amazon DynamoDB table as the data store. Configure the DynamoDB table for on-demand capacity.",
            "Run Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group with an Amazon DynamoDB table as the data store. Configure the DynamoDB table for on-demand capacity.",
            "Run Amazon EC2 instances behind a Network Load Balancer in an Auto Scaling group with an Amazon DynamoDB table as the data store. Configure the DynamoDB table for provisioned capacity."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #267\n\nA company hosts its static website content from an Amazon S3 bucket in the us-east-1 Region. Content is made available through an Amazon CloudFront origin pointing to that bucket. Cross-Region replication is set to create a second copy of the bucket in the ap-southeast-1 Region. Management wants a solution that provides greater availability for the website.Which combination of actions should a solutions architect take to increase availability? ",
        "answers": [
            1,
            4
        ],
        "options": [
            "Add both buckets to the CloudFront origin.",
            "Configure failover routing in Amazon Route 53.",
            "Create a record in Amazon Route 53 pointing to the replica bucket.",
            "Create an additional CloudFront origin pointing to the ap-southeast-1 bucket.",
            "Set up a CloudFront origin group with the us-east-1 bucket as the primary and the ap-southeast-1 bucket as the secondary."
        ],
        "explination": "Correct Answer: BE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #268\n\nA company hosts a training site on a fleet of Amazon EC2 instances. The company anticipates that its new course, which consists of dozens of training videos on the site, will be extremely popular when it is released in 1 week.What should a solutions architect do to minimize the anticipated server load?",
        "answers": [
            2
        ],
        "options": [
            "Store the videos in Amazon ElastiCache for Redis. Update the web servers to serve the videos using the ElastiCache API.",
            "Store the videos in Amazon Elastic File System (Amazon EFS). Create a user data script for the web servers to mount the EFS volume.",
            "Store the videos in an Amazon S3 bucket. Create an Amazon CloudFront distribution with an origin access identity (OAI) of that S3 bucket. Restrict Amazon S3 access to the OAI.",
            "Store the videos in an Amazon S3 bucket. Create an AWS Storage Gateway file gateway to access the S3 bucket. Create a user data script for the web servers to mount the file gateway."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #269\n\nA company runs a production application on a fleet of Amazon EC2 instances. The application reads the data from an Amazon SQS queue and processes the messages in parallel. The message volume is unpredictable and often has intermittent traffic. This application should continually process messages without any downtime.Which solution meets these requirements MOST cost-effectively?",
        "answers": [
            2
        ],
        "options": [
            "Use Spot Instances exclusively to handle the maximum capacity required.",
            "Use Reserved Instances exclusively to handle the maximum capacity required.",
            "Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional capacity.",
            "Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle additional capacity."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #270\n\nA company has a hybrid application hosted on multiple on-premises servers with static IP addresses. There is already a VPN that provides connectivity between the VPC and the on-premises network. The company wants to distribute TCP traffic across the on-premises servers for internet users.What should a solutions architect recommend to provide a highly available and scalable solution?",
        "answers": [
            0
        ],
        "options": [
            "Launch an internet-facing Network Load Balancer (NLB) and register on-premises IP addresses with the NLB.",
            "Launch an internet-facing Application Load Balancer (ALB) and register on-premises IP addresses with the ALB.",
            "Launch an Amazon EC2 instance, attach an Elastic IP address, and distribute traffic to the on-premises servers.",
            "Launch an Amazon EC2 instance with public IP addresses in an Auto Scaling group and distribute traffic to the on-premises servers."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #271\n\nManagement has decided to deploy all AWS VPCs with IPv6 enabled. After some time, a solutions architect tries to launch a new instance and receives an error stating that there is not enough IP address space available in the subnet.What should the solutions architect do to fix this?",
        "answers": [
            2
        ],
        "options": [
            "Check to make sure that only IPv6 was used during the VPC creation.",
            "Create a new IPv4 subnet with a larger range, and then launch the instance.",
            "Create a new IPv6-only subnet with a large range, and then launch the instance.",
            "Disable the IPv4 subnet and migrate all instances to IPv6 only. Once that is complete, launch the instance."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #272\n\nA company has a build server that is in an Auto Scaling group and often has multiple Linux instances running. The build server requires consistent and mountable shared NFS storage for jobs and configurations.Which storage option should a solutions architect recommend?",
        "answers": [
            3
        ],
        "options": [
            "Amazon S3",
            "Amazon FSx",
            "Amazon Elastic Block Store (Amazon EBS)",
            "Amazon Elastic File System (Amazon EFS)"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/efs/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #273\n\nA company has an image processing workload running on Amazon Elastic Container Service (Amazon ECS) in two private subnets. Each private subnet uses aNAT instance for internet access. All images are stored in Amazon S3 buckets. The company is concerned about the data transfer costs between Amazon ECS and Amazon S3.What should a solutions architect do to reduce costs?",
        "answers": [
            2
        ],
        "options": [
            "Configure a NAT gateway to replace the NAT instances.",
            "Configure a gateway endpoint for traffic destined to Amazon S3.",
            "Configure an interface endpoint for traffic destined to Amazon S3.",
            "Configure Amazon CloudFront for the S3 bucket storing the images."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #274\n\nThe financial application at a company stores monthly reports in an Amazon S3 bucket. The vice president of finance has mandated that all access to these reports be logged and that any modifications to the log files be detected.Which actions can a solutions architect take to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Use S3 server access logging on the bucket that houses the reports with the read and write data events and log file validation options enabled.",
            "Use S3 server access logging on the bucket that houses the reports with the read and write management events and log file validation options enabled.",
            "Use AWS CloudTrail to create a new trail. Configure the trail to log read and write data events on the S3 bucket that houses the reports. Log these events to a new bucket, and enable log file validation.",
            "Use AWS CloudTrail to create a new trail. Configure the trail to log read and write management events on the S3 bucket that houses the reports. Log these events to a new bucket, and enable log file validation."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #275\n\nA company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred.Which solution meets these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises systems to mount the Snowball S3 endpoint to provide local access to the data.",
            "Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on-premises systems with local access to the data.",
            "Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of data to cache locally. Mount the gateway storage volumes to provide local access to the data.",
            "Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #276\n\nA company is using a third-party vendor to manage its marketplace analytics. The vendor needs limited programmatic access to resources in the company's account. All the needed policies have been created to grant appropriate access.Which additional component will provide the vendor with the MOST secure access to the account?",
        "answers": [
            1
        ],
        "options": [
            "Create an IAM user.",
            "Implement a service control policy (SCP)",
            "Use a cross-account role with an external ID.",
            "Configure a single sign-on (SSO) identity provider."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html#data-from-iam",
        "topic": "Topic 1"
    },
    {
        "question": "Question #277\n\nA company is developing an ecommerce application that will consist of a load-balanced front end, a container-based application, and a relational database. A solutions architect needs to create a highly available solution that operates with as little manual intervention as possible.Which solutions meet these requirements? ",
        "answers": [
            0,
            3
        ],
        "options": [
            "Create an Amazon RDS DB instance in Multi-AZ mode.",
            "Create an Amazon RDS DB instance and one or more replicas in another Availability Zone.",
            "Create an Amazon EC2 instance-based Docker cluster to handle the dynamic application load.",
            "Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle the dynamic application load.",
            "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type to handle the dynamic application load."
        ],
        "explination": "Correct Answer: AD \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #278\n\nA company has an ecommerce application that stores data in an on-premises SQL database. The company has decided to migrate this database to AWS.However, as part of the migration, the company wants to find a way to attain sub-millisecond responses to common read requests.A solutions architect knows that the increase in speed is paramount and that a small percentage of stale data returned in the database reads is acceptable.What should the solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Build Amazon RDS read replicas.",
            "Build the database as a larger instance type.",
            "Build a database cache using Amazon ElastiCache.",
            "Build a database cache using Amazon Elasticsearch Service (Amazon ES)."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/redis/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #279\n\nA company has an application that ingests incoming messages. These messages are then quickly consumed by dozens of other applications and microservices.The number of messages varies drastically and sometimes spikes as high as 100,000 each second. The company wants to decouple the solution and increase scalability.Which solution meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Persist the messages to Amazon Kinesis Data Analytics. All the applications will read and process the messages.",
            "Deploy the application on Amazon EC2 instances in an Auto Scaling group, which scales the number of EC2 instances based on CPU metrics.",
            "Write the messages to Amazon Kinesis Data Streams with a single shard. All applications will read from the stream and process the messages.",
            "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with one or more Amazon Simple Queue Service (Amazon SQS) subscriptions. All applications then process the messages from the queues."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/kinesis/data-streams/faqs/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #280\n\nA solutions architect is designing the cloud architecture for a company that needs to host hundreds of machine learning models for its users. During startup, the models need to load up to 10 GB of data from Amazon S3 into memory, but they do not need disk access. Most of the models are used sporadically, but the users expect all of them to be highly available and accessible with low latency.Which solution meets the requirements and is MOST cost-effective?",
        "answers": [
            2
        ],
        "options": [
            "Deploy models as AWS Lambda functions behind an Amazon API Gateway for each model.",
            "Deploy models as Amazon Elastic Container Service (Amazon ECS) services behind an Application Load Balancer for each model.",
            "Deploy models as AWS Lambda functions behind a single Amazon API Gateway with path-based routing where one path corresponds to each model.",
            "Deploy models as Amazon Elastic Container Service (Amazon ECS) services behind a single Application Load Balancer with path-based routing where one path corresponds to each model."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #281\n\nA company has created a multi-tier application for its ecommerce website. The website uses an Application Load Balancer that resides in the public subnets, a web tier in the public subnets, and a MySQL cluster hosted on Amazon EC2 instances in the private subnets. The MySQL database needs to retrieve product catalog and pricing information that is hosted on the internet by a third-party provider. A solutions architect must devise a strategy that maximizes security without increasing operational overhead.What should the solutions architect do to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Deploy a NAT instance in the VPC. Route all the internet-based traffic through the NAT instance.",
            "Deploy a NAT gateway in the public subnets. Modify the private subnet route table to direct all internet-bound traffic to the NAT gateway.",
            "Configure an internet gateway and attach it to the VPC. Modify the private subnet route table to direct internet-bound traffic to the internet gateway.",
            "Configure a virtual private gateway and attach it to the VPC. Modify the private subnet route table to direct internet-bound traffic to the virtual private gateway."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #282\n\nA company is backing up on-premises databases to local file server shares using the SMB protocol. The company requires immediate access to 1 week of backup files to meet recovery objectives. Recovery after a week is less likely to occur, and the company can tolerate a delay in accessing those older backup files.What should a solutions architect do to meet these requirements with the LEAST operational effort?",
        "answers": [
            0
        ],
        "options": [
            "Deploy Amazon FSx for Windows File Server to create a file system with exposed file shares with sufficient storage to hold all the desired backups.",
            "Deploy an AWS Storage Gateway file gateway with sufficient storage to hold 1 week of backups. Point the backups to SMB shares from the file gateway.",
            "Deploy Amazon Elastic File System (Amazon EFS) to create a file system with exposed NFS shares with sufficient storage to hold all the desired backups.",
            "Continue to back up to the existing file shares. Deploy AWS Database Migration Service (AWS DMS) and define a copy task to copy backup files older than 1 week to Amazon S3, and delete the backup files from the local file store."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #283\n\nA company has developed a microservices application. It uses a client-facing API with Amazon API Gateway and multiple internal services hosted on AmazonEC2 instances to process user requests. The API is designed to support unpredictable surges in traffic, but internal services may become overwhelmed and unresponsive for a period of time during surges. A solutions architect needs to design a more reliable solution that reduces errors when internal services become unresponsive or unavailable.Which solution meets these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Use AWS Auto Scaling to scale up internal services when there is a surge in traffic.",
            "Use different Availability Zones to host internal services. Send a notification to a system administrator when an internal service becomes unresponsive.",
            "Use an Elastic Load Balancer to distribute the traffic between internal services. Configure Amazon CloudWatch metrics to monitor traffic to internal services.",
            "Use Amazon Simple Queue Service (Amazon SQS) to store user requests as they arrive. Change the internal services to retrieve the requests from the queue for processing."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #284\n\nA company is hosting 60 TB of production-level data in an Amazon S3 bucket. A solutions architect needs to bring that data on premises for quarterly audit requirements. This export of data must be encrypted while in transit. The company has low network bandwidth in place between AWS and its on-premises data center.What should the solutions architect do to meet these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Deploy AWS Migration Hub with 90-day replication windows for data transfer.",
            "Deploy an AWS Storage Gateway volume gateway on AWS. Enable a 90-day replication window to transfer the data.",
            "Deploy Amazon Elastic File System (Amazon EFS), with lifecycle policies enabled, on AWS. Use it to transfer the data.",
            "Deploy an AWS Snowball device in the on-premises data center after completing an export job request in the AWS Snowball console."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #285\n\nA company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to restrict access to audit team IAM user credentials according to the principle of least privilege. Company managers are worried about accidental deletion of documents in the S3 bucket and want a more secure solution.What should a solutions architect do to secure the audit documents?",
        "answers": [
            0
        ],
        "options": [
            "Enable the versioning and MFA Delete features on the S3 bucket.",
            "Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account.",
            "Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during audit dates.",
            "Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #286\n\nA solutions architect is designing a new API using Amazon API Gateway that will receive requests from users. The volume of requests is highly variable; several hours can pass without receiving a single request. The data processing will take place asynchronously, but should be completed within a few seconds after a request is made.Which compute service should the solutions architect have the API invoke to deliver the requirements at the lowest cost?",
        "answers": [
            1
        ],
        "options": [
            "An AWS Glue job",
            "An AWS Lambda function",
            "A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS)",
            "A containerized service hosted in Amazon ECS with Amazon EC2"
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #287\n\nA company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in another AWS Region with minimal downtime.What should a solutions architect do to meet these requirements with the LEAST amount of downtime?",
        "answers": [
            3
        ],
        "options": [
            "Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.",
            "Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be executed when needed. Configure DNS failover to point to the new disaster recovery Region's load balancer.",
            "Create an AWS CloudFormation template to create EC2 instances and a load balancer to be executed when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.",
            "Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger and AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #288\n\nA business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object storage. The chief information security officer has directed that no application traffic between the two services should traverse the public internet.Which capability should the solutions architect use to meet the compliance requirements?",
        "answers": [
            1
        ],
        "options": [
            "AWS Key Management Service (AWS KMS)",
            "VPC endpoint",
            "Private subnet",
            "Virtual private gateway"
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #289\n\nA solutions architect is designing a solution that requires frequent updates to a website that is hosted on Amazon S3 with versioning enabled. For compliance reasons, the older versions of the objects will not be accessed frequently and will need to be deleted after 2 years.What should the solutions architect recommend to meet these requirements at the LOWEST cost?",
        "answers": [
            1
        ],
        "options": [
            "Use S3 batch operations to replace object tags. Expire the objects based on the modified tags.",
            "Configure an S3 Lifecycle policy to transition older versions of objects to S3 Glacier. Expire the objects after 2 years.",
            "Enable S3 Event Notifications on the bucket that sends older objects to the Amazon Simple Queue Service (Amazon SQS) queue for further processing.",
            "Replicate older object versions to a new bucket. Use an S3 Lifecycle policy to expire the objects in the new bucket after 2 years."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #290\n\nA company runs an application on an Amazon EC2 instance backed by Amazon Elastic Block Store (Amazon EBS). The instance needs to be available for 12 hours daily. The company wants to save costs by making the instance unavailable outside the window required for the application. However, the contents of the instance's memory must be preserved whenever the instance is unavailable.What should a solutions architect do to meet this requirement?",
        "answers": [
            1
        ],
        "options": [
            "Stop the instance outside the application's availability window. Start up the instance again when required.",
            "Hibernate the instance outside the application's availability window. Start up the instance again when required.",
            "Use Auto Scaling to scale down the instance outside the application's availability window. Scale up the instance when required.",
            "Terminate the instance outside the application's availability window. Launch the instance by using a preconfigured Amazon Machine Image (AMI) when required."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #21\n\nA company's web application uses an Amazon RDS PostgreSQL DB instance to store its application data. During the financial closing period at the start of every month, Accountants run large queries that impact the database's performance due to high usage. The company wants to minimize the impact that the reporting activity has on the web application.What should a solutions architect do to reduce the impact on the database with the LEAST amount of effort?",
        "answers": [
            0
        ],
        "options": [
            "Create a read replica and direct reporting traffic to the replica.",
            "Create a Multi-AZ database and direct reporting traffic to the standby.",
            "Create a cross-Region read replica and direct reporting traffic to the replica.",
            "Create an Amazon Redshift database and direct reporting traffic to the Amazon Redshift database."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Amazon RDS uses the MariaDB, MySQL, Oracle, PostgreSQL, and Microsoft SQL Server DB engines' built-in replication functionality to create a special type ofDB instance called a read replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the read replica. You can reduce the load on your source DB instance by routing read queries from your applications to the read replica.When you create a read replica, you first specify an existing DB instance as the source. Then Amazon RDS takes a snapshot of the source instance and creates a read-only instance from the snapshot. Amazon RDS then uses the asynchronous replication method for the DB engine to update the read replica whenever there is a change to the source DB instance. The read replica operates as a DB instance that allows only read-only connections. Applications connect to a read replica the same way they do to any DB instance. Amazon RDS replicates all databases in the source DB instance.Reference:https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #22\n\nA company wants to migrate a high performance computing (HPC) application and data from on-premises to the AWS Cloud. The company uses tiered storage on premises with hot high-performance parallel storage to support the application during periodic runs of the application, and more economical cold storage to hold the data when the application is not actively running.Which combination of solutions should a solutions architect recommend to support the storage needs of the application? ",
        "answers": [
            0,
            3
        ],
        "options": [
            "Amazon S3 for cold data storage",
            "Amazon Elastic File System (Amazon EFS) for cold data storage",
            "Amazon S3 for high-performance parallel storage",
            "Amazon FSx for Lustre for high-performance parallel storage",
            "Amazon FSx for Windows for high-performance parallel storage"
        ],
        "total_times_question_attempted": 2,
        "correct_times_question_attempted": 1,
        "current_probability": 0.4
    },
    {
        "question": "Question #23\n\nA company's application is running on Amazon EC2 instances in a single Region. In the event of a disaster, a solutions architect needs to ensure that the resources can also be deployed to a second Region.Which combination of actions should the solutions architect take to accomplish this? ",
        "answers": [
            1,
            3
        ],
        "options": [
            "Detach a volume on an EC2 instance and copy it to Amazon S3.",
            "Launch a new EC2 instance from an Amazon Machine Image (AMI) in a new Region.",
            "Launch a new EC2 instance in a new Region and copy a volume from Amazon S3 to the new instance.",
            "Copy an Amazon Machine Image (AMI) of an EC2 instance and specify a different Region for the destination.",
            "Copy an Amazon Elastic Block Store (Amazon EBS) volume from Amazon S3 and launch an EC2 instance in the destination Region using that EBS volume."
        ],
        "explination": "Correct Answer: BD \ud83d\uddf3\ufe0f                                                Cross Region EC2 AMI Copy -We know that you want to build applications that span AWS Regions and we're working to provide you with the services and features needed to do so. We started out by launching the EBS Snapshot Copy feature late last year. This feature gave you the ability to copy a snapshot from Region to Region with just a couple of clicks. In addition, last month we made a significant reduction (26% to 83%) in the cost of transferring data between AWS Regions, making it less expensive to operate in more than one AWS region.Today we are introducing a new feature: Amazon Machine Image (AMI) Copy. AMI Copy enables you to easily copy your Amazon Machine Images between AWSRegions. AMI Copy helps enable several key scenarios including:Simple and Consistent Multi-Region Deployment \u05d2\u20ac\" You can copy an AMI from one region to another, enabling you to easily launch consistent instances based on the same AMI into different regions.Scalability \u05d2\u20ac\" You can more easily design and build world-scale applications that meet the needs of your users, regardless of their location.Performance \u05d2\u20ac\" You can increase performance by distributing your application and locating critical components of your application in closer proximity to your users.You can also take advantage of region-specific features such as instance types or other AWS services.Even Higher Availability \u05d2\u20ac\" You can design and deploy applications across AWS regions, to increase availability.Once the new AMI is in an Available state the copy is complete.Reference:https://aws.amazon.com/blogs/aws/ec2-ami-copy-between-regions/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #24\n\nA solutions architect needs to ensure that API calls to Amazon DynamoDB from Amazon EC2 instances in a VPC do not traverse the internet.What should the solutions architect do to accomplish this? ",
        "answers": [
            0,
            1
        ],
        "options": [
            "Create a route table entry for the endpoint.",
            "Create a gateway endpoint for DynamoDB.",
            "Create a new DynamoDB table that uses the endpoint.",
            "Create an ENI for the endpoint in each of the subnets of the VPC.",
            "Create a security group entry in the default security group to provide access."
        ],
        "explination": "Correct Answer: AB \ud83d\uddf3\ufe0f                                                A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.Gateway endpoints -A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:Amazon S3 -DynamoDB -Reference:https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #25\n\nA company's legacy application is currently relying on a single-instance Amazon RDS MySQL database without encryption. Due to new compliance requirements, all existing and new data in this database must be encrypted.How should this be accomplished?",
        "answers": [
            2
        ],
        "options": [
            "Create an Amazon S3 bucket with server-side encryption enabled. Move all the data to Amazon S3. Delete the RDS instance.",
            "Enable RDS Multi-AZ mode with encryption at rest enabled. Perform a failover to the standby instance to delete the original instance.",
            "Take a Snapshot of the RDS instance. Create an encrypted copy of the snapshot. Restore the RDS instance from the encrypted snapshot.",
            "Create an RDS read replica with encryption at rest enabled. Promote the read replica to master and switch the application over to the new master. Delete the old RDS instance."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                How do I encrypt Amazon RDS snapshots?The following steps are applicable to Amazon RDS for MySQL, Oracle, SQL Server, PostgreSQL, or MariaDB.Important: If you use Amazon Aurora, you can restore an unencrypted Aurora DB cluster snapshot to an encrypted Aurora DB cluster if you specify an AWS KeyManagement Service (AWS KMS) encryption key when you restore from the unencrypted DB cluster snapshot. For more information, see Limitations of AmazonRDS Encrypted DB Instances.Open the Amazon RDS console, and then choose Snapshots from the navigation pane.Select the snapshot that you want to encrypt.Under Snapshot Actions, choose Copy Snapshot.Choose your Destination Region, and then enter your New DB Snapshot Identifier.Change Enable Encryption to Yes.Select your Master Key from the list, and then choose Copy Snapshot.After the snapshot status is available, the Encrypted field will be True to indicate that the snapshot is encrypted.You now have an encrypted snapshot of your DB. You can use this encrypted DB snapshot to restore the DB instance from the DB snapshot.Reference:https://aws.amazon.com/premiumsupport/knowledge-center/encrypt-rds-snapshots/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #26\n\nA manufacturing company wants to implement predictive maintenance on its machinery equipment. The company will install thousands of IoT sensors that will send data to AWS in real time. A solutions architect is tasked with implementing a solution that will receive events in an ordered manner for each machinery asset and ensure that data is saved for further processing at a later time.Which solution would be MOST efficient?",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon Kinesis Data Streams for real-time events with a partition for each equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon S3.",
            "Use Amazon Kinesis Data Streams for real-time events with a shard for each equipment asset. Use Amazon Kinesis Data Firehose to save data to Amazon Elastic Block Store (Amazon EBS).",
            "Use an Amazon SQS FIFO queue for real-time events with one queue for each equipment asset. Trigger an AWS Lambda function for the SQS queue to save data to Amazon Elastic File System (Amazon EFS).",
            "Use an Amazon SQS standard queue for real-time events with one queue for each equipment asset. Trigger an AWS Lambda function from the SQS queue to save data to Amazon S3."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #27\n\nA company's website runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The website has a mix of dynamic and static content. Users around the globe are reporting that the website is slow.Which set of actions will improve website performance for users worldwide?",
        "answers": [
            0
        ],
        "options": [
            "Create an Amazon CloudFront distribution and configure the ALB as an origin. Then update the Amazon Route 53 record to point to the CloudFront distribution.",
            "Create a latency-based Amazon Route 53 record for the ALB. Then launch new EC2 instances with larger instance sizes and register the instances with the ALB.",
            "Launch new EC2 instances hosting the same web application in different Regions closer to the users. Then register instances with the same ALB using cross- Region VPC peering.",
            "Host the website in an Amazon S3 bucket in the Regions closest to the users and delete the ALB and EC2 instances. Then update an Amazon Route 53 record to point to the S3 buckets."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                What Is Amazon CloudFront?Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users.CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving withCloudFront, the user is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.Routing traffic to an Amazon CloudFront web distribution by using your domain name.If you want to speed up delivery of your web content, you can use Amazon CloudFront, the AWS content delivery network (CDN). CloudFront can deliver your entire website \u05d2\u20ac\" including dynamic, static, streaming, and interactive content \u05d2\u20ac\" by using a global network of edge locations. Requests for your content are automatically routed to the edge location that gives your users the lowest latency.To use CloudFront to distribute your content, you create a web distribution and specify settings such as the Amazon S3 bucket or HTTP server that you wantCloudFront to get your content from, whether you want only selected users to have access to your content, and whether you want to require users to use HTTPS.When you create a web distribution, CloudFront assigns a domain name to the distribution, such asd111111abcdef8.cloudfront.net. You can use this domain name in the URLs for your content, for example:[1]Alternatively, you might prefer to use your own domain name in URLs, for example:[1]If you want to use your own domain name, use Amazon Route 53 to create an alias record that points to your CloudFront distribution. An alias record is a Route53 extension to DNS. It's similar to a CNAME record, but you can create an alias record both for the root domain, such as example.com, and for subdomains, such aswww.example.com. (You can create CNAME records only for subdomains.) When Route 53 receives a DNS query that matches the name and type of an alias record, Route 53 responds with the domain name that is associated with your distribution.Reference:https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #28\n\nA company has been storing analytics data in an Amazon RDS instance for the past few years. The company asked a solutions architect to find a solution that allows users to access this data using an API. The expectation is that the application will experience periods of inactivity but could receive bursts of traffic within seconds.Which solution should the solutions architect suggest?",
        "answers": [
            2
        ],
        "options": [
            "Set up an Amazon API Gateway and use Amazon ECS.",
            "Set up an Amazon API Gateway and use AWS Elastic Beanstalk.",
            "Set up an Amazon API Gateway and use AWS Lambda functions.",
            "Set up an Amazon API Gateway and use Amazon EC2 with Auto Scaling."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                AWS Lambda -With Lambda, you can run code for virtually any type of application or backend service \u05d2\u20ac\" all with zero administration. Just upload your code and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app.How it works -Amazon API Gateway -Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs andWebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management. API Gateway has no minimum fees or startup costs. You pay for the API calls you receive and the amount of data transferred out and, with the API Gateway tiered pricing model, you can reduce your cost as your API usage scales.Reference:https://aws.amazon.com/lambda/https://aws.amazon.com/api-gateway/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #29\n\nA company must generate sales reports at the beginning of every month. The reporting process launches 20 Amazon EC2 instances on the first of the month. The process runs for 7 days and cannot be interrupted. The company wants to minimize costs.Which pricing model should the company choose?",
        "answers": [
            3
        ],
        "options": [
            "Reserved Instances",
            "Spot Block Instances",
            "On-Demand Instances",
            "Scheduled Reserved Instances"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Explanation -Scheduled Reserved Instances -Scheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. You reserve the capacity in advance, so that you know it is available when you need it. You pay for the time that the instances are scheduled, even if you do not use them.Scheduled Instances are a good choice for workloads that do not run continuously, but do run on a regular schedule. For example, you can use ScheduledInstances for an application that runs during business hours or for batch processing that runs at the end of the week.If you require a capacity reservation on a continuous basis, Reserved Instances might meet your needs and decrease costs.How Scheduled Instances Work -Amazon EC2 sets aside pools of EC2 instances in each Availability Zone for use as Scheduled Instances. Each pool supports a specific combination of instance type, operating system, and network.To get started, you must search for an available schedule. You can search across multiple pools or a single pool. After you locate a suitable schedule, purchase it.You must launch your Scheduled Instances during their scheduled time periods, using a launch configuration that matches the following attributes of the schedule that you purchased: instance type, Availability Zone, network, and platform. When you do so, Amazon EC2 launches EC2 instances on your behalf, based on the specified launch specification. Amazon EC2 must ensure that the EC2 instances have terminated by the end of the current scheduled time period so that the capacity is available for any other Scheduled Instances it is reserved for. Therefore, Amazon EC2 terminates the EC2 instances three minutes before the end of the current scheduled time period.You can't stop or reboot Scheduled Instances, but you can terminate them manually as needed. If you terminate a Scheduled Instance before its current scheduled time period ends, you can launch it again after a few minutes. Otherwise, you must wait until the next scheduled time period.The following diagram illustrates the lifecycle of a Scheduled Instance.Reference:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #30\n\nA gaming company has multiple Amazon EC2 instances in a single Availability Zone for its multiplayer game that communicates with users on Layer 4. The chief technology officer (CTO) wants to make the architecture highly available and cost-effective.What should a solutions architect do to meet these requirements? ?",
        "answers": [
            2,
            4
        ],
        "options": [
            "Increase the number of EC2 instances.",
            "Decrease the number of EC2 instances.",
            "Configure a Network Load Balancer in front of the EC2 instances.",
            "Configure an Application Load Balancer in front of the EC2 instances.",
            "Configure an Auto Scaling group to add or remove instances in multiple Availability Zones automatically."
        ],
        "explination": "Correct Answer: CE \ud83d\uddf3\ufe0f                                                Network Load Balancer overview -A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration.When you enable an Availability Zone for the load balancer, Elastic Load Balancing creates a load balancer node in the Availability Zone. By default, each load balancer node distributes traffic across the registered targets in its Availability Zone only. If you enable cross-zone load balancing, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. For more information, see Availability Zones.If you enable multiple Availability Zones for your load balancer and ensure that each target group has at least one target in each enabled Availability Zone, this increases the fault tolerance of your applications. For example, if one or more target groups does not have a healthy target in an Availability Zone, we remove theIP address for the corresponding subnet from DNS, but the load balancer nodes in the other Availability Zones are still available to route traffic. If a client doesn't honor the time-to-live (TTL) and sends requests to the IP address after it is removed from DNS, the requests fail.For TCP traffic, the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, destination port, and TCP sequence number. The TCP connections from a client have different source ports and sequence numbers, and can be routed to different targets. Each individual TCP connection is routed to a single target for the life of the connection.For UDP traffic, the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, and destination port. A UDP flow has the same source and destination, so it is consistently routed to a single target throughout its lifetime. Different UDP flows have different source IP addresses and ports, so they can be routed to different targets.An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.The size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. You can adjust its size to meet demand, either manually or by using automatic scaling.An Auto Scaling group starts by launching enough instances to meet its desired capacity. It maintains this number of instances by performing periodic health checks on the instances in the group. The Auto Scaling group continues to maintain a fixed number of instances even if an instance becomes unhealthy. If an instance becomes unhealthy, the group terminates the unhealthy instance and launches another instance to replace it.Reference:https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #291\n\nA solutions architect is creating a new VPC design. There are two public subnets for the load balancer, two private subnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions architect has already created a security group for the load balancer allowing port 443 from0.0.0.0/0. Company policy requires that each resource has the least access required to still be able to perform its tasks.Which additional configuration strategy should the solutions architect use to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.",
            "Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.",
            "Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.",
            "Create a network ACL for the web servers and allow port 443 from the load balancer. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #292\n\nA company hosts historical weather records in Amazon S3. The records are downloaded from the company's website by a way of a URL that resolves to a domain name. Users all over the world access this content through subscriptions. A third-party provider hosts the company's root domain name, but the company recently migrated some of its services to Amazon Route 53. The company wants to consolidate contracts, reduce latency for users, and reduce costs related to serving the application to subscribers.Which solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Create a web distribution on Amazon CloudFront to serve the S3 content for the application. Create a CNAME record in a Route 53 hosted zone that points to the CloudFront distribution, resolving to the application's URL domain name.",
            "Create a web distribution on Amazon CloudFront to serve the S3 content for the application. Create an ALIAS record in the Amazon Route 53 hosted zone that points to the CloudFront distribution, resolving to the application's URL domain name.",
            "Create an A record in a Route 53 hosted zone for the application. Create a Route 53 traffic policy for the web application, and configure a geolocation rule. Configure health checks to check the health of the endpoint and route DNS queries to other endpoints if an endpoint is unhealthy.",
            "Create an A record in a Route 53 hosted zone for the application. Create a Route 53 traffic policy for the web application, and configure a geoproximity rule. Configure health checks to check the health of the endpoint and route DNS queries to other endpoints if an endpoint is unhealthy."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #293\n\nA company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway to deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests before dispatching them to the processing microservices.The company provisioned as much DynamoDB throughput as its budget allows, but the company is still experiencing availability issues and is losing user requests.What should a solutions architect do to address this issue without impacting existing users?",
        "answers": [
            1
        ],
        "options": [
            "Add throttling on the API Gateway with server-side throttling limits.",
            "Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.",
            "Create a secondary index in DynamoDB for the table with the user requests.",
            "Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #294\n\nA company is moving its on-premises applications to Amazon EC2 instances. However, as a result of fluctuating compute requirements, the EC2 instances must always be ready to use between 8 AM and 5 PM in specific Availability Zones.Which EC2 instances should the company choose to run the applications?",
        "answers": [
            0
        ],
        "options": [
            "Scheduled Reserved Instances",
            "On-Demand Instances",
            "Spot Instances as part of a Spot Fleet",
            "EC2 instances in an Auto Scaling group"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #295\n\nA company is launching a new application deployed on an Amazon Elastic Container Service (Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is monitoring CPU and memory usage because it is expecting high traffic to the application upon its launch. However, the company wants to reduce costs when utilization decreases.What should a solutions architect recommend?",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon EC2 Auto Scaling to scale at certain periods based on previous traffic patterns.",
            "Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm.",
            "Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.",
            "Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #296\n\nA company is building an application on Amazon EC2 instances that generates temporary transactional data. The application requires access to data storage that can provide configurable and consistent IOPS.What should a solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "Provision an EC2 instance with a Throughput Optimized HDD (st1) root volume and a Cold HDD (sc1) data volume.",
            "Provision an EC2 instance with a Throughput Optimized HDD (st1) volume that will serve as the root and data volume.",
            "Provision an EC2 instance with a General Purpose SSD (gp2) root volume and Provisioned IOPS SSD (io1) data volume.",
            "Provision an EC2 instance with a General Purpose SSD (gp2) root volume. Configure the application to store its data in an Amazon S3 bucket."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #297\n\nA solutions architect needs to design a resilient solution for Windows users' home directories. The solution must provide fault tolerance, file-level backup and recovery, and access control, based upon the company's Active Directory.Which storage solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Configure Amazon S3 to store the users' home directories. Join Amazon S3 to Active Directory.",
            "Configure a Multi-AZ file system with Amazon FSx for Windows File Server. Join Amazon FSx to Active Directory.",
            "Configure Amazon Elastic File System (Amazon EFS) for the users' home directories. Configure AWS Single Sign-On with Active Directory.",
            "Configure Amazon Elastic Block Store (Amazon EBS) to store the users' home directories. Configure AWS Single Sign-On with Active Directory."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #298\n\nA company wants to move a multi-tiered application from on premises to the AWS Cloud to improve the application's performance. The application consists of application tiers that communicate with each other by way of RESTful services. Transactions are dropped when one tier becomes overloaded. A solutions architect must design a solution that resolves these issues and modernizes the application.Which solution meets these requirements and is the MOST operationally efficient?",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application layer. Use Amazon Simple Queue Service (Amazon SQS) as the communication layer between application services.",
            "Use Amazon CloudWatch metrics to analyze the application performance history to determine the server's peak utilization during the performance failures. Increase the size of the application server's Amazon EC2 instances to meet the peak requirements.",
            "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SNS queue length and scale up and down as required.",
            "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #299\n\nA company serves a multilingual website from a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). This architecture is currently running in the us-west-1 Region but is exhibiting high request latency for users located in other parts of the world.The website needs to serve requests quickly and efficiently regardless of a user's location. However, the company does not want to recreate the existing architecture across multiple Regions.How should a solutions architect accomplish this?",
        "answers": [
            1
        ],
        "options": [
            "Replace the existing architecture with a website served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin.",
            "Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to only cache based on the Accept-Language request header.",
            "Set up Amazon API Gateway with the ALB as an integration. Configure API Gateway to use an HTTP integration type. Set up an API Gateway stage to enable the API cache.",
            "Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the instances plus the ALB behind an Amazon Route 53 record set with a geolocation routing policy."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #300\n\nA software vendor is deploying a new software-as-a-service (SaaS) solution that will be utilized by many AWS users. The service is hosted in a VPC behind aNetwork Load Balancer. The software vendor wants to provide access to this service to users with the least amount of administrative overhead and without exposing the service to the public internet.What should a solutions architect do to accomplish this goal?",
        "answers": [
            2
        ],
        "options": [
            "Create a peering VPC connection from each user's VPC to the software vendor's VPC.",
            "Deploy a transit VPC in the software vendor's AWS account. Create a VPN connection with each user account.",
            "Connect the service in the VPC with an AWS Private Link endpoint. Have users subscribe to the endpoint.",
            "Deploy a transit VPC in the software vendor's AWS account. Create an AWS Direct Connect connection with each user account."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #301\n\nA user wants to list the IAM role that is attached to their Amazon EC2 instance. The user has login access to the EC2 instance but does not have IAM permissions.What should a solutions architect do to retrieve this information?",
        "answers": [
            0
        ],
        "options": [
            "Run the following EC2 command: curl http://169.254.169.254/latest/meta-data/iam/info",
            "Run the following EC2 command: curl http://169.254.169.254/latest/user-data/iam/info",
            "Run the following EC2 command: http://169.254.169.254/latest/dynamic/instance-identity/",
            "Run the following AWS CLI command: aws iam get-instance-profile --instance-profile-name ExampleInstanceProfile"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #302\n\nA company has an application that is hosted on Amazon EC2 instances in two private subnets. A solutions architect must make the application available on the public internet with the least amount of administrative effort.What should the solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "Create a load balancer and associate two public subnets from the same Availability Zones as the private instances. Add the private instances to the load balancer.",
            "Create a load balancer and associate two private subnets from the same Availability Zones as the private instances. Add the private instances to the load balancer.",
            "Create an Amazon Machine Image (AMI) of the instances in the private subnet and restore in the public subnet. Create a load balancer and associate two public subnets from the same Availability Zones as the public instances.",
            "Create an Amazon Machine Image (AMI) of the instances in the private subnet and restore in the public subnet. Create a load balancer and associate two private subnets from the same Availability Zones as the public instances."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #303\n\nA company has two applications: a sender application that sends messages with payloads to be processed and a processing application intended to receive messages with payloads. The company wants to implement an AWS service to handle messages between the two applications. The sender application can send about 1,000 messages each hour. The messages may take up to 2 days to be processed. If the messages fail to process, they must be retained so that they do not impact the processing of any remaining messages.Which solution meets these requirements and is the MOST operationally efficient?",
        "answers": [
            2
        ],
        "options": [
            "Set up an Amazon EC2 instance running a Redis database. Configure both applications to use the instance. Store, process, and delete the messages, respectively.",
            "Use an Amazon Kinesis data stream to receive the messages from the sender application. Integrate the processing application with the Kinesis Client Library (KCL).",
            "Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue. Configure a dead-letter queue to collect the messages that failed to process.",
            "Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications to process. Integrate the sender application to write to the SNS topic."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #304\n\nA company's website hosted on Amazon EC2 instances processes classified data stored in Amazon S3. Due to security concerns, the company requires a private and secure connection between its EC2 resources and Amazon S3.Which solution meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Set up S3 bucket policies to allow access from a VPC endpoint.",
            "Set up an IAM policy to grant read-write access to the S3 bucket.",
            "Set up a NAT gateway to access resources outside the private subnet.",
            "Set up an access key ID and a secret access key to access the S3 bucket."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-overview.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #305\n\nA company hosts its multi-tier, public web application in the AWS Cloud. The web application runs on Amazon EC2 instances, and its database runs on AmazonRDS. The company is anticipating a large increase in sales during an upcoming holiday weekend. A solutions architect needs to build a solution to analyze the performance of the web application with a granularity of no more than 2 minutes.What should the solutions architect do to meet this requirement?",
        "answers": [
            1
        ],
        "options": [
            "Send Amazon CloudWatch logs to Amazon Redshift. Use Amazon QuickSight to perform further analysis.",
            "Enable detailed monitoring on all EC2 instances. Use Amazon CloudWatch metrics to perform further analysis.",
            "Create an AWS Lambda function to fetch EC2 logs from Amazon CloudWatch Logs. Use Amazon CloudWatch metrics to perform further analysis.",
            "Send EC2 logs to Amazon S3. Use Amazon Redshift to fetch logs from the S3 bucket to process raw data for further analysis with Amazon QuickSight."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #306\n\nA company has developed a new video game as a web application. The application is in a three-tier architecture in a VPC with Amazon RDS for MySQL. In the database layer several players will compete concurrently online. The game's developers want to display a top-10 scoreboard in near-real time and offer the ability to stop and restore the game while preserving the current scores.What should a solutions architect do to meet these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web application to display.",
            "Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.",
            "Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard in a section of the application.",
            "Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and serve the read traffic to the web application."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #307\n\nA company is moving its on-premises Oracle database to Amazon Aurora PostgreSQL. The database has several applications that write to the same tables. The applications need to be migrated one by one with a month in between each migration Management has expressed concerns that the database has a high number of reads and writes. The data must be kept in sync across both databases throughout tie migration.What should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task and a table mapping to select all cables.",
            "Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.",
            "Use the AWS Schema Conversion Tool with AWS DataBase Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.",
            "Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a compute optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select the largest tables."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #308\n\nA company recently migrated a message processing system to AWS. The system receives messages into an ActiveMQ queue running on an Amazon EC2 instance. Messages are processed by a consumer application running on Amazon EC2. The consumer application processes the messages and writes results to a MySQL database running on Amazon EC2. The company wants this application to be highly available with low operational complexity.Which architecture offers the HIGHEST availability?",
        "answers": [
            3
        ],
        "options": [
            "Add a second ActiveMQ server to another Availability Zone. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.",
            "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.",
            "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Use Amazon RDS for MySQL with Multi-AZ enabled.",
            "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an Auto Scaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for MySQL with Multi-AZ enabled."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #309\n\nA company is planning on deploying a newly built application on AWS in a default VPC. The application will consist of a web layer and database layer. The web server was created in public subnets, and the MySQL database was created in private subnets. All subnets are created with the default network ACL settings, and the default security group in the VPC will be replaced with new custom security groups.The following are the key requirements:\u2711 The web servers must be accessible only to users on an SSL connection.\u2711 The database should be accessible to the web layer, which is created in a public subnet only.\u2711 All traffic to and from the IP range 182.20.0.0/16 subnet should be blocked.Which combination of steps meets these requirements? ",
        "answers": [
            1,
            3
        ],
        "options": [
            "Create a database server security group with inbound and outbound rules for MySQL port 3306 traffic to and from anywhere (0 0.0.0/0).",
            "Create a database server security group with an inbound rule for MySQL port 3306 and specify the source as a web server security group.",
            "Create a web server security group with an inbound allow rule for HTTPS port 443 traffic from anywhere (0.0.0.0/0) and an inbound deny rule for IP range 182.20.0.0/16.",
            "Create a web server security group with an inbound rule for HTTPS port 443 traffic from anywhere (0.0.0.0/0). Create network ACL inbound and outbound deny rules for IP range 182.20.0.0/16.",
            "Create a web server security group with inbound and outbound rules for HTTPS port 443 traffic to and from anywhere (0.0.0.0/0). Create a network ACL inbound deny rule for IP range 182.20.0.0/16."
        ],
        "explination": "Correct Answer: BD \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #310\n\nA company has an on-premises application that collects data and stores it to an on-premises NFS server. The company recently set up a 10 Gbps AWS DirectConnect connection. The company is running out of storage capacity on premises. The company needs to migrate the application data from on premises to theAWS Cloud while maintaining low-latency access to the data from the on-premises application.What should a solutions architect do to meet these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Deploy AWS Storage Gateway for the application data, and use the file gateway to store the data in Amazon S3. Connect the on-premises application servers to the file gateway using NFS.",
            "Attach an Amazon Elastic File System (Amazon EFS) file system to the NFS server, and copy the application data to the EFS file system. Then connect the on-premises application to Amazon EFS.",
            "Configure AWS Storage Gateway as a volume gateway. Make the application data available to the on-premises application from the NFS server and with Amazon Elastic Block Store (Amazon EBS) snapshots.",
            "Create an AWS DataSync agent with the NFS server as the source location and an Amazon Elastic File System (Amazon EFS) file system as the destination for application data transfer. Connect the on-premises application to the EFS file system."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #311\n\nA solutions architect needs to design a network that will allow multiple Amazon EC2 instances to access a common data source used for mission-critical data that can be accessed by all the EC2 instances simultaneously. The solution must be highly scalable, easy to implement and support the NFS protocol.Which solution meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Create an Amazon EFS file system. Configure a mount target in each Availability Zone. Attach each instance to the appropriate mount target.",
            "Create an additional EC2 instance and configure it as a file server. Create a security group that allows communication between the instances and apply that to the additional instance.",
            "Create an Amazon S3 bucket with the appropriate permissions. Create a role in AWS IAM that grants the correct permissions to the S3 bucket. Attach the role to the EC2 instances that need access to the data.",
            "Create an Amazon EBS volume with the appropriate permissions. Create a role in AWS IAM that grants the correct permissions to the EBS volume. Attach the role to the EC2 instances that need access to the data."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #312\n\nA company hosts its application using Amazon Elastic Container Service (Amazon ECS) and wants to ensure high availability. The company wants to be able to deploy updates to its application even if nodes in one Availability Zone are not accessible.The expected request volume for the application is 100 requests per second, and each container task is able to serve at least 60 requests per second. The company set up Amazon ECS with a rolling update deployment type with the minimum healthy percent parameter set to 50% and the maximum percent set to100%.Which configuration of tasks and Availability Zones meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Deploy the application across two Availability Zones, with one task in each Availability Zone.",
            "Deploy the application across two Availability Zones, with two tasks in each Availability Zone.",
            "Deploy the application across three Availability Zones, with one task in each Availability Zone.",
            "Deploy the application across three Availability Zones, with two tasks in each Availability Zone."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #313\n\nA solutions architect wants all new users to have specific complexity requirements and mandatory rotation periods for IAM user passwords. What should the solutions architect do to accomplish this?",
        "answers": [
            0
        ],
        "options": [
            "Set an overall password policy for the entire AWS account",
            "Set a password policy for each IAM user in the AWS account.",
            "Use third-party vendor software to set password requirements.",
            "Attach an Amazon CloudWatch rule to the Create_newuser event to set the password with the appropriate requirements."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #314\n\nA company wants to improve the availability and performance of its hybrid application. The application consists of a stateful TCP-based workload hosted onAmazon EC2 instances in different AWS Regions and a stateless UOP-based workload hosted on premises.Which combination of actions should a solutions architect take to improve availability and performance? ",
        "answers": [
            0,
            3
        ],
        "options": [
            "Create an accelerator using AWS Global Accelerator. Add the load balancers as endpoints.",
            "Create an Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-based routing to route requests to the load balancers.",
            "Configure two Application Load Balancers in each Region. The first will route to the EC2 endpoints and the second will route to the on-premises endpoints.",
            "Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure a Network Load Balancer in each Region that routes to the on- premises endpoints.",
            "Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure an Application Load Balancer in each Region that routes to the on-premises endpoints"
        ],
        "explination": "Correct Answer: AD \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #315\n\nA solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The application will run on Amazon EC2 On-DemandInstances and will automatically scale across multiple Availability Zones. The EC2 instances will scale up and down frequently throughout the day. An ApplicationLoad Balancer (ALB) will handle the load distribution. The architecture needs to support distributed session data management. The company is willing to make changes to code if needed.What should the solutions architect do to ensure that the architecture supports distributed session data management?",
        "answers": [
            0
        ],
        "options": [
            "Use Amazon ElastiCache to manage and store session data.",
            "Use session affinity (sticky sessions) of the ALB to manage session data.",
            "Use Session Manager from AWS Systems Manager to manage the session.",
            "Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage the session."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #316\n\nA company has an ecommerce application running in a single VPC. The application stack has a single web server and an Amazon RDS Multi-AZ DB instance.The company launches new products twice a month. This increases website traffic by approximately 400% for a minimum of 72 hours. During product launches, users experience slow response times and frequent timeout errors in their browsers.What should a solutions architect do to mitigate the slow response times and timeout errors while minimizing operational overhead?",
        "answers": [
            0
        ],
        "options": [
            "Increase the instance size of the web server.",
            "Add an Application Load Balancer and an additional web server.",
            "Add Amazon EC2 Auto Scaling and an Application Load Balancer.",
            "Deploy an Amazon ElastiCache cluster to store frequently accessed data."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #317\n\nA solutions architect is designing an architecture to run a third-party database server. The database software is memory intensive and has a CPU-based licensing model where the cost increases with the number of vCPU cores within the operating system. The solutions architect must select an Amazon EC2 instance with sufficient memory to run the database software, but the selected instance has a large number of vCPUs. The solutions architect must ensure that the vCPUs will not be underutilized and must minimize costs.Which solution meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Select and launch a smaller EC2 instance with an appropriate number of vCPUs.",
            "Configure the CPU cores and threads on the selected EC2 instance during instance launch.",
            "Create a new EC2 instance and ensure multithreading is enabled when configuring the instance details.",
            "Create a new Capacity Reservation and select the appropriate instance type. Launch the instance into this new Capacity Reservation."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #318\n\nA company receives 10 TB of instrumentation data each day from several machines located at a single factory. The data consists of JSON files stored on a storage area network (SAN) in an on-premises data center located within the factory. The company wants to send this data to Amazon S3 where it can be accessed by several additional systems that provide critical near-real-lime analytics. A secure transfer is important because the data is considered sensitive.Which solution offers the MOST reliable data transfer?",
        "answers": [
            3
        ],
        "options": [
            "AWS DataSync over public internet",
            "AWS DataSync over AWS Direct Connect",
            "AWS Database Migration Service (AWS DMS) over public internet",
            "AWS Database Migration Service (AWS DMS) over AWS Direct Connect"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #319\n\nA company is creating a web application that will store a large number of images in Amazon S3. The images will be accessed by users over variable periods of time. The company wants to:\u2711 Retain all the images\u2711 Incur no cost for retrieval.\u2711 Have minimal management overhead.\u2711 Have the images available with no impact on retrieval time.Which solution meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Implement S3 Intelligent-Tiering",
            "Implement S3 storage class analysis",
            "Implement an S3 Lifecycle policy to move data to S3 Standard-Infrequent Access (S3 Standard-IA).",
            "Implement an S3 Lifecycle policy to move data to S3 One Zone-Infrequent Access (S3 One Zone-IA)."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #320\n\nA company hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day.What should a solutions architect do to transmit and process the clickstream data?",
        "answers": [
            2
        ],
        "options": [
            "Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster with the data to generate analytics.",
            "Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 data lake for Amazon Redshift to use for analysis.",
            "Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is added to the S3 bucket, run an AWS Lambda function to process the data for analysis.",
            "Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #321\n\nA company wants to build an online marketplace application on AWS as a set of loosely coupled microservices. For this application, when a customer submits a new order, two microservices should handle the event simultaneously. The Email microservice will send a confirmation email, and the OrderProcessing microservice will start the order delivery process. If a customer cancels an order, the OrderCancellation and Email microservices should handle the event simultaneously.A solutions architect wants to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) to design the messaging between the microservices.How should the solutions architect design the solution?",
        "answers": [
            3
        ],
        "options": [
            "Create a single SQS queue and publish order events to it. The Email, OrderProcessing, and OrderCancellation microservices can then consume messages off the queue.",
            "Create three SNS topics for each microservice. Publish order events to the three topics. Subscribe each of the Email, OrderProcessing, and OrderCancellation microservices to its own topic.",
            "Create an SNS topic and publish order events to it. Create three SQS queues for the Email, OrderProcessing, and OrderCancellation microservices. Subscribe all SQS queues to the SNS topic with message filtering.",
            "Create two SQS queues and publish order events to both queues simultaneously. One queue is for the Email and OrderProcessing microservices. The second queue is for the Email and OrderCancellation microservices."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #322\n\nA company is running a multi-tier ecommerce web application in the AWS Cloud. The application runs on Amazon EC2 Instances with an Amazon RDS MySQLMulti-AZ DB instance. Amazon RDS is configured with the latest generation instance with 2,000 GB of storage in an Amazon Elastic Block Store (Amazon EBS)General Purpose SSD (gp2) volume. The database performance impacts the application during periods of high demand.After analyzing the logs in Amazon CloudWatch Logs, a database administrator finds that the application performance always degrades when the number of read and write IOPS is higher than 6.000.What should a solutions architect do to improve the application performance?",
        "answers": [
            2
        ],
        "options": [
            "Replace the volume with a Magnetic volume.",
            "Increase the number of IOPS on the gp2 volume.",
            "Replace the volume with a Provisioned IOPS (PIOPS) volume.",
            "Replace the 2,000 GB gp2 volume with two 1,000 GBgp2 volumes."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #323\n\nA company has an application that uses Amazon Elastic File System (Amazon EFS) to store data. The files are 1 GB in size or larger and are accessed often only for the first few days after creation. The application data is shared across a cluster of Linux servers. The company wants to reduce storage costs tor the application.What should a solutions architect do to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Implement Amazon FSx and mount the network drive on each server.",
            "Move the files from Amazon Elastic File System (Amazon EFS) and store them locally on each Amazon EC2 instance.",
            "Configure a Lifecycle policy to move the files to the EFS Infrequent Access (IA) storage class after 7 days.",
            "Move the files to Amazon S3 with S3 lifecycle policies enabled. Rewrite the application to support mounting the S3 bucket."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #324\n\nA company has a service that produces event data. The company wants to use AWS to process the event data as it is received. The data is written in a specific order that must be maintained throughout processing. The company wants to implement a solution that minimizes operational overhead.How should a solutions architect accomplish this?",
        "answers": [
            0
        ],
        "options": [
            "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages. Set up an AWS Lambda function to process messages from the queue.",
            "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an AWS Lambda function as a subscriber.",
            "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages. Set up an AWS Lambda function to process messages from the queue independently.",
            "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #325\n\nA company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region for an upcoming event that will last 1 week.What should the company do to guarantee the EC2 capacity?",
        "answers": [
            3
        ],
        "options": [
            "Purchase Reserved Instances that specify the Region needed.",
            "Create an On-Demand Capacity Reservation that specifies the Region needed.",
            "Purchase Reserved Instances that specify the Region and three Availability Zones needed.",
            "Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #326\n\nA company wants to migrate its web application to AWS. The legacy web application consists of a web tier, an application tier, and a MySQL database. The re- architected application must consist of technologies that do not require the administration team to manage instances or clusters.Which combination of services should a solutions architect include in the overall architecture? ",
        "answers": [
            3,
            4
        ],
        "options": [
            "Amazon Aurora Serverless",
            "Amazon EC2 Spot Instances",
            "Amazon Elasticsearch Service (Amazon ES)",
            "Amazon RDS for MySQL",
            "AWS Fargate"
        ],
        "explination": "Correct Answer: DE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #327\n\nAn ecommerce company is experiencing an increase in user traffic. The company's store is deployed on Amazon EC2 instances as a two-tier two application consisting of a web tier and a separate database tier. As traffic increases, the company notices that the architecture is causing significant delays in sending timely marketing and order confirmation email to users. The company wants to reduce the time it spends resolving complex email delivery issues and minimize operational overhead.What should a solutions architect do to meet these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Create a separate application tier using EC2 instances dedicated to email processing.",
            "Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).",
            "Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS).",
            "Create a separate application tier using EC2 instances dedicated to email processing. Place the instances in an Auto Scaling group."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #328\n\nA company recently started using Amazon Aurora as the data store for its global ecommerce application. When large reports are run, developers report that the ecommerce application is performing poorly. After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the ReadIOPS and CPUUtilization metrics are spiking when monthly reports run.What is the MOST cost-effective solution?",
        "answers": [
            3
        ],
        "options": [
            "Migrate the monthly reporting to Amazon Redshift.",
            "Migrate the monthly reporting to an Aurora Replica.",
            "Migrate the Aurora database to a larger instance class.",
            "Increase the Provisioned IOPS on the Aurora instance."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #329\n\nA company uses on-premises servers to host its applications. The company is running out of storage capacity. The applications use both block storage and NFS storage. The company needs a high-performing solution that supports local caching without re-architecting its existing applications.Which combination of actions should a solutions architect take to meet these requirements? ",
        "answers": [
            3,
            4
        ],
        "options": [
            "Mount Amazon S3 as a file system to the on-premises servers.",
            "Deploy an AWS Storage Gateway file gateway to replace NFS storage.",
            "Deploy AWS Snowball Edge to provision NFS mounts to on-premises servers.",
            "Deploy an AWS Storage Gateway volume gateway to replace the block storage.",
            "Deploy Amazon Elastic Fife System (Amazon EFS) volumes and mount them to on-premises servers."
        ],
        "explination": "Correct Answer: DE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #330\n\nA solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time.Which solution meets these requirements and is MOST secure?",
        "answers": [
            1
        ],
        "options": [
            "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.",
            "Amazon EC2 instances in private subnets Configure. Configure a public Application Load Balancer with multiple redundant Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.",
            "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.",
            "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #331\n\nA company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy endpoints.Which solution meets these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.",
            "Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.",
            "Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.",
            "Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in- memory cache for DynamoDB hosting the application data."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #332\n\nA company is designing an internet-facing web application. The application runs on Amazon EC2 for Linux-based instances that store sensitive user data inAmazon RDS MySQL Multi-AZ DB instances. The EC2 instances are in public subnets, and the RDS DB instances are in private subnets. The security team has mandated that the DB instances be secured against web-based attacks.What should a solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "Ensure the EC2 instances are part of an Auto Scaling group and are behind an Application Load Balancer. Configure the EC2 instance iptables rules to drop suspicious web traffic. Create a security group for the DB instances. Configure the RDS security group to only allow port 3306 inbound from the individual EC2 instances.",
            "Ensure the EC2 instances are part of an Auto Scaling group and are behind an Application Load Balancer. Move DB instances to the same subnets that EC2 instances are located in. Create a security group for the DB instances. Configure the RDS security group to only allow port 3306 inbound from the individual EC2 instances.",
            "Ensure the EC2 instances are part of an Auto Scaling group and are behind an Application Load Balancer. Use AWS WAF to monitor inbound web traffic for threats. Create a security group for the web application servers and a security group for the DB instances. Configure the RDS security group to only allow port 3306 inbound from the web application server security group.",
            "Ensure the EC2 instances are part of an Auto Scaling group and are behind an Application Load Balancer. Use AWS WAF to monitor inbound web traffic for threats. Configure the Auto Scaling group to automatically create new DB instances under heavy traffic. Create a security group for the RDS DB instances. Configure the RDS security group to only allow port 3306 inbound."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #333\n\nA development team stores its Amazon RDS MySQL DB instance user name and password credentials in a configuration file. The configuration file is stored as plaintext on the root device volume of the team's Amazon EC2 instance. When the team's application needs to reach the database, it reads the file and loads the credentials into the code. The team has modified the permissions of the configuration file so that only the application can read its content. A solutions architect must design a more secure solution.What should the solutions architect do to meet this requirement?",
        "answers": [
            3
        ],
        "options": [
            "Store the configuration file in Amazon S3. Grant the application access to read the configuration file.",
            "Create an IAM role with permission to access the database. Attach this IAM role to the EC2 instance.",
            "Enable SSL connections on the database instance. Alter the database user to require SSL when logging in.",
            "Move the configuration file to an EC2 instance store, and create an Amazon Machine Image (AMI) of the instance. Launch new instances from this AMI."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #334\n\nA company wants a storage option that enables its data science team to analyze its data on premises and in the AWS Cloud. The team needs to be able to run statistical analyses by using the data on premises and by using a fleet of Amazon EC2 instances across multiple Availability Zones.What should a solutions architect do to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Use an AWS Storage Gateway tape gateway to copy the on-premises files into Amazon S3.",
            "Use an AWS Storage Gateway volume gateway to copy the on-premises files into Amazon S3.",
            "Use an AWS Storage Gateway file gateway to copy the on-premises files to Amazon Elastic Block Store (Amazon EBS).",
            "Attach an Amazon Elastic File System (Amazon EFS) file system to the on-premises servers. Copy the files to Amazon EFS."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #335\n\nA company wants to improve the availability and performance of its stateless UDP-based workload. The workload is deployed on Amazon EC2 instances in multiple AWS Regions.What should a solutions architect recommend to accomplish this?",
        "answers": [
            3
        ],
        "options": [
            "Place the EC2 instances behind Network Load Balancers (NLBs) in each Region. Create an accelerator using AWS Global Accelerator. Use the NLBs as endpoints for the accelerator.",
            "Place the EC2 instances behind Application Load Balancers (ALBs) in each Region. Create an accelerator using AWS Global Accelerator. Use the ALBs as endpoints for the accelerator.",
            "Place the EC2 instances behind Network Load Balancers (NLBs) in each Region. Create an Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-based routing to route requests to the NLBs.",
            "Place the EC2 instances behind Application Load Balancers (ALBs) in each Region. Create an Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-based routing to route requests to the ALBs."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #336\n\nA company wants to use high performance computing (HPC) infrastructure on AWS for financial risk modeling. The company's HPC workloads run on Linux. EachHPC workflow runs on hundreds of AmazonEC2 Spot Instances, is short-lived, and generates thousands of output files that are ultimately stored in persistent storage for analytics and long-term future use.The company seeks a cloud storage solution that permits the copying of on premises data to long-term persistent storage to make data available for processing by all EC2 instances. The solution should also be a high performance file system that is integrated with persistent storage to read and write datasets and output files.Which combination of AWS services meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Amazon FSx for Lustre integrated with Amazon S3",
            "Amazon FSx for Windows File Server integrated with Amazon S3",
            "Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS)",
            "Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2) volume"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #337\n\nA solutions architect must design a database solution for a high-traffic ecommerce web application. The database stores customer profiles and shopping cart information. The database must support a peak load of several million requests each second and deliver responses in milliseconds. The operational overhead form an aging and scaling the database must be minimized.Which database solution should the solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Amazon Aurora",
            "Amazon DynamoDB",
            "Amazon RDS",
            "Amazon Redshift"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #338\n\nA company is working with an external vendor that requires write access to the company's Amazon Simple Queue Service (Amazon SQS) queue. The vendor has its own AWS account.What should a solutions architect do to implement least privilege access?",
        "answers": [
            3
        ],
        "options": [
            "Update the permission policy on the SQS queue to give write access to the vendor's AWS account.",
            "Create an IAM user with write access to the SQS queue and share the credentials for the IAM user.",
            "Update AWS Resource Access Manager to provide write access to the SQS queue from the vendor's AWS account.",
            "Create a cross-account role with access to all SQS queues and use the vendor's AWS account in the trust document for the role."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #339\n\nA company is creating a three-tier web application consisting of a web server, an application server, and a database server. The application will track GPS coordinates of packages as they are being delivered. The application will update the database every 0-5 seconds.The tracking will need to read a fast as possible for users to check the status of their packages. Only a few packages might be tracked on some days, whereas millions of package might be tracked on other days. Tracking will need to be searchable by tracking ID customer ID and order ID. Order than 1 month no longer read to be tracked.What should a solutions architect recommend to accomplish this with minimal cost of ownership?",
        "answers": [
            1
        ],
        "options": [
            "Use Amazon DynamoDB Enable Auto Scaling on the DynamoDB table. Schedule an automatic deletion script for items older than 1 month.",
            "Use Amazon DynamoDB with global secondary indexes. Enable Auto Scaling on the DynamoDB table and the global secondary indexes. Enable TTL on the DynamoDB table.",
            "Use an Amazon RDS On-Demand instance with Provisioned IOPS (PIOPS). Enable Amazon CloudWatch alarms to send notifications when PIOPS are exceeded. Increase and decrease PIOPS as needed.",
            "Use an Amazon RDS Reserved Instance with Provisioned IOPS (PIOPS). Enable Amazon CloudWatch alarms to send notification when PIOPS are exceeded. Increase and decrease PIOPS as needed."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #340\n\nA solutions architect is creating a data processing job that runs once daily and can take up to 2 hours to complete. If the job is interrupted, it has to restart from the beginning.How should the solutions architect address this issue in the MOST cost-effective manner?",
        "answers": [
            2
        ],
        "options": [
            "Create a script that runs locally on an Amazon EC2 Reserved Instance that is triggered by a cron job.",
            "Create an AWS Lambda function triggered by an Amazon EventBridge (Amazon CloudWatch Events) scheduled event.",
            "Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon EventBridge (Amazon CloudWatch Events) scheduled event.",
            "Use an Amazon Elastic Container Service (Amazon ECS) task running on Amazon EC2 triggered by an Amazon EventBridge (Amazon CloudWatch Events) scheduled event."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #341\n\nA company needs to store data in Amazon S3. A compliance requirement states that when any changes are made to objects the previous state of the object with any changes must be preserved. Additionally, files older than 5 years should not be accessed but need to be archived for auditing.What should a solutions architect recommend that is MOST cost-effective?",
        "answers": [
            2
        ],
        "options": [
            "Enable object-level versioning and S3 Object Lock in governance mode",
            "Enable object-level versioning and S3 Object Lock in compliance mode",
            "Enable object-level versioning. Enable a lifecycle policy to move data older than 5 years to S3 Glacier Deep Archive",
            "Enable object-level versioning. Enable a lifecycle policy to move data older than 5 years to S3 Standard-Infrequent Access (S3 Standard-IA)"
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #342\n\nA new employee has joined a company as a deployment engineer. The deployment engineer will be using AWS CloudFormation templates to create multiple AWS resources. A solutions architect wants the deployment engineer to perform job activities while following the principle of least privilege.Which combination of actions should the solutions architect take to accomplish this goal? ",
        "answers": [
            3,
            4
        ],
        "options": [
            "Have the deployment engineer use AWS account roof user credentials for performing AWS CloudFormation stack operations.",
            "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the PowerUsers IAM policy attached.",
            "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the Administrate/Access IAM policy attached.",
            "Create a new IAM User for the deployment engineer and add the IAM user to a group that has an IAM policy that allows AWS CloudFormation actions only.",
            "Create an IAM role for the deployment engineer to explicitly define the permissions specific to the AWS CloudFormation stack and launch stacks using Dial IAM role."
        ],
        "explination": "Correct Answer: DE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #343\n\nA company is planning to use an Amazon DynamoDB table for data storage. The company is concerned about cost optimization. The table will not be used on most mornings in the evenings, the read and write traffic will often be unpredictable. When traffic spikes occur they will happen very quickly.What should a solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Create a DynamoDB table in on-demand capacity mode.",
            "Create a DynamoDB table with a global secondary Index.",
            "Create a DynamoDB table with provisioned capacity and auto scaling.",
            "Create a DynamoDB table in provisioned capacity mode, and configure it as a global table."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #344\n\nA meteorological startup company has a custom web application to sell weather data to its users online. The company uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new weather event is recorded. The company does not want this new service to affect the performance of the current application.What should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?",
        "answers": [
            0
        ],
        "options": [
            "Use DynamoDB transactions to write new event data to the table. Configure the transactions to notify internal teams.",
            "Have the current application publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Have each team subscribe to one topic.",
            "Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe.",
            "Add a custom attribute to each record to flag new items. Write a cron job that scans the table every minute for items that are new and notifies an Amazon Simple Queue Service (Amazon SQS) queue to which the teams can subscribe."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #345\n\nA company is preparing to deploy a new serverless workload. A solutions architect needs to configure permissions for invoking an AWS Lambda function. The function will be triggered by an Amazon EventBridge (Amazon CloudWatch Events) rule. Permissions should be configured using the principle of least privilege.Which solution will meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Add an execution role to the function with lambda:InvokeFunction as the action and * as the principal.",
            "Add an execution role to the function with lambda:InvokeFunction as the action and Service:amazonaws.com as the principal.",
            "Add a resource-based policy to the function with lambda:'* as the action and Service:events.amazonaws.com as the principal.",
            "Add a resource-based policy to the function with lambda:InvokeFunction as the action and Service:events.amazonaws.com as the principal."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #346\n\nA company is building its web application using containers on AWS. The company requires three instances of the web application to run at all times. The application must be able to scale to meet increases in demand. Management is extremely sensitive to cost but agrees that the application should be highly available.What should a solutions architect recommend?",
        "answers": [
            3
        ],
        "options": [
            "Create an Amazon Elastic Container Service (Amazon ECS) cluster using the Fargate launch type. Create a task definition for the web application. Create an ECS service with a desired count of three tasks.",
            "Create an Amazon Elastic Container Service (Amazon ECS) cluster using the Amazon EC2 launch type with three container instances in one Availability Zone. Create a task definition for the web application. Place one task for each container instance.",
            "Create an Amazon Elastic Container Service (Amazon ECS) cluster using the Fargate launch type with one container instance in three different Availability Zones. Create a task definition for the web application. Create an ECS service with a desired count of three tasks.",
            "Create an Amazon Elastic Container Service (Amazon ECS) cluster using the Amazon EC2 launch type with one container instance in two different Availability Zones. Create a task definition for the web application. Place two tasks on one container instance and one task on the remaining container instance."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #347\n\nA company is Re-architecting a strongly coupled application to be loosely coupled. Previously the application used a request/response pattern to communicate between tiers. The company plans to use Amazon Simple Queue Service (Amazon SQS) to achieve decoupling requirements. The initial design contains one queue for requests and one for responses. However, this approach is not processing all the messages as the application scales.What should a solutions architect do to resolve this issue?",
        "answers": [
            0
        ],
        "options": [
            "Configure a dead-letter queue on the ReceiveMessage API action of the SQS queue.",
            "Configure a FIFO queue, and use the message deduplication ID and message group ID.",
            "Create a temporary queue, with the Temporary Queue Client to receive each response message.",
            "Create a queue for each request and response on startup for each producer, and use a correlation ID message attribute."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #348\n\nA company is launching an ecommerce website on AWS. This website is built with a three-tier architecture that includes a MySQL database in a Multi-AZ deployment of Amazon Aurora MySQL. The website application must be highly available and will initially be launched in an AWS Region with three AvailabilityZones. The application produces a metric that describes the load the application experiences.Which solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Configure an Application Load Balancer (ALB) with Amazon EC2 Auto Scaling behind the ALB with scheduled scaling.",
            "Configure an Application Load Balancer (ALB) and Amazon EC2 Auto Scaling behind the ALB with a simple scaling policy.",
            "Configure a Network Load Balancer (NLB) and launch a Spot Fleet with Amazon EC2 Auto Scaling behind the NLB.",
            "Configure an Application Load Balancer (ALB) and Amazon EC2 Auto Scaling behind the ALB with a target tracking scaling policy."
        ],
        "total_times_question_attempted": 11,
        "correct_times_question_attempted": 10,
        "current_probability": 0.009761289309090914
    },
    {
        "question": "Question #349\n\nA solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should be protected throughout the entire application stack, and access to the information should be restricted to certain applications.Which action should the solutions architect take?",
        "answers": [
            0
        ],
        "options": [
            "Configure a CloudFront signed URL",
            "Configure a CloudFront signed cookie.",
            "Configure a CloudFront field-level encryption profile.",
            "Configure a CloudFront and set the Origin Protocol Policy setting to HTTPS. Only for the Viewer Protocol Pokey."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #350\n\nA solutions architect is redesigning a monolithic application to be a loosely coupled application composed of two microservices: Microservice A and MicroserviceB.Microservice A places messages in a main Amazon Simple Queue Service (Amazon SQS) queue for Microservice B to consume. When Microservice B fails to process a message after four retries, the message needs to be removed from the queue and stored for further investigation.What should the solutions architect do to meet these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Create an SQS dead-letter queue. Microservice B adds failed messages to that queue after it receives and fails to process the message four times.",
            "Create an SQS dead-letter queue. Configure the main SQS queue to deliver messages to the dead-letter queue after the message has been received four times.",
            "Create an SQS queue for failed messages. Microservice A adds failed messages to that queue after Microservice B receives and fails to process the message four times.",
            "Create an SQS queue for failed messages. Configure the SQS queue for failed messages to pull messages from the main SQS queue after the original message has been received four times."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #351\n\nA company has NFS servers in an on-premises data center that need to periodically back up small amounts of data to Amazon S3. Which solution meets these requirements and is MOST cost-effective?",
        "answers": [
            2
        ],
        "options": [
            "Set up AWS Glue to copy the data from the on-premises servers to Amazon S3.",
            "Set up an AWS DataSync agent on the on-premises servers, and sync the data to Amazon S3.",
            "Set up an SFTP sync using AWS Transfer for SFTP to sync data from on-premises to Amazon S3.",
            "Set up an AWS Direct Connect connection between the on-premises data center and a VPC, and copy the data to Amazon S3."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #352\n\nA company runs its production workload on an Amazon Aurora MySQL DB cluster that includes six Aurora Replicas. The company wants near-real-lime reporting queries from one of its departments to be automatically distributed across three of the Aurora Replicas. Those three replicas have a different compute and memory specification from the rest of the DB cluster.Which solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Create and use a custom endpoint for the workload.",
            "Create a three-node cluster clone and use the reader endpoint.",
            "Use any of the instance endpoints for the selected three nodes.",
            "Use the reader endpoint to automatically distribute the read-only workload."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #353\n\nA company has multiple applications that use Amazon RDS for MySQL as is database. The company recently discovered that a new custom reporting application has increased the number of Queries on the database. This is slowing down performance.How should a solutions architect resolve this issue with the LEAST amount of application changes?",
        "answers": [
            3
        ],
        "options": [
            "Add a secondary DB instance using Multi-AZ.",
            "Set up a road replica and Multi-AZ on Amazon RDS.",
            "Set up a standby replica and Multi-AZ on Amazon RDS.",
            "Use caching on Amazon RDS to improve the overall performance."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #354\n\nA company wants to automate the security assessment of its Amazon EC2 instances. The company needs to validate and demonstrate that security and compliance standards are being followed throughout the development process.What should a solutions architect do to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Use Amazon Macie to automatically discover, classify and protect the EC2 instances.",
            "Use Amazon GuardDuty to publish Amazon Simple Notification Service (Amazon SNS) notifications.",
            "Use Amazon Inspector with Amazon CloudWatch to publish Amazon Simple Notification Service (Amazon SNS) notifications",
            "Use Amazon EventBridge (Amazon CloudWatch Events) to detect and react to changes in the status of AWS Trusted Advisor checks."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0,
        "tags": [
            "EC2"
        ]
    },
    {
        "question": "Question #355\n\nA company stores 200 GB of data each month in Amazon S3. The company needs to perform analytics on this data at the end of each month to determine the number of items sold in each sales region for the previous month.Which analytics strategy is MOST cost-effective for the company to use?",
        "answers": [
            0
        ],
        "options": [
            "Create an Amazon Elasticsearch Service (Amazon ES) cluster. Query the data in Amazon ES. Visualize the data by using Kibana.",
            "Create a table in the AWS Glue Data Catalog. Query the data in Amazon S3 by using Amazon Athena. Visualize the data in Amazon QuickSight.",
            "Create an Amazon EMR cluster. Query the data by using Amazon EMR, and store the results in Amazon S3. Visualize the data in Amazon QuickSight.",
            "Create an Amazon Redshift cluster. Query the data in Amazon Redshift, and upload the results to Amazon S3. Visualize the data in Amazon QuickSight."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #356\n\nA company wants to move its on-premises network attached storage (NAS) to AWS. The company wants to make the data available to any Linux instances within its VPC and ensure changes are automatically synchronized across all instances accessing the data store. The majority of the data is accessed very rarely, and some files are accessed by multiple users at the same time.Which solution meets these requirements and is MOST cost-effective?",
        "answers": [
            3
        ],
        "options": [
            "Create an Amazon Elastic Block Store (Amazon EBS) snapshot containing the data. Share it with users within the VPC.",
            "Create an Amazon S3 bucket that has a lifecycle policy set to transition the data to S3 Standard-Infrequent Access (S3 Standard-IA) after the appropriate number of days.",
            "Create an Amazon Elastic File System (Amazon EFS) file system within the VPC. Set the throughput mode to Provisioned and to the required amount of IOPS to support concurrent usage.",
            "Create an Amazon Elastic File System (Amazon EFS) file system within the VPC. Set the lifecycle policy to transition the data to EFS Infrequent Access (EFS IA) after the appropriate number of days."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #357\n\nA company plans to host a survey website on AWS. The company anticipates an unpredictable amount of traffic. This traffic results in asynchronous updates to the database. The company wants to ensure that writes to the database hosted on AWS do not get dropped.How should the company write its application to handle these database requests?",
        "answers": [
            0
        ],
        "options": [
            "Configure the application to publish to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the database to the SNS topic.",
            "Configure the application to subscribe to an Amazon Simple Notification Service (Amazon SNS) topic. Publish the database updates to the SNS topic.",
            "Use Amazon Simple Queue Service (Amazon SQS) FIFO queues to queue the database connection until the database has resources to write the data.",
            "Use Amazon Simple Queue Service (Amazon SQS) FIFO queues for capturing the writes and draining the queue as each write is made to the database."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #358\n\nA company that recently started using AWS establishes a Site-to-Site VPN between its on-premises datacenter and AWS. The company's security mandate states that traffic originating from on premises should stay within the company's private IP space when communicating with an Amazon Elastic Container Service(Amazon ECS) cluster that is hosting a sample web application.Which solution meets this requirement?",
        "answers": [
            2
        ],
        "options": [
            "Configure a gateway endpoint for Amazon ECS. Modify the route table to include an entry pointing to the ECS cluster.",
            "Create a Network Load Balancer and AWS PrivateLink endpoint for Amazon ECS in the same VPC that is hosting the ECS cluster.",
            "Create a Network Load Balancer in one VPC and an AWS PrivateLink endpoint for Amazon ECS in another VPC. Connect the two VPCs by using VPC peering.",
            "Configure an Amazon Route 53 record with Amazon ECS as the target. Apply a server certificate to Route 53 from AWS Certificate Manager (ACM) for SSL offloading."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #359\n\nA solutions architect must analyze and update a company's existing IAM policies prior to deploying a new workload. The solutions architect created the following policy:What is the net effect of this policy?",
        "answers": [
            3
        ],
        "options": [
            "Users will be allowed all actions except s3:PutObject if multi-factor authentication (MFA) is enabled.",
            "Users will be allowed all actions except s3:PutObject if multi-factor authentication (MFA) is not enabled.",
            "Users will be denied all actions except s3:PutObject if multi-factor authentication (MFA) is enabled.",
            "Users will be denied all actions except s3:PutObject if multi-factor authentication (MFA) is not enabled."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #360\n\nA company is running a multi-tier web application on premises. The web application is containerized and runs on a number of Linux hosts connected to aPostgreSQL database that contains user records. The operational overhead of maintaining the infrastructure and capacity planning is limiting the company's growth. A solutions architect must improve the application's infrastructure.Which combination of actions should the solutions architect take to accomplish this? ",
        "answers": [
            2,
            3
        ],
        "options": [
            "Migrate the PostgreSQL database to Amazon Aurora.",
            "Migrate the web application to be hosted on Amazon EC2 instances.",
            "Set up an Amazon CloudFront distribution for the web application content.",
            "Set up Amazon ElastiCache between the web application and the PostgreSQL database.",
            "Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS)."
        ],
        "explination": "Correct Answer: CD \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #361\n\nAn application allows users at a company's headquarters to access product data. The product data is stored in an Amazon RDS MySQL DB instance. The operations team has isolated an application performance slowdown and wants to separate read traffic from write traffic. A solutions architect needs to optimize the application's performance quickly.What should the solutions architect recommend?",
        "answers": [
            3
        ],
        "options": [
            "Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary Availability Zone.",
            "Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary Availability Zone.",
            "Create read replicas for the database. Configure the read replicas with half of the compute and storage resources as the source database.",
            "Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #362\n\nA company is using Amazon DynamoDB with provisioned throughput for the database tier of its ecommerce website. During flash sales, customers experience periods of time when the database cannot handle the high number of transactions taking place. This causes the company to lose transactions. During normal periods, the database performs appropriately.Which solution solves the performance problem the company faces?",
        "answers": [
            0
        ],
        "options": [
            "Switch DynamoDB to on-demand mode during flash sales.",
            "Implement DynamoDB Accelerator for fast in memory performance.",
            "Use Amazon Kinesis to queue transactions for processing to DynamoDB.",
            "Use Amazon Simple Queue Service (Amazon SQS) to queue transactions to DynamoDB."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #363\n\nA company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules between the application tiers.What should a solutions architect do to correct this issue?",
        "answers": [
            1
        ],
        "options": [
            "Create security group rules using the instance ID as the source or destination.",
            "Create security group rules using the security group ID as the source or destination.",
            "Create security group rules using the VPC CIDR blocks as the source or destination.",
            "Create security group rules using the subnet CIDR blocks as the source or destination."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #364\n\nA company requires that all versions of objects in its Amazon S3 bucket be retained. Current object versions will be frequently accessed during the first 30 days, after which they will be rarely accessed and must be retrievable within 5 minutes. Previous object versions need to be kept forever, will be rarely accessed, and can be retrieved within 1 week. All storage solutions must be highly available and highly durable.What should a solutions architect recommend to meet these requirements in the MOST cost-effective manner?",
        "answers": [
            1
        ],
        "options": [
            "Create an S3 lifecycle policy for the bucket that moves current object versions from S3 Standard storage to S3 Glacier after 30 days and moves previous object versions to S3 Glacier after 1 day.",
            "Create an S3 lifecycle policy for the bucket that moves current object versions from S3 Standard storage to S3 Glacier after 30 days and moves previous object versions to S3 Glacier Deep Archive after 1 day.",
            "Create an S3 lifecycle policy for the bucket that moves current object versions from S3 Standard storage to S3 Standard-infrequent Access (S3 Standard-IA) after 30 days and moves previous object versions to S3 Glacier Deep Archive after 1 day.",
            "Create an S3 lifecycle policy for the bucket that moves current object versions from S3 Standard storage to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days and moves previous object versions to S3 Glacier Deep Archive after 1 day."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #365\n\nA development team is collaborating with another company to create an integrated product. The other company needs to access an Amazon Simple QueueService (Amazon SQS) queue that is contained in the development team's account. The other company wants to poll the queue without giving up its own account permissions to do so.How should a solutions architect provide access to the SQS queue?",
        "answers": [
            2
        ],
        "options": [
            "Create an instance profile that provides the other company access to the SQS queue.",
            "Create an IAM policy that provides the other company access to the SQS queue.",
            "Create an SQS access policy that provides the other company access to the SQS queue.",
            "Create an Amazon Simple Notification Service (Amazon SNS) access policy that provides the other company access to the SQS queue."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #366\n\nA company is developing a video conversion application hosted on AWS. The application will be available in two tiers: a free tier and a paid tier. Users in the paid tier will have their videos converted first, and then the tree tier users will have their videos converted.Which solution meets these requirements and is MOST cost-effective?",
        "answers": [
            3
        ],
        "options": [
            "One FIFO queue for the paid tier and one standard queue for the free tier.",
            "A single FIFO Amazon Simple Queue Service (Amazon SQS) queue for all file types.",
            "A single standard Amazon Simple Queue Service (Amazon SQS) queue for all file types.",
            "Two standard Amazon Simple Queue Service (Amazon SQS) queues with one for the paid tier and one for the free tier."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #367\n\nAn administrator of a large company wants to monitor for and prevent any cryptocurrency-related attacks on the company's AWS accounts.Which AWS service can the administrator use to protect the company against attacks?",
        "answers": [
            2
        ],
        "options": [
            "Amazon Cognito",
            "Amazon GuardDuty",
            "Amazon Inspector",
            "Amazon Macie"
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #368\n\nA company has applications hosted on Amazon EC2 instances with IPv6 addresses. The applications must initiate communications with other external applications using the internet. However, the company's security policy states that any external service cannot initiate a connection to the EC2 instances. What should a solutions architect recommend to resolve this issue?",
        "answers": [
            3
        ],
        "options": [
            "Create a NAT gateway and make it the destination of the subnet's route table.",
            "Create an internet gateway and make it the destination of the subnet's route table.",
            "Create a virtual private gateway and make it the destination of the subnet's route table.",
            "Create an egress-only internet gateway and make it the destination of the subnet's route table."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #369\n\nA company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses AmazonElastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive.Which storage solution is MOST cost-effective?",
        "answers": [
            0
        ],
        "options": [
            "Use AWS Storage Gateway for files to store and process the video content.",
            "Use AWS Storage Gateway for volumes to store and process the video content.",
            "Use Amazon Elastic File System (Amazon EFS) for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).",
            "Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon ElasticBlock Store (Amazon EBS) volume attached to the server for processing."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #370\n\nA company wants to host its web application on AWS using multiple Amazon EC2 instances across different AWS Regions. Since the application content will be specific to each geographic region, the client requests need to be routed to the server that hosts the content for that clients Region.What should a solutions architect do to accomplish this?",
        "answers": [
            2
        ],
        "options": [
            "Configure Amazon Route 53 with a latency routing policy.",
            "Configure Amazon Route 53 with a weighted routing policy.",
            "Configure Amazon Route 53 with a geolocation routing policy.",
            "Configure Amazon Route 53 with a multivalue answer routing policy"
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #371\n\nA solutions architect is planning the deployment of a new static website. The solution must minimize costs and provide at least 99% availability. Which solution meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Deploy the application to an Amazon S3 bucket in one AWS Region that has versioning disabled.",
            "Deploy the application to Amazon EC2 instances that run in two AWS Regions and two Availability Zones.",
            "Deploy the application to an Amazon S3 bucket that has versioning and cross-Region replication enabled.",
            "Deploy the application to an Amazon EC2 instance that runs in one AWS Region and one Availability Zone."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #372\n\nA recently created startup built a three-tier web application. The front end has static content. The application layer is based on microservices. User data is stored as JSON documents that need to be accessed with low latency. The company expects regular traffic to be low during the first year, with peaks in traffic when it publicizes new features every month. The startup team needs to minimize operational overhead costs.What should a solutions architect recommend to accomplish this?",
        "answers": [
            2
        ],
        "options": [
            "Use Amazon S3 static website hosting to store and serve the front end. Use AWS Elastic Beanstalk for the application layer. Use Amazon DynamoDB to store user data.",
            "Use Amazon S3 static website hosting to store and serve the front end. Use Amazon Elastic KubernetesService (Amazon EKS) for the application layer. Use Amazon DynamoDB to store user data.",
            "Use Amazon S3 static website hosting to store and serve the front end. Use Amazon API Gateway and AWS Lambda functions for the application layer. Use Amazon DynamoDB to store user data.",
            "Use Amazon S3 static website hosting to store and serve the front end. Use Amazon API Gateway and AWS Lambda functions for the application layer. Use Amazon RDS with read replicas to store user data."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #373\n\nA company is building a payment application that must be highly available even during regional service disruptions. A solutions architect must design a data storage solution that can be easily replicated and used in other AWS Regions. The application also requires low-latency atomicity, consistency, isolation, and durability (ACID) transactions that need to be immediately available to generate reports The development team also needs to use SQL.Which data storage solution meets these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Amazon Aurora Global Database",
            "Amazon DynamoDB global tables",
            "Amazon S3 with cross-Region replication and Amazon Athena",
            "MySQL on Amazon EC2 instances with Amazon Elastic Block Store (Amazon EBS) snapshot replication"
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #374\n\nA company stores call recordings on a monthly basis. Statistically, the recorded data may be referenced randomly within a year but accessed rarely after 1 year.Files that are newer than 1 year old must be queried and retrieved as quickly as possible. A delay in retrieving older files is acceptable. A solutions architect needs to store the recorded data at a minimal cost.Which solution is MOST cost-effective?",
        "answers": [
            1
        ],
        "options": [
            "Store individual files in Amazon S3 Glacier and store search metadata in object tags created in S3 Glacier Query S3 Glacier tags and retrieve the files from S3 Glacier.",
            "Store individual files in Amazon S3. Use lifecycle policies to move the files to Amazon S3 Glacier after1 year. Query and retrieve the files from Amazon S3 or S3 Glacier.",
            "Archive individual files and store search metadata for each archive in Amazon S3. Use lifecycle policies to move the files to Amazon S3 Glacier after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.",
            "Archive individual files in Amazon S3. Use lifecycle policies to move the files to Amazon S3 Glacier after 1 year. Store search metadata in Amazon DynamoDB. Query the files from DynamoDB and retrieve them from Amazon S3 or S3 Glacier."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #375\n\nA company is developing a new machine learning model solution in AWS. The models are developed as independent microservices that fetch about 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent.The company provides models to hundreds of users. The usage patterns for the models are irregular Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time.Which solution meets these requirements?",
        "answers": [
            3
        ],
        "options": [
            "The requests from the API are sent to an Application Load Balancer (ALB). Models are deployed as AWS Lambda functions invoked by the ALB.",
            "The requests from the API are sent to the models Amazon Simple Queue Service (Amazon SQS) queue. Models are deployed as AWS Lambda functions triggered by SQS events AWS Auto Scaling is enabled on Lambda to increase the number of vCPUs based on the SQS queue size.",
            "The requests from the API are sent to the model's Amazon Simple Queue Service (Amazon SQS) queue. Models are deployed as Amazon Elastic Container Service (Amazon ECS) services reading from the queue AWS App Mesh scales the instances of the ECS cluster based on the SQS queue size.",
            "The requests from the API are sent to the models Amazon Simple Queue Service (Amazon SQS) queue. Models are deployed as Amazon Elastic Container Service (Amazon ECS) services reading from the queue AWS Auto Scaling is enabled on Amazon ECS for both the cluster and copies of the service based on the queue size."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #376\n\nA company has no existing file share services. A new project requires access to file storage that is mountable as a drive for on-premises desktops. The file server must authenticate users to an Active Directory domain before they are able to access the storage.Which service will allow Active Directory users to mount storage as a drive on their desktops?",
        "answers": [
            3
        ],
        "options": [
            "Amazon S3 Glacier",
            "AWS DataSync",
            "AWS Snowball Edge",
            "AWS Storage Gateway"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #377\n\nA company is preparing to launch a public-facing web application in the AWS Cloud. The architecture consists of Amazon EC2 instances within a VPC behind anElastic Load Balancer (ELB). A third party service is used for the DNS. The company's solutions architect must recommend a solution to detect and protect against largescale DDoS attacks.Which solution meets these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Enable Amazon GuardDuty on the account.",
            "Enable Amazon Inspector on the EC2 instances.",
            "Enable AWS Shield and assign Amazon Route 53 to it.",
            "Enable AWS Shield Advanced and assign the ELB to it."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #378\n\nA company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort.What should a solutions architect do to meet these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Use AWS Key Management Service (AWS KMS) customer master keys (CMKs) to create keys. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation.",
            "Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an AWS Lambda function that rotates the credentials in Secret Manager.",
            "Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.",
            "Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Parameter Store."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #379\n\nA company is running a multi-tier web application on AWS. The application runs its database tier on Amazon Aurora MySQL. The application and database tiers are in the us-east-1 Region. A database administrator who regularly monitors the Aurora DB cluster finds that an intermittent increase in read traffic is creating high CPUutilization on the read replica and causing increased read latency of the application.What should a solutions architect do to improve read scalability?",
        "answers": [
            3
        ],
        "options": [
            "Reboot the Aurora DB cluster.",
            "Create a cross-Region read replica",
            "Increase the instance class of the read replica.",
            "Configure Aurora Auto Scaling for the read replica."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #380\n\nA company's order fulfillment service uses a MySQL database. The database needs to support a large number of concurrent queries and transactions. Developers are spending time patching and tuning the database. This is causing delays in releasing new product features.The company wants to use cloud-based services to help address this new challenge. The solution must allow the developers to migrate the database with little or no code changes and must optimize performance.Which service should a solutions architect use to meet these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Amazon Aurora",
            "Amazon DynamoDB",
            "Amazon ElastiCache",
            "MySQL on Amazon EC2"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #381\n\nA company is planning to transfer multiple terabytes of data to AWS. The data is collected offline from ships. The company want to run complex transformation before transferring the data.Which AWS service should a solutions architect recommend for this migration?",
        "answers": [
            3
        ],
        "options": [
            "AWS Snowball",
            "AWS Snowmobile",
            "AWS Snowball Edge Storage Optimize",
            "AWS Snowball Edge Compute Optimize"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #382\n\nA company is running an online transaction processing (OLTP) workload on AWS. This workload uses an unencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database snapshots are taken from this instance.What should a solutions architect do to ensure the database and snapshots are always encrypted moving forward?",
        "answers": [
            0
        ],
        "options": [
            "Encrypt a copy of the latest DB snapshot. Replace existing DB instance by restoring the encrypted snapshot.",
            "Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots to it. Enable encryption on the DB instance.",
            "Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS). Restore encrypted snapshot to an existing DB instance.",
            "Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS Key Management Service (AWS KMS) managed keys (SSE-KMS)."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #383\n\nA company is selling up an application to use an Amazon RDS MySQL DB instance. The database must be architected for high availability across AvailabilityZones and AWS Regions with minimal downtime.How should a solutions architect meet this requirement?",
        "answers": [
            1
        ],
        "options": [
            "Set up an RDS MySQL Multi-AZ DB instance. Configure an appropriate backup window.",
            "Set up an RDS MySQL Multi-AZ DB instance. Configure a read replica in a different Region.",
            "Set up an RDS MySQL Single-AZ DB instance. Configure a read replica in a different Region.",
            "Set up an RDS MySQL Single-AZ DB instance. Copy automated snapshots to at least one other Region."
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0,
        "tags": [
            "Downtime"
        ]
    },
    {
        "question": "Question #384\n\nA company hosts its web application on AWS using seven Amazon EC2 instances. The company requires that the IP addresses of all healthy EC2 instances be returned in response to DNS queries.Which policy should be used to meet this requirement?",
        "answers": [
            2
        ],
        "options": [
            "Simple routing policy",
            "Latency routing policy",
            "Multi-value routing policy",
            "Geolocation routing policy"
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #385\n\nA company has 700 TB of backup data stored in network attached storage (NAS) in its data center This backup data need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer.What should a solutions architect do to migrate and store the data at the LOWEST cost?",
        "answers": [
            0
        ],
        "options": [
            "Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.",
            "Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on premises to Amazon S3 Glacier.",
            "Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.",
            "Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync task to copy files from the on-premises NAS storage to Amazon S3 Glacier."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #386\n\nA company is preparing to deploy a data lake on AWS. A solutions architect must define the encryption strategy tor data at rest m Amazon S3/ The company's security policy states:\u2711 Keys must be rotated every 90 days.\u2711 Strict separation of duties between key users and key administrators must be implemented.\u2711 Auditing key usage must be possible.What should the solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Server-side encryption with AWS KMS managed keys (SSE-KMS) with customer managed customer master keys (CMKs)",
            "Server-side encryption with AWS KMS managed keys (SSE-KMS) with AWS managed customer master keys (CMKs)",
            "Server-side encryption with Amazon S3 managed keys (SSE-S3) with customer managed customer master keys (CMKs)",
            "Server-side encryption with Amazon S3 managed keys (SSE-S3) with AWS managed customer master keys (CMKs)"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #387\n\nA company has an application that generates a large number of files, each approximately 5 MB in size. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be deleted. Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days.Which storage solution is MOST cost-effective?",
        "answers": [
            2
        ],
        "options": [
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation. Delete the files 4 years after object creation.",
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days from object creation. Delete the files 4 years after object creation.",
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.",
            "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Move the files to S3 Glacier 4 years after object creation."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #388\n\nA company previously migrated its data warehouse solution to AWS. The company also has an AWS Direct Connect connection. Corporate office users query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 50 MB and each webpage sent by the visualization tool is approximately 500 KB. Result sets returned by the data warehouse are not cached.Which solution provides the LOWEST data transfer egress cost for the company?",
        "answers": [
            0
        ],
        "options": [
            "Host the visualization tool on premises and query the data warehouse directly over the internet.",
            "Host the visualization tool in the same AWS Region as the data warehouse. Access it over the internet.",
            "Host the visualization tool on premises and query the data warehouse directly over a Direct Connect connection at a location in the same AWS Region.",
            "Host the visualization tool in the same AWS Region as the data warehouse and access it over a DirectConnect connection at a location in the same Region."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #389\n\nA mobile gaming company runs application servers on Amazon EC2 instances. The servers receive updates from players every 15 minutes. The mobile game creates a JSON object of the progress made in the game since the last update, and sends the JSON object to an Application Load Balancer. As the mobile game is played, game updates are being lost. The company wants to create a durable way to get the updates in older.What should a solutions architect recommend to decouple the system?",
        "answers": [
            2
        ],
        "options": [
            "Use Amazon Kinesis Data Streams to capture the data and store the JSON object in Amazon S3.",
            "Use Amazon Kinesis Data Firehose to capture the data and store the JSON object in Amazon S3.",
            "Use Amazon Simple Queue Service (Amazon SQS) FIFO queues to capture the data and EC2 instances to process the messages in the queue.",
            "Use Amazon Simple Notification Service (Amazon SNS) to capture the data and EC2 instances to process the messages sent to the Application Load Balancer."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #390\n\nA company has an application that runs on Amazon EC2 instances within a private subnet in a VPC. The instances access data in an Amazon S3 bucket in the same AWS Region. The VPC contains a NAT gateway in a public subnet to access the S3 bucket. The company wants to reduce costs by replacing the NAT gateway without compromising security or redundancy.Which solution meets these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Replace the NAT gateway with a NAT instance.",
            "Replace the NAT gateway with an internet gateway.",
            "Replace the NAT gateway with a gateway VPC endpoint.",
            "Replace the NAT gateway with an AWS Direct Connect connection."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #31\n\nA company currently operates a web application backed by an Amazon RDS MySQL database. It has automated backups that are run daily and are not encrypted. A security audit requires future backups to be encrypted and the unencrypted backups to be destroyed. The company will make at least one encrypted backup before destroying the old backups.What should be done to enable encryption for future backups?",
        "answers": [
            2
        ],
        "options": [
            "Enable default encryption for the Amazon S3 bucket where backups are stored.",
            "Modify the backup section of the database configuration to toggle the Enable encryption check box.",
            "Create a snapshot of the database. Copy it to an encrypted snapshot. Restore the database from the encrypted snapshot.",
            "Enable an encrypted read replica on RDS for MySQL. Promote the encrypted read replica to primary. Remove the original database instance."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                However, because you can encrypt a copy of an unencrypted DB snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of your original DB instance.DB instances that are encrypted can't be modified to disable encryption.You can't have an encrypted read replica of an unencrypted DB instance or an unencrypted read replica of an encrypted DB instance.Encrypted read replicas must be encrypted with the same key as the source DB instance when both are in the same AWS Region.You can't restore an unencrypted backup or snapshot to an encrypted DB instance.To copy an encrypted snapshot from one AWS Region to another, you must specify the KMS key identifier of the destination AWS Region. This is because KMS encryption keys are specific to the AWS Region that they are created in.Reference:https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #32\n\nA company is hosting a website behind multiple Application Load Balancers. The company has different distribution rights for its content around the world. A solutions architect needs to ensure that users are served the correct content without violating distribution rights.Which configuration should the solutions architect choose to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Configure Amazon CloudFront with AWS WAF.",
            "Configure Application Load Balancers with AWS WAF.",
            "Configure Amazon Route 53 with a geolocation policy.",
            "Configure Amazon Route 53 with a geoproximity routing policy."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html(geolocation routing)",
        "topic": "Topic 1"
    },
    {
        "question": "Question #33\n\nA solutions architect has created a new AWS account and must secure AWS account root user access.Which combination of actions will accomplish this? ",
        "answers": [
            0,
            1
        ],
        "options": [
            "Ensure the root user uses a strong password.",
            "Enable multi-factor authentication to the root user.",
            "Store root user access keys in an encrypted Amazon S3 bucket.",
            "Add the root user to a group containing administrative permissions.",
            "Apply the required permissions to the root user with an inline policy document."
        ],
        "explination": "Correct Answer: AB \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #34\n\nA solutions architect at an ecommerce company wants to back up application log data to Amazon S3. The solutions architect is unsure how frequently the logs will be accessed or which logs will be accessed the most. The company wants to keep costs as low as possible by using the appropriate S3 storage class.Which S3 storage class should be implemented to meet these requirements?",
        "answers": [
            1
        ],
        "options": [
            "S3 Glacier",
            "S3 Intelligent-Tiering",
            "S3 Standard-Infrequent Access (S3 Standard-IA)",
            "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 1,
        "current_probability": 0.0
    },
    {
        "question": "Question #35\n\nA company's website is used to sell products to the public. The site runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer(ALB). There is also an Amazon CloudFront distribution, and AWS WAF is being used to protect against SQL injection attacks. The ALB is the origin for theCloudFront distribution. A recent review of security logs revealed an external malicious IP that needs to be blocked from accessing the website.What should a solutions architect do to protect the application?",
        "answers": [
            1
        ],
        "options": [
            "Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP address.",
            "Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address.",
            "Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.",
            "Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the malicious IP address."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from. Later in the process, when you create a web ACL, you specify whether to allow or block requests from those IP addresses.AWS Web Application Firewall (WAF) \u05d2\u20ac\" Helps to protect your web applications from common application-layer exploits that can affect availability or consume excessive resources. As you can see in my post (New \u05d2\u20ac\" AWS WAF), WAF allows you to use access control lists (ACLs), rules, and conditions that define acceptable or unacceptable requests or IP addresses. You can selectively allow or deny access to specific parts of your web application and you can also guard against various SQL injection attacks. We launched WAF with support for Amazon CloudFront.Reference:https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-loadbalancers/ https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #36\n\nA solutions architect is designing an application for a two-step order process. The first step is synchronous and must return to the user with little latency. The second step takes longer, so it will be implemented in a separate component. Orders must be processed exactly once and in the order in which they are received.How should the solutions architect integrate these components?",
        "answers": [
            2
        ],
        "options": [
            "Use Amazon SQS FIFO queues.",
            "Use an AWS Lambda function along with Amazon SQS standard queues.",
            "Create an SNS topic and subscribe an Amazon SQS FIFO queue to that topic.",
            "Create an SNS topic and subscribe an Amazon SQS Standard queue to that topic."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #37\n\nA web application is deployed in the AWS Cloud. It consists of a two-tier architecture that includes a web layer and a database layer. The web server is vulnerable to cross-site scripting (XSS) attacks.What should a solutions architect do to remediate the vulnerability?",
        "answers": [
            2
        ],
        "options": [
            "Create a Classic Load Balancer. Put the web layer behind the load balancer and enable AWS WAF.",
            "Create a Network Load Balancer. Put the web layer behind the load balancer and enable AWS WAF.",
            "Create an Application Load Balancer. Put the web layer behind the load balancer and enable AWS WAF.",
            "Create an Application Load Balancer. Put the web layer behind the load balancer and use AWS Shield Standard."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Working with cross-site scripting match conditionsAttackers sometimes insert scripts into web requests in an effort to exploit vulnerabilities in web applications. You can create one or more cross-site scripting match conditions to identify the parts of web requests, such as the URI or the query string, that you want AWS WAF Classic to inspect for possible malicious scripts. Later in the process, when you create a web ACL, you specify whether to allow or block requests that appear to contain malicious scripts.Web Application Firewall -You can now use AWS WAF to protect your web applications on your Application Load Balancers. AWS WAF is a web application firewall that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources.Reference:https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-xss-conditions.html https://aws.amazon.com/elasticloadbalancing/features/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #38\n\nA company's website is using an Amazon RDS MySQL Multi-AZ DB instance for its transactional data storage. There are other internal systems that query this DB instance to fetch data for internal batch processing. The RDS DB instance slows down significantly when the internal systems fetch data. This impacts the website's read and write performance, and the users experience slow response times.Which solution will improve the website's performance?",
        "answers": [
            3
        ],
        "options": [
            "Use an RDS PostgreSQL DB instance instead of a MySQL database.",
            "Use Amazon ElastiCache to cache the query responses for the website.",
            "Add an additional Availability Zone to the current RDS MySQL Multi-AZ DB instance.",
            "Add a read replica to the RDS DB instance and configure the internal systems to query the read replica."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Amazon RDS Read Replicas -Enhanced performance -You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. Read replicas allow you to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Because read replicas can be promoted to master status, they are useful as part of a sharding implementation.To further maximize read performance, Amazon RDS for MySQL allows you to add table indexes directly to Read Replicas, without those indexes being present on the master.Reference:https://aws.amazon.com/rds/features/read-replicas",
        "topic": "Topic 1"
    },
    {
        "question": "Question #39\n\nAn application runs on Amazon EC2 instances across multiple Availability Zones. The instances run in an Amazon EC2 Auto Scaling group behind an ApplicationLoad Balancer. The application performs best when the CPU utilization of the EC2 instances is at or near 40%.What should a solutions architect do to maintain the desired performance across all instances in the group?",
        "answers": [
            1
        ],
        "options": [
            "Use a simple scaling policy to dynamically scale the Auto Scaling group.",
            "Use a target tracking policy to dynamically scale the Auto Scaling group.",
            "Use an AWS Lambda function to update the desired Auto Scaling group capacity.",
            "Use scheduled scaling actions to scale up and scale down the Auto Scaling group."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                \u05d2\u20acWith target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 AutoScaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling djustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the targetvalue, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern. For example, you can use target tracking scaling to: Configure a target tracking scaling policy to keep the average aggregate CPU utilization of your Auto Scaling group at 40 percent. Configure a target tracking scaling policy to keep the request count per target of your Application Load Balancer target group at 1000 for your AutoScaling group.\u05d2\u20acReference:https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #40\n\nA company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning.How should the scaling be changed to address the staff complaints and keep costs to a minimum?",
        "answers": [
            0
        ],
        "options": [
            "Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.",
            "Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.",
            "Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.",
            "Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens."
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 0,
        "current_probability": 1.0
    },
    {
        "question": "Question #391\n\nA company hosts a website on premises and wants to migrate it to the AWS Cloud. The website exposes a single hostname to the internet but it routes its functions to different on-premises server groups based on the path of the URL. The server groups are scaled independently depending on the needs of the functions they support. The company has an AWS Direct Connect connection configured to its on-premises network.What should a solutions architect do to provide path-based routing to send the traffic to the correct group of servers?",
        "answers": [
            1
        ],
        "options": [
            "Route all traffic to an internet gateway. Configure pattern matching rules at the internet gateway to route traffic to the group of servers supporting that path.",
            "Route all traffic to a Network Load Balancer (NLB) with target groups for each group of servers. Use pattern matching rules at the NLB to route traffic to the correct target group.",
            "Route all traffic to an Application Load Balancer (ALB). Configure path-based routing at the ALB to route traffic to the correct target group for the servers supporting that path.",
            "Use Amazon Route 53 as the DNS server. Configure Route 53 path-based alias records to route traffic to the correct Elastic Load Balancer for the group of servers supporting that path."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #392\n\nAn application uses an Amazon RDS MySQL DB instance. The RDS database is becoming low on disk space. A solutions architect wants to increase the disk space without downtime. Which solution meets these requirements with the LEAST amount of effort?",
        "answers": [
            0
        ],
        "options": [
            "Enable storage auto scaling in RDS.",
            "Increase the RDS database instance size.",
            "Change the RDS database instance storage type to Provisioned IOPS.",
            "Back up the RDS database, increase the storage capacity, restore the database and stop the previous instance."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #393\n\nAn ecommerce website is deploying its web application as Amazon Elastic Container Service (Amazon ECS) container instances behind an Application LoadBalancer (ALB). During periods of high activity, the website slows down and availability is reduced. A solutions architect uses Amazon CloudWatch alarms to receive notifications whenever there is an availability issue so they can scale out resources. Company management wants a solution that automatically responds to such events.Which solution meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Set up AWS Auto Scaling to scale out the ECS service when there are timeouts on the ALB. Set up AWS Auto Scaling to scale out the ECS cluster when the CPU or memory reservation is too high.",
            "Set up AWS Auto Scaling to scale out the ECS service when the ALB CPU utilization is too high. Setup AWS Auto Scaling to scale out the ECS cluster when the CPU or memory reservation is too high.",
            "Set up AWS Auto Scaling to scale out the ECS service when the service's CPU utilization is too high. Set up AWS Auto Scaling to scale out the ECS cluster when the CPU or memory reservation is too high.",
            "Set up AWS Auto Scaling to scale out the ECS service when the ALB target group CPU utilization is too high. Set up AWS Auto Scaling to scale out the ECS cluster when the CPU or memory reservation is too high."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #394\n\nA company has a website deployed on AWS. The database backend is hosted on Amazon RDS for MySQL with a primary instance and five read replicas to support scaling needs. The read replicas should lag no more than 1 second behind the primary instance to support the user experience.As traffic on the website continues to increase, the replicas are falling further behind during periods of peak load, resulting in complaints from users when searches yield inconsistent results. A solutions architect needs to reduce the replication lag as much as possible, with minimal changes to the application code or operational requirements.Which solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Migrate the database to Amazon Aurora MySQL. Replace the MySQL read replicas with Aurora Replicas and enable Aurora Auto Scaling",
            "Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the website to check the cache before querying the database read endpoints.",
            "Migrate the database from Amazon RDS to MySQL running on Amazon EC2 compute instances. Choose very large compute optimized instances for all replica nodes.",
            "Migrate the database to Amazon DynamoDB. Initially provision a large number of read capacity units (RCUs) to support the required throughput with on- demand capacity scaling enabled."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #395\n\nA company has an API-based inventory reporting application running on Amazon EC2 instances. The application stores information in an Amazon DynamoDB table. The company's distribution centers have an on-premises shipping application that calls an API to update the inventory before printing shipping labels. The company has been experiencing application interruptions several times each day, resulting in lost transactions.What should a solutions architect recommend to improve application resiliency?",
        "answers": [
            0
        ],
        "options": [
            "Modify the shipping application to write to a local database.",
            "Modify the application APIs to run serverless using AWS Lambda",
            "Configure Amazon API Gateway to call the EC2 inventory application APIs.",
            "Modify the application to send inventory updates using Amazon Simple Queue Service (Amazon SQS)."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #396\n\nA company has a three-tier environment on AWS that ingests sensor data from its users' devices. The traffic flows through a Network Load Balancer (NLB) then toAmazon EC2 instances for the web tier, and finally toEC2 instances for the application tier that makes database calls.What should a solutions architect do to improve the security of data in transit to the web tier?",
        "answers": [
            2
        ],
        "options": [
            "Configure a TLS listener and add the server certificate on the NLB.",
            "Configure AWS Shield Advanced and enable AWS WAF on the NLB.",
            "Change the load balancer to an Application Load Balancer and attach AWS WAF to it.",
            "Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances using AWS Key Management Service (AWS KMS)."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #397\n\nA company runs an online marketplace web application on AWS. The application serves hundreds of thousands of users during peak hours. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval.What should a solutions architect recommend to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon write. Use DynamoDB Streams to share the transactions data with other applications.",
            "Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.",
            "Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in AmazonDynamoDB. Other applications can consume the transactions data off the Kinesis data stream.",
            "Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #398\n\nA company has a dynamic web application hosted on two Amazon EC2 instances. The company has its own SSL certificate, which is on each instance to performSSL termination.There has been an increase in traffic recently, and the operations team determined that SSL encryption and decryption is causing the compute capacity of the web servers to reach their maximum limit.What should a solutions architect do to increase the application's performance?",
        "answers": [
            3
        ],
        "options": [
            "Create a new SSL certificate using AWS Certificate Manager (ACM). Install the ACM certificate on each instance.",
            "Create an Amazon S3 bucket. Migrate the SSL certificate to the S3 bucket. Configure the EC2 instances to reference the bucket for SSL termination.",
            "Create another EC2 instance as a proxy server. Migrate the SSL certificate to the new instance and configure it to direct connections to the existing EC2 instances.",
            "Import the SSL certificate into AWS Certificate Manager (ACM). Create an Application Load Balancer with an HTTPS listener that uses the SSL certificate from ACM."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #399\n\nA web application must persist order data to Amazon S3 to support neat-real time processing. A solutions architect needs create an architecture that is both scalable and fault tolerant.Which solutions meet these requirements? ",
        "answers": [
            1,
            4
        ],
        "options": [
            "Write the order event to an Amazon DynamoDB table. Use DynamoDB Streams to trigger an AWS Lambda function that parses the payload and writes the data to Amazon S3.",
            "Write the order event to an Amazon Simple Queue Service (Amazon SQS) queue. Use the queue to trigger an AWSLambda function that parsers the payload and writes the data to Amazon S3.",
            "Write the order event to an Amazon Simple Notification Service (Amazon SNS) topic. Use the SNS topic to trigger an AWS Lambda function that parses the payload and writes the data to Amazon S3.",
            "Write the order event to an Amazon Simple Queue Service (Amazon SQS) queue. Use an Amazon EventBridge (Amazon CloudWatch Events) rule to trigger an AWS Lambda function that parses the payload and writes the data to Amazon S3.",
            "Write the order event to an Amazon Simple Notification Service (Amazon SNS) topic. Use an Amazon EventBridge (Amazon CloudWatch Events) rule to trigger an AWS Lambda function that parses the payload andwrites the data to Amazon S3."
        ],
        "explination": "Correct Answer: BE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #400\n\nA company has an application hosted on Amazon EC2 instances in two VPCs across different AWS Regions. To communicate with each other, the instances use the internet for connectivity. The security team wants to ensure that no communication between the instances happens over the internet.What should a solutions architect do to accomplish this?",
        "answers": [
            3
        ],
        "options": [
            "Create a NAT gateway and update the route table of the EC2 instances' subnet.",
            "Create a VPC endpoint and update the route table of the EC2 instances' subnet.",
            "Create a VPN connection and update the route table of the EC2 instances' subnet.",
            "Create a VPC peering connection and update the route table of the EC2 instances' subnet."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #401\n\nAn online shopping application accesses an Amazon RDS Multi-AZ DB instance. Database performance is slowing down the application. After upgrading to the next-generation instance type, there was no significant performance improvement.Analysis shows approximately 700 IOPS are sustained, common queries run for long durations and memory utilization is high.Which application change should a solutions architect recommend to resolve these issues?",
        "answers": [
            2
        ],
        "options": [
            "Migrate the RDS instance to an Amazon Redshift cluster and enable weekly garbage collection.",
            "Separate the long-running queries into a new Multi-AZ RDS database and modify the application to query whichever database is needed.",
            "Deploy a two-node Amazon ElastiCache cluster and modify the application to query the cluster first and query the database only if needed.",
            "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue for common queries and query it first and query the database only if needed."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #402\n\nA company is preparing to store confidential data in Amazon S3. For compliance reasons, the data must be encrypted at rest. Encryption key usage must be logged for auditing purposes. Keys must be rotated every year.Which solution meets these requirements and is the MOST operationally efficient?",
        "answers": [
            3
        ],
        "options": [
            "Server-side encryption with customer-provided keys (SSE-C)",
            "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
            "Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with manual rotation",
            "Server-side encryption with AWS KMS (SSE-KMS) customer master keys (CMKs) with automatic rotation"
        ],
        "total_times_question_attempted": 1,
        "correct_times_question_attempted": 0,
        "current_probability": 1.0
    },
    {
        "question": "Question #403\n\nA company is preparing to migrate its on-premises application to AWS. The application consists of application servers and a Microsoft SQL Server database The database cannot be migrated to a different engine because SQL Server features are used in the application's NET code. The company wants to attain the greatest availability possible while minimizing operational and management overhead.What should a solutions architect do to accomplish this?",
        "answers": [
            1
        ],
        "options": [
            "Install SQL Server on Amazon EC2 in a Multi-AZ deployment.",
            "Migrate the data to Amazon RDS for SQL Server in a Multi-AZ deployment.",
            "Deploy the database on Amazon RDS for SQL Server with Multi-AZ Replicas.",
            "Migrate the data to Amazon RDS for SQL Server in a cross-Region Multi-AZ deployment."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #404\n\nA company has an application running on Amazon EC2 instances in a private subnet. The application needs to store and retrieve data in Amazon S3. To reduce costs, the company wants to configure its AWS resources in a cost-effective manner.How should the company accomplish this?",
        "answers": [
            1
        ],
        "options": [
            "Deploy a NAT gateway to access the S3 buckets.",
            "Deploy AWS Storage Gateway to access the S3 buckets.",
            "Deploy an S3 gateway endpoint to access the S3 buckets.",
            "Deploy an S3 interface endpoint to access the S3 buckets."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #405\n\nA media company has an application that tracks user clicks on its websites and performs analytics to provide near-real time recommendations. The application has a Heel of Amazon EC2 instances that receive data from the websites and send the data to an Amazon RDS DB instance. Another fleet of EC2 instances hosts the portion of the application that is continuously checking changes in the database and executing SQL queries to provide recommendations. Management has requested a redesign to decouple the infrastructure. The solution must ensure that data analysts are writing SQL to analyze the data only No data can the lost during the deployment.What should a solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Use Amazon Kinesis Data Streams to capture the data from the websites Kinesis Data Firehose to persist the data on Amazon S3, and Amazon Athena to query the data.",
            "Use Amazon Kinesis Data Streams to capture the data from the websites. Kinesis Data Analytics to query the data, and Kinesis Data Firehose to persist the data on Amazon S3.",
            "Use Amazon Simple Queue Service (Amazon SQS) to capture the data from the websites, keep the fleet of EC2 instances, and change to a bigger instance type in the Auto Scaling group configuration.",
            "Use Amazon Simple Notification Service (Amazon SNS) to receive data from the websites and proxy the messages to AWS Lambda functions that execute the queries and persist the data. Change Amazon RDS to Amazon Aurora Serverless to persist the data."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #406\n\nA company runs an application that uses multiple Amazon EC2 instances to gather data from its users. The data is then processed and transferred to Amazon S3 for long-term storage. A review of the application shows that there were long periods of time when the EC2 instances were not being used. A solutions architect needs to design a solution that optimizes utilization and reduces costs.Which solution meets these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon EC2 in an Auto Scaling group with On-Demand instances.",
            "Build the application to use Amazon Lightsail with On-Demand Instances.",
            "Create an Amazon CloudWatch cron job to automatically stop the EC2 instances when there is no activity.",
            "Redesign the application to use an event-driven design with Amazon Simple Queue Service (Amazon SQS) and AWS Lambda."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #407\n\nA company is using Site-to-Site VPN connections for secure connectivity to its AWS Cloud resources from on premises. Due to an increase in traffic across theVPN connections to the Amazon EC2 instances, users are experiencing slower VPN connectivity.Which solution will improve the VPN throughput?",
        "answers": [
            0
        ],
        "options": [
            "Implement multiple customer gateways for the same network to scale the throughput.",
            "Use a transit gateway with equal cost multipath routing and add additional VPN tunnels.",
            "Configure a virtual private gateway with equal cost multipath routing and multiple channels.",
            "Increase the number of tunnels in the VPN configuration to scale the throughput beyond the default limit."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #408\n\nA company has a mobile game that reads most of its metadata from an Amazon RDS DB instance. As the game increased in popularity developers noticed slowdowns related to the game's metadata load times. Performance metrics indicate that simply scaling the database will not help. A solutions architect must explore all options that include capabilities for snapshots replication and sub-millisecond response times.What should the solutions architect recommend to solve these issues?",
        "answers": [
            1
        ],
        "options": [
            "Migrate the database to Amazon Aurora with Aurora Replicas.",
            "Migrate the database to Amazon DyramoDB with global tables.",
            "Add an Amazon ElastiCache for Redis layer in front of the database.",
            "Add an Amazon ElastiCache for Memcached layer in front of the database."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #409\n\nA company has several Amazon EC2 instances set up in a private subnet for security reasons. These instances host applications that read and write large amounts of data to and from Amazon S3 regularly. Currently, subnet routing directs all the traffic destined for the internet through a NAT gateway. The company wants to optimize the overall cost without impacting the ability of the application to communicate with Amazon S3 or the outside internet.What should a solutions architect do to optimize costs?",
        "answers": [
            2
        ],
        "options": [
            "Create an additional NAT gateway. Update the route table to route to the NAT gateway. Update the network ACL to allow S3 traffic.",
            "Create an internet gateway. Update the route table to route traffic to the internet gateway. Update the network ACL to allow S3 traffic.",
            "Create a VPC endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct traffic to the VPC endpoint.",
            "Create an AWS Lambda function outside of the VPC to handle S3 requests. Attach an IAM policy to the EC2 instances, allowing them to invoke the Lambda function."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #410\n\nA company is deploying an application in three AWS Regions using an Application Load Balancer Amazon Route 53 will be used to distribute traffic between theseRegions.Which Route 53 configuration should a solutions architect use to provide the MOST high-performing experience?",
        "answers": [
            0
        ],
        "options": [
            "Create an A record with a latency policy.",
            "Create an A record with a geolocation policy.",
            "Create a CNAME record with a failover policy.",
            "Create a CNAME record with a geoproximity policy."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #41\n\nA financial services company has a web application that serves users in the United States and Europe. The application consists of a database tier and a web server tier. The database tier consists of a MySQL database hosted in us-east-1. Amazon Route 53 geoproximity routing is used to direct traffic to instances in the closest Region. A performance review of the system reveals that European users are not receiving the same level of query performance as those in the UnitedStates.Which changes should be made to the database tier to improve performance?",
        "answers": [
            3
        ],
        "options": [
            "Migrate the database to Amazon RDS for MySQL. Configure Multi-AZ in one of the European Regions.",
            "Migrate the database to Amazon DynamoDB. Use DynamoDB global tables to enable replication to additional Regions.",
            "Deploy MySQL instances in each Region. Deploy an Application Load Balancer in front of MySQL to reduce the load on the primary instance.",
            "Migrate the database to an Amazon Aurora global database in MySQL compatibility mode. Configure read replicas in one of the European Regions."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #42\n\nA company hosts a static website on-premises and wants to migrate the website to AWS. The website should load as quickly as possible for users around the world. The company also wants the most cost-effective solution.What should a solutions architect do to accomplish this?",
        "answers": [
            1
        ],
        "options": [
            "Copy the website content to an Amazon S3 bucket. Configure the bucket to serve static webpage content. Replicate the S3 bucket to multiple AWS Regions.",
            "Copy the website content to an Amazon S3 bucket. Configure the bucket to serve static webpage content. Configure Amazon CloudFront with the S3 bucket as the origin.",
            "Copy the website content to an Amazon EBS-backed Amazon EC2 instance running Apache HTTP Server. Configure Amazon Route 53 geolocation routing policies to select the closest origin.",
            "Copy the website content to multiple Amazon EBS-backed Amazon EC2 instances running Apache HTTP Server in multiple AWS Regions. Configure Amazon CloudFront geolocation routing policies to select the closest origin."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                What Is Amazon CloudFront?Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users.CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving withCloudFront, the user is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.Using Amazon S3 Buckets for Your OriginWhen you use Amazon S3 as an origin for your distribution, you place any objects that you want CloudFront to deliver in an Amazon S3 bucket. You can use any method that is supported by Amazon S3 to get your objects into Amazon S3, for example, the Amazon S3 console or API, or a third-party tool. You can create a hierarchy in your bucket to store the objects, just as you would with any other Amazon S3 bucket.Using an existing Amazon S3 bucket as your CloudFront origin server doesn't change the bucket in any way; you can still use it as you normally would to store and access Amazon S3 objects at the standard Amazon S3 price. You incur regular Amazon S3 charges for storing the objects in the bucket.Reference:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #43\n\nA solutions architect is designing storage for a high performance computing (HPC) environment based on Amazon Linux. The workload stores and processes a large amount of engineering drawings that require shared storage and heavy computing.Which storage option would be the optimal solution?",
        "answers": [
            1
        ],
        "options": [
            "Amazon Elastic File System (Amazon EFS)",
            "Amazon FSx for Lustre",
            "Amazon EC2 instance store",
            "Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS SSD (io1)"
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Explanation -Amazon FSx for Lustre -Amazon FSx for Lustre is a new, fully managed service provided by AWS based on the Lustre file system. Amazon FSx for Lustre provides a high-performance file system optimized for fast processing of workloads such as machine learning, high performance computing (HPC), video processing, financial modeling, and electronic design automation (EDA).FSx for Lustre allows customers to create a Lustre filesystem on demand and associate it to an Amazon S3 bucket. As part of the filesystem creation, Lustre reads the objects in the buckets and adds that to the file system metadata. Any Lustre client in your VPC is then able to access the data, which gets cached on the high- speed Lustre filesystem. This is ideal for HPC workloads, because you can get the speed of an optimized Lustre file system without having to manage the complexity of deploying, optimizing, and managing the Lustre cluster.Additionally, having the filesystem work natively with Amazon S3 means you can shut down the Lustre filesystem when you don't need it but still access objects inAmazon S3 via other AWS Services. FSx for Lustre also allows you to also write the output of your HPC job back to Amazon S3.Reference:https://d1.awsstatic.com/whitepapers/AWS%20Partner%20Network_HPC%20Storage%20Options_2019_FINAL.pdf(p.8)",
        "topic": "Topic 1"
    },
    {
        "question": "Question #44\n\nA company is performing an AWS Well-Architected Framework review of an existing workload deployed on AWS. The review identified a public-facing website running on the same Amazon EC2 instance as a Microsoft Active Directory domain controller that was install recently to support other AWS services. A solutions architect needs to recommend a new design that would improve the security of the architecture and minimize the administrative demand on IT staff.What should the solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Use AWS Directory Service to create a managed Active Directory. Uninstall Active Directory on the current EC2 instance.",
            "Create another EC2 instance in the same subnet and reinstall Active Directory on it. Uninstall Active Directory.",
            "Use AWS Directory Service to create an Active Directory connector. Proxy Active Directory requests to the Active domain controller running on the current EC2 instance.",
            "Enable AWS Single Sign-On (AWS SSO) with Security Assertion Markup Language (SAML) 2.0 federation with the current Active Directory controller. Modify the EC2 instance's security group to deny public access to Active Directory."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                AWS Managed Microsoft AD -AWS Directory Service lets you run Microsoft Active Directory (AD) as a managed service. AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed Microsoft AD, is powered by Windows Server 2012 R2. When you select and launch this directory type, it is created as a highly available pair of domain controllers connected to your virtual private cloud (VPC). The domain controllers run in different Availability Zones in a region of your choice. Host monitoring and recovery, data replication, snapshots, and software updates are automatically configured and managed for you.Reference:https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_microsoft_ad.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #45\n\nA company hosts a static website within an Amazon S3 bucket. A solutions architect needs to ensure that data can be recovered in case of accidental deletion.Which action will accomplish this?",
        "answers": [
            0
        ],
        "options": [
            "Enable Amazon S3 versioning.",
            "Enable Amazon S3 Intelligent-Tiering.",
            "Enable an Amazon S3 lifecycle policy.",
            "Enable Amazon S3 cross-Region replication."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Data can be recover if versioning enable, also it provide a extra protection like file delete,MFA delete. MFA. Delete only works for CLI or API interaction, not in theAWS Management Console. Also, you cannot make version DELETE actions with MFA using IAM user credentials. You must use your root AWS account.Object Versioning -[1](version 222222) in a single bucket. S3 Versioning protects you from the consequences of unintended overwrites and deletions. You can also use it to archive objects so that you have access to previous versions.You must explicitly enable S3 Versioning on your bucket. By default, S3 Versioning is disabled. Regardless of whether you have enabled Versioning, each object in your bucket has a version ID. If you have not enabled Versioning, Amazon S3 sets the value of the version ID to null. If S3 Versioning is enabled, Amazon S3 assigns a version ID value for the object. This value distinguishes it from other versions of the same key.Reference:https://books.google.com.sg/books?id=wv45DQAAQBAJ&pg=PA39&lpg=PA39&dq=hosts+a+static+website+within+an+Amazon+S3+bucket.+A+solutions+architect+needs+to+ensure+that+data+can+be+recovered+in+case+of+accidental+deletion&source=bl&ots=0NolP5igY5&sig=ACfU3U3opL9Jha6jM2EI8x7EcjK4rigQHQ&hl=en&sa=X&ved=2ahUKEwiS9e3yy7vpAhVx73MBHZNoDnQQ6AEwAH oECBQQAQ#v=onepage&q=hosts%20a%20static%20website%20within%20an%20Amazon%20S3%20bucket.%20A%20solutions%20architect%20needs%20to%20ensure%20that%20data%20can%20be%20recovered%20in%20case%20of%20accidental%20deletion&f=false https://aws.amazon.com/blogs/security/securing-access-to-aws-using-mfa-part-3/ https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #46\n\nA company's production application runs online transaction processing (OLTP) transactions on an Amazon RDS MySQL DB instance. The company is launching a new reporting tool that will access the same data. The reporting tool must be highly available and not impact the performance of the production application.How can this be achieved?",
        "answers": [
            1
        ],
        "options": [
            "Create hourly snapshots of the production RDS DB instance.",
            "Create a Multi-AZ RDS Read Replica of the production RDS DB instance.",
            "Create multiple RDS Read Replicas of the production RDS DB instance. Place the Read Replicas in an Auto Scaling group.",
            "Create a Single-AZ RDS Read Replica of the production RDS DB instance. Create a second Single-AZ RDS Read Replica from the replica."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Amazon RDS Read Replicas Now Support Multi-AZ DeploymentsAmazon RDS Read Replicas enable you to create one or more read-only copies of your database instance within the same AWS Region or in a different AWSRegion. Updates made to the source database are then asynchronously copied to your Read Replicas. In addition to providing scalability for read-heavy workloads, Read Replicas can be promoted to become a standalone database instance when needed.Amazon RDS Multi-AZ deployments provide enhanced availability for database instances within a single AWS Region. With Multi-AZ, your data is synchronously replicated to a standby in a different Availability Zone (AZ). In the event of an infrastructure failure, Amazon RDS performs an automatic failover to the standby, minimizing disruption to your applications.You can now use Read Replicas with Multi-AZ as part of a disaster recovery (DR) strategy for your production databases. A well-designed and tested DR plan is critical for maintaining business continuity after a disaster. A Read Replica in a different region than the source database can be used as a standby database and promoted to become the new production database in case of a regional disruption.Reference:https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/#:~",
        "topic": "Topic 1"
    },
    {
        "question": "Question #47\n\nA company runs an application in a branch office within a small data closet with no virtualized compute resources. The application data is stored on an NFS volume. Compliance standards require a daily offsite backup of the NFS volume.Which solution meets these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Install an AWS Storage Gateway file gateway on premises to replicate the data to Amazon S3.",
            "Install an AWS Storage Gateway file gateway hardware appliance on premises to replicate the data to Amazon S3.",
            "Install an AWS Storage Gateway volume gateway with stored volumes on premises to replicate the data to Amazon S3.",
            "Install an AWS Storage Gateway volume gateway with cached volumes on premises to replicate the data to Amazon S3."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                AWS Storage Gateway Hardware ApplianceHardware Appliance -Storage Gateway is available as a hardware appliance, adding to the existing support for VMware ESXi, Microsoft Hyper-V, and Amazon EC2. This means that you can now make use of Storage Gateway in situations where you do not have a virtualized environment, server-class hardware or IT staff with the specialized skills that are needed to manage them. You can order appliances from Amazon.com for delivery to branch offices, warehouses, and \u05d2\u20acoutpost\u05d2\u20ac offices that lack dedicated IT resources. Setup (as you will see in a minute) is quick and easy, and gives you access to three storage solutions:File Gateway \u05d2\u20ac\" A file interface to Amazon S3, accessible via NFS or SMB. The files are stored as S3 objects, allowing you to make use of specialized S3 features such as lifecycle management and cross-region replication. You can trigger AWS Lambda functions, run Amazon Athena queries, and use Amazon Macie to discover and classify sensitive data.Reference:https://aws.amazon.com/blogs/aws/new-aws-storage-gateway-hardware-appliance/ https://aws.amazon.com/storagegateway/file/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #48\n\nA company's web application is using multiple Linux Amazon EC2 instances and storing data on Amazon Elastic Block Store (Amazon EBS) volumes. The company is looking for a solution to increase the resiliency of the application in case of a failure and to provide storage that complies with atomicity, consistency, isolation, and durability (ACID).What should a solutions architect do to meet these requirements?",
        "answers": [
            2
        ],
        "options": [
            "Launch the application on EC2 instances in each Availability Zone. Attach EBS volumes to each EC2 instance.",
            "Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Mount an instance store on each EC2 instance.",
            "Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Store data on Amazon Elastic File System (Amazon EFS) and mount a target on each instance.",
            "Create an Application Load Balancer with Auto Scaling groups across multiple Availability Zones. Store data using Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                How Amazon EFS Works with Amazon EC2The following illustration shows an example VPC accessing an Amazon EFS file system. Here, EC2 instances in the VPC have file systems mounted.In this illustration, the VPC has three Availability Zones, and each has one mount target created in it. We recommend that you access the file system from a mount target within the same Availability Zone. One of the Availability Zones has two subnets. However, a mount target is created in only one of the subnets.Benefits of Auto Scaling -Better fault tolerance. Amazon EC2 Auto Scaling can detect when an instance is unhealthy, terminate it, and launch an instance to replace it. You can also configure Amazon EC2 Auto Scaling to use multiple Availability Zones. If one Availability Zone becomes unavailable, Amazon EC2 Auto Scaling can launch instances in another one to compensate.Better availability. Amazon EC2 Auto Scaling helps ensure that your application always has the right amount of capacity to handle the current traffic demand.Better cost management. Amazon EC2 Auto Scaling can dynamically increase and decrease capacity as needed. Because you pay for the EC2 instances you use, you save money by launching instances when they are needed and terminating them when they aren't.Reference:https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#how-it-works-ec2 https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #49\n\nA security team to limit access to specific services or actions in all of the team's AWS accounts. All accounts belong to a large organization in AWS Organizations.The solution must be scalable and there must be a single point where permissions can be maintained.What should a solutions architect do to accomplish this?",
        "answers": [
            3
        ],
        "options": [
            "Create an ACL to provide access to the services or actions.",
            "Create a security group to allow accounts and attach it to user groups.",
            "Create cross-account roles in each account to deny access to the services or actions.",
            "Create a service control policy in the root organizational unit to deny access to the services or actions."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Service Control Policy concepts -SCPs offer central access controls for all IAM entities in your accounts. You can use them to enforce the permissions you want everyone in your business to follow. Using SCPs, you can give your developers more freedom to manage their own permissions because you know they can only operate within the boundaries you define.You create and apply SCPs through AWS Organizations. When you create an organization, AWS Organizations automatically creates a root, which forms the parent container for all the accounts in your organization. Inside the root, you can group accounts in your organization into organizational units (OUs) to simplify management of these accounts. You can create multiple OUs within a single organization, and you can create OUs within other OUs to form a hierarchical structure. You can attach SCPs to the organization root, OUs, and individual accounts. SCPs attached to the root and OUs apply to all OUs and accounts inside of them.SCPs use the AWS Identity and Access Management (IAM) policy language; however, they do not grant permissions. SCPs enable you set permission guardrails by defining the maximum available permissions for IAM entities in an account. If a SCP denies an action for an account, none of the entities in the account can take that action, even if their IAM permissions allow them to do so. The guardrails set in SCPs apply to allIAM entities in the account, which include all users, roles, and the account root user.Reference:https://aws.amazon.com/blogs/security/how-to-use-service-control-policies-to-set-permission-guardrails-across-accounts-in-your-aws-organization/#:~:text=Central%20security%20administrators%20use%20service,users%20and%20roles)%20adhere%20to.&text=Now%2C%20using%20SCPs%2C%20you%20can,your%20organization%20or%20organizational%20unithttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #50\n\nA data science team requires storage for nightly log processing. The size and number of logs is unknown and will persist for 24 hours only.What is the MOST cost-effective solution?",
        "answers": [
            1
        ],
        "options": [
            "Amazon S3 Glacier",
            "Amazon S3 Standard",
            "Amazon S3 Intelligent-Tiering",
            "Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)"
        ],
        "total_times_question_attempted": 0,
        "correct_times_question_attempted": 0,
        "current_probability": 0,
        "tags": [
            "S3"
        ]
    },
    {
        "question": "Question #51\n\nA company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon Elastic Block Store(Amazon EBS) volume. For better scalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind an Application Load Balancer. After completing this change, users reported that each time they refreshed the website, they could see one subset of their documents or the other, but never all of the documents at the same time.What should a solutions architect propose to ensure users see all of their documents at once?",
        "answers": [
            2
        ],
        "options": [
            "Copy the data so both EBS volumes contain all the documents.",
            "Configure the Application Load Balancer to direct a user to the server with the documents.",
            "Copy the data from both EBS volumes to Amazon Elastic File System (Amazon EFS). Modify the application to save new documents to Amazon Elastic File System (Amazon EFS).",
            "Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Amazon EFS provides file storage in the AWS Cloud. With Amazon EFS, you can create a file system, mount the file system on an Amazon EC2 instance, and then read and write data to and from your file system. You can mount an Amazon EFS file system in your VPC, through the Network File System versions 4.0 and4.1 (NFSv4) protocol. We recommend using a current generation Linux NFSv4.1 client, such as those found in the latest Amazon Linux, Redhat, and UbuntuAMIs, in conjunction with the Amazon EFS Mount Helper. For instructions, see Using the amazon-efs-utils Tools.For a list of Amazon EC2 Linux Amazon Machine Images (AMIs) that support this protocol, see NFS Support. For some AMIs, you'll need to install an NFS client to mount your file system on your Amazon EC2 instance. For instructions, see Installing the NFS Client.You can access your Amazon EFS file system concurrently from multiple NFS clients, so applications that scale beyond a single connection can access a file system. Amazon EC2 instances running in multiple Availability Zones within the same AWS Region can access the file system, so that many users can access and share a common data source.How Amazon EFS Works with Amazon EC2Reference:https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#how-it-works-ec2",
        "topic": "Topic 1"
    },
    {
        "question": "Question #52\n\nA company is planning to use Amazon S3 to store images uploaded by its users. The images must be encrypted at rest in Amazon S3. The company does not want to spend time managing and rotating the keys, but it does want to control who can access those keys.What should a solutions architect use to accomplish this?",
        "answers": [
            3
        ],
        "options": [
            "Server-Side Encryption with keys stored in an S3 bucket",
            "Server-Side Encryption with Customer-Provided Keys (SSE-C)",
            "Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)",
            "Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS)"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                \"Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3, but with some additional benefits and charges for using this service. There are separate permissions for the use of a CMK that provides added protection against unauthorized access of your objects in Amazon S3. SSE-KMS also provides you with an audit trail that shows when your CMK was used and by whom.\"Server-Side Encryption: Using SSE-KMSYou can protect data at rest in Amazon S3 by using three different modes of server-side encryption: SSE-S3, SSE-C, or SSE-KMS.SSE-S3 requires that Amazon S3 manage the data and master encryption keys. For more information about SSE-S3, see Protecting Data Using Server-SideEncryption with Amazon S3-Managed Encryption Keys (SSE-S3).SSE-C requires that you manage the encryption key. For more information about SSE-C, see Protecting Data Using Server-Side Encryption with Customer-Provided Encryption Keys (SSE-C).SSE-KMS requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS.The remainder of this topic discusses how to protect data by using server-side encryption with AWSKMS-managed keys (SSE-KMS).You can request encryption and select a CMK by using the Amazon S3 console or API. In the console, check the appropriate box to perform encryption and select your CMK from the list. For the Amazon S3 API, specify encryption and choose your CMK by setting the appropriate headers in a GET or PUT request.Reference:https://aws.amazon.com/kms/faqs/https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html#sse",
        "topic": "Topic 1"
    },
    {
        "question": "Question #53\n\nA company is running an ecommerce application on Amazon EC2. The application consists of a stateless web tier that requires a minimum of 10 instances, and a peak of 250 instances to support the application's usage. The application requires 50 instances 80% of the time.Which solution should be used to minimize costs?",
        "answers": [
            3
        ],
        "options": [
            "Purchase Reserved Instances to cover 250 instances.",
            "Purchase Reserved Instances to cover 80 instances. Use Spot Instances to cover the remaining instances.",
            "Purchase On-Demand Instances to cover 40 instances. Use Spot Instances to cover the remaining instances.",
            "Purchase Reserved Instances to cover 50 instances. Use On-Demand and Spot Instances to cover the remaining instances."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Reserved Instances -Having 50 EC2 RIs provide a discounted hourly rate and an optional capacity reservation for EC2 instances. AWS Billing automatically applies your RI's discounted rate when attributes of EC2 instance usage match attributes of an active RI.If an Availability Zone is specified, EC2 reserves capacity matching the attributes of the RI. The capacity reservation of an RI is automatically utilized by running instances matching these attributes.You can also choose to forego the capacity reservation and purchase an RI that is scoped to a region. RIs that are scoped to a region automatically apply the RI's discount to instance usage across AZs and instance sizes in a region, making it easier for you to take advantage of the RI's discounted rate.On-Demand Instance -On-Demand instances let you pay for compute capacity by the hour or second (minimum of 60 seconds) with no long-term commitments. This frees you from the costs and complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed costs into much smaller variable costs.The pricing below includes the cost to run private and public AMIs on the specified operating system (\u05d2\u20acWindows Usage\u05d2\u20ac prices apply to Windows Server 2003 R2,2008, 2008 R2, 2012, 2012 R2, 2016, and 2019). Amazon also provides you with additional instances for Amazon EC2 running Microsoft Windows with SQLServer, Amazon EC2 running SUSE Linux Enterprise Server, Amazon EC2 running Red Hat Enterprise Linux and Amazon EC2 running IBM that are priced differently.Spot Instances -A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2, and adjusted gradually based on the long-term supply of and demand for Spot Instances. YourSpot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price.Reference:https://aws.amazon.com/ec2/pricing/reserved-instances/https://aws.amazon.com/ec2/pricing/on-demand/https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #54\n\nA company has deployed an API in a VPC behind an internet-facing Application Load Balancer (ALB). An application that consumes the API as a client is deployed in a second account in private subnets behind a NAT gateway. When requests to the client application increase, the NAT gateway costs are higher than expected. A solutions architect has configured the ALB to be internal.Which combination of architectural changes will reduce the NAT gateway costs? ",
        "answers": [
            3,
            4
        ],
        "options": [
            "Configure a VPC peering connection between the two VPCs. Access the API using the private address.",
            "Configure an AWS Direct Connect connection between the two VPCs. Access the API using the private address.",
            "Configure a ClassicLink connection for the API into the client VPC. Access the API using the ClassicLink address.",
            "Configure a PrivateLink connection for the API into the client VPC. Access the API using the PrivateLink address.",
            "Configure an AWS Resource Access Manager connection between the two accounts. Access the API using the private address."
        ],
        "explination": "Correct Answer: DE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #55\n\nA solutions architect is tasked with transferring 750 TB of data from an on-premises network-attached file system located at a branch office Amazon S3 Glacier.The migration must not saturate the on-premises 1 Mbps internet connection.Which solution will meet these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Create an AWS site-to-site VPN tunnel to an Amazon S3 bucket and transfer the files directly. Transfer the files directly by using the AWS CLI.",
            "Order 10 AWS Snowball Edge Storage Optimized devices, and select an S3 Glacier vault as the destination.",
            "Mount the network-attached file system to an S3 bucket, and copy the files directly. Create a lifecycle policy to transition the S3 objects to Amazon S3 Glacier.",
            "Order 10 AWS Snowball Edge Storage Optimized devices, and select an Amazon S3 bucket as the destination. Create a lifecycle policy to transition the S3 objects to Amazon S3 Glacier."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #56\n\nA company has a two-tier application architecture that runs in public and private subnets. Amazon EC2 instances running the web application are in the public subnet and a database runs on the private subnet. The web application instances and the database are running in a single Availability Zone (AZ).Which combination of steps should a solutions architect take to provide high availability for this architecture? ",
        "answers": [
            1,
            4
        ],
        "options": [
            "Create new public and private subnets in the same AZ for high availability.",
            "Create an Amazon EC2 Auto Scaling group and Application Load Balancer spanning multiple AZs.",
            "Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer.",
            "Create new public and private subnets in a new AZ. Create a database using Amazon EC2 in one AZ.",
            "Create new public and private subnets in the same VPC, each in a new AZ. Migrate the database to an Amazon RDS multi-AZ deployment."
        ],
        "explination": "Correct Answer: BE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #57\n\nA solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The solution must prevent an accidental deletion of the documents and ensure that all versions of the documents are available. Users must be able to download, modify, and upload documents.Which combination of actions should be taken to meet these requirements? ",
        "answers": [
            1,
            3
        ],
        "options": [
            "Enable a read-only bucket ACL.",
            "Enable versioning on the bucket.",
            "Attach an IAM policy to the bucket.",
            "Enable MFA Delete on the bucket.",
            "Encrypt the bucket using AWS KMS."
        ],
        "explination": "Correct Answer: BD \ud83d\uddf3\ufe0f                                                Object Versioning -[1](version 222222) in a single bucket. S3 Versioning protects you from the consequences of unintended overwrites and deletions. You can also use it to archive objects so that you have access to previous versions.To customize your data retention approach and control storage costs, use object versioning with Object lifecycle management. For information about creating S3Lifecycle policies using the AWS Management Console, see How Do I Create a Lifecycle Policy for an S3 Bucket? in the Amazon Simple Storage Service ConsoleUser Guide.If you have an object expiration lifecycle policy in your non-versioned bucket and you want to maintain the same permanent delete behavior when you enable versioning, you must add a noncurrent expiration policy. The noncurrent expiration lifecycle policy will manage the deletes of the noncurrent object versions in the version-enabled bucket. (A version-enabled bucket maintains one current and zero or more noncurrent object versions.)You must explicitly enable S3 Versioning on your bucket. By default, S3 Versioning is disabled. Regardless of whether you have enabled Versioning, each object in your bucket has a version ID. If you have not enabled Versioning, Amazon S3 sets the value of the version ID to null. If S3 Versioning is enabled, Amazon S3 assigns a version ID value for the object. This value distinguishes it from other versions of the same key.Enabling and suspending versioning is done at the bucket level. When you enable versioning on an existing bucket, objects that are already stored in the bucket are unchanged. The version IDs (null), contents, and permissions remain the same. After you enable S3 Versioning for a bucket, each object that is added to the bucket gets a version ID, which distinguishes it from other versions of the same key.Only Amazon S3 generates version IDs, and they can't be edited. Version IDs are Unicode, UTF-8 encoded, URL-ready, opaque strings that are no more than1,024 bytes long. The following is an example: 3/L4kqtJlcpXroDTDmJ+rmSpXd3dIbrHY+MTRCxf3vjVBH40Nr8X8gdRQBpUMLUo.Using MFA delete -If a bucket's versioning configuration is MFA Delete\u05d2\u20ac\"enabled, the bucket owner must include the x-amz-mfa request header in requests to permanently delete an object version or change the versioning state of the bucket. Requests that include x-amz-mfa must use HTTPS. The header's value is the concatenation of your authentication device's serial number, a space, and the authentication code displayed on it. If you do not include this request header, the request fails.Reference:https://aws.amazon.com/s3/features/https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #58\n\nAn application hosted on AWS is experiencing performance problems, and the application vendor wants to perform an analysis of the log file to troubleshoot further. The log file is stored on Amazon S3 and is 10 GB in size. The application owner will make the log file available to the vendor for a limited time.What is the MOST secure way to do this?",
        "answers": [
            2
        ],
        "options": [
            "Enable public read on the S3 object and provide the link to the vendor.",
            "Upload the file to Amazon WorkDocs and share the public link with the vendor.",
            "Generate a presigned URL and have the vendor download the log file before it expires.",
            "Create an IAM user for the vendor to provide access to the S3 bucket and the application. Enforce multi-factor authentication."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Share an object with others -All objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant time-limited permission to download the objects.When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method(GET to download the object) and expiration date and time. The presigned URLs are valid only for the specified duration.Anyone who receives the presigned URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a presigned URL.Reference:https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #59\n\nA solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company.How should security groups be configured in this situation? ",
        "answers": [
            0,
            2
        ],
        "options": [
            "Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.",
            "Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.",
            "Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier.",
            "Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier.",
            "Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier."
        ],
        "explination": "Correct Answer: AC \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #60\n\nA company allows its developers to attach existing IAM policies to existing IAM roles to enable faster experimentation and agility. However, the security operations team is concerned that the developers could attach the existing administrator policy, which would allow the developers to circumvent any other security policies.How should a solutions architect address this issue?",
        "answers": [
            3
        ],
        "options": [
            "Create an Amazon SNS topic to send an alert every time a developer creates a new policy.",
            "Use service control policies to disable IAM activity across all accounts in the organizational unit.",
            "Prevent the developers from attaching any policies and assign all IAM duties to the security operations team.",
            "Set an IAM permissions boundary on the developer IAM role that explicitly denies attaching the administrator policy."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #61\n\nA company has a multi-tier application that runs six front-end web servers in an Amazon EC2 Auto Scaling group in a single Availability Zone behind anApplication Load Balancer (ALB). A solutions architect needs to modify the infrastructure to be highly available without modifying the application.Which architecture should the solutions architect choose that provides high availability?",
        "answers": [
            1
        ],
        "options": [
            "Create an Auto Scaling group that uses three instances across each of two Regions.",
            "Modify the Auto Scaling group to use three instances across each of two Availability Zones.",
            "Create an Auto Scaling template that can be used to quickly create more instances in another Region.",
            "Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance traffic to the web tier."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Expanding Your Scaled and Load-Balanced Application to an Additional Availability Zone.When one Availability Zone becomes unhealthy or unavailable, Amazon EC2 Auto Scaling launches new instances in an unaffected zone. When the unhealthyAvailability Zone returns to a healthy state, Amazon EC2 Auto Scaling automatically redistributes the application instances evenly across all of the zones for yourAuto Scaling group. Amazon EC2 Auto Scaling does this by attempting to launch new instances in the Availability Zone with the fewest instances. If the attempt fails, however, Amazon EC2 Auto Scaling attempts to launch in other Availability Zones until it succeeds.You can expand the availability of your scaled and load-balanced application by adding an Availability Zone to your Auto Scaling group and then enabling that zone for your load balancer. After you've enabled the new Availability Zone, the load balancer begins to route traffic equally among all the enabled zones.Reference:https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #62\n\nA company runs an application on a group of Amazon Linux EC2 instances. For compliance reasons, the company must retain all application log files for 7 years.The log files will be analyzed by a reporting tool that must access all files concurrently.Which storage solution meets these requirements MOST cost-effectively?",
        "answers": [
            3
        ],
        "options": [
            "Amazon Elastic Block Store (Amazon EBS)",
            "Amazon Elastic File System (Amazon EFS)",
            "Amazon EC2 instance store",
            "Amazon S3"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Amazon S3 -Requests to Amazon S3 can be authenticated or anonymous. Authenticated access requires credentials that AWS can use to authenticate your requests. When making REST API calls directly from your code, you create a signature using valid credentials and include the signature in your request. Amazon Simple StorageService (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides easy-to-use management features so you can organize your data and configure finely-tuned access controls to meet your specific business, organizational, and compliance requirements. Amazon S3 is designed for 99.999999999%(11 9's) of durability, and stores data for millions of applications for companies all around the world.Reference:https://aws.amazon.com/s3/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #63\n\nA media streaming company collects real-time data and stores it in a disk-optimized database system. The company is not getting the expected throughput and wants an in-memory database storage solution that performs faster and provides high availability using data replication.Which database should a solutions architect recommend?",
        "answers": [
            2
        ],
        "options": [
            "Amazon RDS for MySQL",
            "Amazon RDS for PostgreSQL.",
            "Amazon ElastiCache for Redis",
            "Amazon ElastiCache for Memcached"
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                In-memory databases on AWS Amazon Elasticache for Redis.Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides submillisecond latency to power internet-scale, real-time applications.Developers can use ElastiCache for Redis as an in-memory nonrelational database. The ElastiCache for Redis cluster configuration supports up to 15 shards and enables customers to run Redis workloads with up to 6.1 TB of in-memory capacity in a single cluster. ElastiCache for Redis also provides the ability to add and remove shards from a running cluster. You can dynamically scaleout and even scale in your Redis cluster workloads to adapt to changes in demand.Reference:https://aws.amazon.com/elasticache/redis/faqs/https://aws.amazon.com/nosql/in-memory/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #64\n\nA company hosts its product information webpages on AWS. The existing solution uses multiple Amazon C2 instances behind an Application Load Balancer in anAuto Scaling group. The website also uses a custom DNS name and communicates with HTTPS only using a dedicated SSL certificate. The company is planning a new product launch and wants to be sure that users from around the world have the best possible experience on the new website.What should a solutions architect do to meet these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Redesign the application to use Amazon CloudFront.",
            "Redesign the application to use AWS Elastic Beanstalk.",
            "Redesign the application to use a Network Load Balancer.",
            "Redesign the application to use Amazon S3 static website hosting."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                What Is Amazon CloudFront?Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users.CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving withCloudFront, the user is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.If the content is already in the edge location with the lowest latency, CloudFront delivers it immediately.If the content is not in that edge location, CloudFront retrieves it from an origin that you've defined \u05d2\u20ac\" such as an Amazon S3 bucket, a MediaPackage channel, or an HTTP server (for example, a web server) that you have identified as the source for the definitive version of your content.As an example, suppose that you're serving an image from a traditional web server, not from CloudFront. For example, you might serve an image,[1]Your users can easily navigate to this URL and see the image. But they probably don't know that their request was routed from one network to another \u05d2\u20ac\" through the complex collection of interconnected networks that comprise the internet \u05d2\u20ac\" until the image was found.CloudFront speeds up the distribution of your content by routing each user request through the AWS backbone network to the edge location that can best serve your content. Typically, this is a CloudFront edge server that provides the fastest delivery to the viewer. Using the AWS network dramatically reduces the number of networks that your users' requests must pass through, which improves performance. Users get lower latency \u05d2\u20ac\" the time it takes to load the first byte of the file \u05d2\u20ac\" and higher data transfer rates.You also get increased reliability and availability because copies of your files (also known as objects) are now held (or cached) in multiple edge locations around the world.Reference:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #65\n\nA solutions architect is designing the cloud architecture for a new application being deployed on AWS. The process should run in parallel while adding and removing application nodes as needed based on the number of jobs to be processed. The processor application is stateless. The solutions architect must ensure that the application is loosely coupled and the job items are durably stored.Which design should the solutions architect use?",
        "answers": [
            2
        ],
        "options": [
            "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage.",
            "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network usage.",
            "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue.",
            "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f                                                Amazon Simple Queue Service -Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Get started with SQS in minutes using the AWS console, Command Line Interface or SDK of your choice, and three simple commands.SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.Scaling Based on Amazon SQS -There are some scenarios where you might think about scaling in response to activity in an Amazon SQS queue. For example, suppose that you have a web app that lets users upload images and use them online. In this scenario, each image requires resizing and encoding before it can be published. The app runs on EC2 instances in an Auto Scaling group, and it's configured to handle your typical upload rates. Unhealthy instances are terminated and replaced to maintain current instance levels at all times. The app places the raw bitmap data of the images in an SQS queue for processing. It processes the images and then publishes the processed images where they can be viewed by users. The architecture for this scenario works well if the number of image uploads doesn't vary over time. But if the number of uploads changes over time, you might consider using dynamic scaling to scale the capacity of your Auto Scaling group.Reference:https://aws.amazon.com/sqs/#:~:text=Amazon%20SQS%20leverages%20the%20AWS,queues%20provide%20nearly%20unlimited%20throughput https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #66\n\nA marketing company is storing CSV files in an Amazon S3 bucket for statistical analysis. An application on an Amazon EC2 instance needs permission to efficiently process the CSV data stored in the S3 bucket.Which action will MOST securely grant the EC2 instance access to the S3 bucket?",
        "answers": [
            2
        ],
        "options": [
            "Attach a resource-based policy to the S3 bucket.",
            "Create an IAM user for the application with specific permissions to the S3 bucket.",
            "Associate an IAM role with least privilege permissions to the EC2 instance profile.",
            "Store AWS credentials directly on the EC2 instance for applications on the instance to use for API calls."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #67\n\nA company has on-premises servers that run a relational database. The database serves high-read traffic for users in different locations. The company wants to migrate the database to AWS with the least amount of effort. The database solution must support high availability and must not affect the company's current traffic flow.Which solution meets these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Use a database in Amazon RDS with Multi-AZ and at least one read replica.",
            "Use a database in Amazon RDS with Multi-AZ and at least one standby replica.",
            "Use databases that are hosted on multiple Amazon EC2 instances in different AWS Regions.",
            "Use databases that are hosted on Amazon EC2 instances behind an Application Load Balancer in different Availability Zones."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #68\n\nA company's application is running on Amazon EC2 instances within an Auto Scaling group behind an Elastic Load Balancer. Based on the application's history, the company anticipates a spike in traffic during a holiday each year. A solutions architect must design a strategy to ensure that the Auto Scaling group proactively increases capacity to minimize any performance impact on application users.Which solution will meet these requirements?",
        "answers": [
            1
        ],
        "options": [
            "Create an Amazon CloudWatch alarm to scale up the EC2 instances when CPU utilization exceeds 90%.",
            "Create a recurring scheduled action to scale up the Auto Scaling group before the expected period of peak demand.",
            "Increase the minimum and maximum number of EC2 instances in the Auto Scaling group during the peak demand period.",
            "Configure an Amazon Simple Notification Service (Amazon SNS) notification to send alerts when there are autoscaling:EC2_INSTANCE_LAUNCH events."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #69\n\nA company hosts an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes for an AmazonRDS table, and deletes -the message from the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages.What should a solutions architect do to ensure messages are being processed once only?",
        "answers": [
            3
        ],
        "options": [
            "Use the CreateQueue API call to create a new queue.",
            "Use the AddPermission API call to add appropriate permissions.",
            "Use the ReceiveMessage API call to set an appropriate wait time.",
            "Use the ChangeMessageVisibility API call to increase the visibility timeout."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #70\n\nAn Amazon EC2 administrator created the following policy associated with an IAM group containing several users:What is the effect of this policy?",
        "answers": [
            2
        ],
        "options": [
            "Users can terminate an EC2 instance in any AWS Region except us-east-1.",
            "Users can terminate an EC2 instance with the IP address 10.100.100.1 in the us-east-1 Region.",
            "Users can terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254.",
            "Users cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #71\n\nA solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be available on demand. The event is expected to attract a global online audience.Which service will improve the performance of both the real-time and on-demand steaming?",
        "answers": [
            0
        ],
        "options": [
            "Amazon CloudFront",
            "AWS Global Accelerator",
            "Amazon Route S3",
            "Amazon S3 Transfer Acceleration"
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-streaming-video.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #72\n\nA company has a three-tier image-sharing application. It uses an Amazon EC2 instance for the front-end layer, another for the backend tier, and a third for theMySQL database. A solutions architect has been tasked with designing a solution that is highly available, and requires the least amount of changes to the applicationWhich solution meets these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon S3 to host the front-end layer and AWS Lambda functions for the backend layer. Move the database to an Amazon DynamoDB table and use Amazon S3 to store and serve users' images.",
            "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end and backend layers. Move the database to an Amazon RDS instance with multiple read replicas to store and serve users' images.",
            "Use Amazon S3 to host the front-end layer and a fleet of Amazon EC2 instances in an Auto Scaling group for the backend layer. Move the database to a memory optimized instance type to store and serve users' images.",
            "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end and backend layers. Move the database to an Amazon RDS instance with a Multi-AZ deployment. Use Amazon S3 to store and serve users' images."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #73\n\nA solutions architect is designing a system to analyze the performance of financial markets while the markets are closed. The system will run a series of compute- intensive jobs for 4 hours every night. The time to complete the compute jobs is expected to remain constant, and jobs cannot be interrupted once started. Once completed, the system is expected to run for a minimum of 1 year.Which type of Amazon EC2 instances should be used to reduce the cost of the system?",
        "answers": [
            3
        ],
        "options": [
            "Spot Instances",
            "On-Demand Instances",
            "Standard Reserved Instances",
            "Scheduled Reserved Instances"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Reference:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html",
        "topic": "Topic 1"
    },
    {
        "question": "Question #74\n\nA company built a food ordering application that captures user data and stores it for future analysis. The application's static front end is deployed on an AmazonEC2 instance. The front-end application sends the requests to the backend application running on separate EC2 instance. The backend application then stores the data in Amazon RDS.What should a solutions architect do to decouple the architecture and make it scalable?",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon S3 to serve the front-end application, which sends requests to Amazon EC2 to execute the backend application. The backend application will process and store the data in Amazon RDS.",
            "Use Amazon S3 to serve the front-end application and write requests to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon EC2 instances to the HTTP/HTTPS endpoint of the topic, and process and store the data in Amazon RDS.",
            "Use an EC2 instance to serve the front end and write requests to an Amazon SQS queue. Place the backend instance in an Auto Scaling group, and scale based on the queue depth to process and store the data in Amazon RDS.",
            "Use Amazon S3 to serve the static front-end application and send requests to Amazon API Gateway, which writes the requests to an Amazon SQS queue. Place the backend instances in an Auto Scaling group, and scale based on the queue depth to process and store the data in Amazon RDS."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #75\n\nA solutions architect needs to design a managed storage solution for a company's application that includes high-performance machine learning functionality. This application runs on AWS Fargate and the connected storage needs to have concurrent access to files and deliver high performance.Which storage option should the solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Create an Amazon S3 bucket for the application and establish an IAM role for Fargate to communicate with Amazon S3.",
            "Create an Amazon FSx for Lustre file share and establish an IAM role that allows Fargate to communicate with FSx for Lustre.",
            "Create an Amazon Elastic File System (Amazon EFS) file share and establish an IAM role that allows Fargate to communicate with Amazon Elastic File System (Amazon EFS).",
            "Create an Amazon Elastic Block Store (Amazon EBS) volume for the application and establish an IAM role that allows Fargate to communicate with Amazon Elastic Block Store (Amazon EBS)."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #76\n\nA bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles during peak operating hours. The company wants to use these data points in its existing analytics platform. A solutions architect must determine the most viable multi-tier option to support this architecture. The data points must be accessible from the REST API.Which action meets these requirements for storing and retrieving location data?",
        "answers": [
            3
        ],
        "options": [
            "Use Amazon Athena with Amazon S3.",
            "Use Amazon API Gateway with AWS Lambda.",
            "Use Amazon QuickSight with Amazon Redshift.",
            "Use Amazon API Gateway with Amazon Kinesis Data Analytics."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/kinesis/data-analytics/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #77\n\nA solutions architect is designing a web application that will run on Amazon EC2 instances behind an Application Load Balancer (ALB). The company strictly requires that the application be resilient against malicious internet activity and attacks, and protect against new common vulnerabilities and exposures.What should the solutions architect recommend?",
        "answers": [
            1
        ],
        "options": [
            "Leverage Amazon CloudFront with the ALB endpoint as the origin.",
            "Deploy an appropriate managed rule for AWS WAF and associate it with the ALB.",
            "Subscribe to AWS Shield Advanced and ensure common vulnerabilities and exposures are blocked.",
            "Configure network ACLs and security groups to allow only ports 80 and 443 to access the EC2 instances."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #78\n\nA company has an application that calls AWS Lambda functions. A code review shows that database credentials are stored in a Lambda function's source code, which violates the company's security policy. The credentials must be securely stored and must be automatically rotated on an ongoing basis to meet security policy requirements.What should a solutions architect recommend to meet these requirements in the MOST secure manner?",
        "answers": [
            1
        ],
        "options": [
            "Store the password in AWS CloudHSM. Associate the Lambda function with a role that can use the key ID to retrieve the password from CloudHSM. Use CloudHSM to automatically rotate the password.",
            "Store the password in AWS Secrets Manager. Associate the Lambda function with a role that can use the secret ID to retrieve the password from Secrets Manager. Use Secrets Manager to automatically rotate the password.",
            "Store the password in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can use the key ID to retrieve the password from AWS KMS. Use AWS KMS to automatically rotate the uploaded password.",
            "Move the database password to an environment variable that is associated with the Lambda function. Retrieve the password from the environment variable by invoking the function. Create a deployment script to automatically rotate the password."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f                                                Reference:https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials-amazon-rds-database-types-oracle/",
        "topic": "Topic 1"
    },
    {
        "question": "Question #79\n\nA company is managing health records on-premises. The company must keep these records indefinitely, disable any modifications to the records once they are stored, and granularly audit access at all levels. The chief technology officer (CTO) is concerned because there are already millions of records not being used by any application, and the current infrastructure is running out of space. The CTO has requested a solutions architect design a solution to move existing data and support future records.Which services can the solutions architect recommend to meet these requirements?",
        "answers": [
            0
        ],
        "options": [
            "Use AWS DataSync to move existing data to AWS. Use Amazon S3 to store existing and new data. Enable Amazon S3 object lock and enable AWS CloudTrail with data events.",
            "Use AWS Storage Gateway to move existing data to AWS. Use Amazon S3 to store existing and new data. Enable Amazon S3 object lock and enable AWS CloudTrail with management events.",
            "Use AWS DataSync to move existing data to AWS. Use Amazon S3 to store existing and new data. Enable Amazon S3 object lock and enable AWS CloudTrail with management events.",
            "Use AWS Storage Gateway to move existing data to AWS. Use Amazon Elastic Block Store (Amazon EBS) to store existing and new data. Enable Amazon S3 object lock and enable Amazon S3 server access logging."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #80\n\nA company wants to use Amazon S3 for the secondary copy of its on-premises dataset. The company would rarely need to access this copy. The storage solution's cost should be minimal.Which storage solution meets these requirements?",
        "answers": [
            3
        ],
        "options": [
            "S3 Standard",
            "S3 Intelligent-Tiering",
            "S3 Standard-Infrequent Access (S3 Standard-IA)",
            "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #81\n\nA company's operations team has an existing Amazon S3 bucket configured to notify an Amazon SQS queue when new objects are created within the bucket. The development team also wants to receive events when new objects are created. The existing operations team workflow must remain intact.Which solution would satisfy these requirements?",
        "answers": [
            3
        ],
        "options": [
            "Create another SQS queue. Update the S3 events in the bucket to also update the new queue when a new object is created.",
            "Create a new SQS queue that only allows Amazon S3 to access the queue. Update Amazon S3 to update this queue when a new object is created.",
            "Create an Amazon SNS topic and SQS queue for the bucket updates. Update the bucket to send events to the new topic. Updates both queues to poll Amazon SNS.",
            "Create an Amazon SNS topic and SQS queue for the bucket updates. Update the bucket to send events to the new topic. Add subscriptions for both queues in the topic."
        ],
        "explination": "Correct Answer: D \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #82\n\nAn application runs on Amazon EC2 instances in private subnets. The application needs to access an Amazon DynamoDB table. What is the MOST secure way to access the table while ensuring that the traffic does not leave the AWS network?",
        "answers": [
            0
        ],
        "options": [
            "Use a VPC endpoint for DynamoDB.",
            "Use a NAT gateway in a public subnet.",
            "Use a NAT instance in a private subnet.",
            "Use the internet gateway attached to the VPC."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #83\n\nA company built an application that lets users check in to places they visit, rank the places, and add reviews about their experiences. The application is successful with a rapid increase in the number of users every month.The chief technology officer fears the database supporting the current Infrastructure may not handle the new load the following month because the single AmazonRDS for MySQL instance has triggered alarms related to resource exhaustion due to read requests.What can a solutions architect recommend to prevent service Interruptions at the database layer with minimal changes to code?",
        "answers": [
            0
        ],
        "options": [
            "Create RDS read replicas and redirect read-only traffic to the read replica endpoints. Enable a Multi-AZ deployment.",
            "Create an Amazon EMR cluster and migrate the data to a Hadoop Distributed File System (HDFS) with a replication factor of 3.",
            "Create an Amazon ElastiCache cluster and redirect all read-only traffic to the cluster. Set up the cluster to be deployed in three Availability Zones.",
            "Create an Amazon DynamoDB table to replace the RDS instance and redirect all read-only traffic to the DynamoDB table. Enable DynamoDB Accelerator to offload traffic from the main table."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #84\n\nA company is looking for a solution that can store video archives in AWS from old news footage. The company needs to minimize costs and will rarely need to restore these files. When the files are needed, they must be available in a maximum of five minutes.What is the MOST cost-effective solution?",
        "answers": [
            0
        ],
        "options": [
            "Store the video archives in Amazon S3 Glacier and use Expedited retrievals.",
            "Store the video archives in Amazon S3 Glacier and use Standard retrievals.",
            "Store the video archives in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).",
            "Store the video archives in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #85\n\nA company has created a VPC with multiple private subnets in multiple Availability Zones (AZs) and one public subnet in one of the AZs. The public subnet is used to launch a NAT gateway. There are instances in the private subnets that use a NAT gateway to connect to the internet. In case of an AZ failure, the company wants to ensure that the instances are not all experiencing internet connectivity issues and that there is a backup plan ready.Which solution should a solutions architect recommend that is MOST highly available?",
        "answers": [
            2
        ],
        "options": [
            "Create a new public subnet with a NAT gateway in the same AZ. Distribute the traffic between the two NAT gateways.",
            "Create an Amazon EC2 NAT instance in a new public subnet. Distribute the traffic between the NAT gateway and the NAT instance.",
            "Create public subnets in each AZ and launch a NAT gateway in each subnet. Configure the traffic from the private subnets in each AZ to the respective NAT gateway.",
            "Create an Amazon EC2 NAT instance in the same public subnet. Replace the NAT gateway with the NAT instance and associate the instance with an Auto Scaling group with an appropriate scaling policy."
        ],
        "explination": "Correct Answer: C \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #86\n\nA healthcare company stores highly sensitive patient records. Compliance requires that multiple copies be stored in different locations. Each record must be stored for 7 years. The company has a service level agreement (SLA) to provide records to government agencies immediately for the first 30 days and then within4 hours of a request thereafter.What should a solutions architect recommend?",
        "answers": [
            0
        ],
        "options": [
            "Use Amazon S3 with cross-Region replication enabled. After 30 days, transition the data to Amazon S3 Glacier using lifecycle policy.",
            "Use Amazon S3 with cross-origin resource sharing (CORS) enabled. After 30 days, transition the data to Amazon S3 Glacier using a lifecycle policy.",
            "Use Amazon S3 with cross-Region replication enabled. After 30 days, transition the data to Amazon S3 Glacier Deep Achieve using a lifecycle policy.",
            "Use Amazon S3 with cross-origin resource sharing (CORS) enabled. After 30 days, transition the data to Amazon S3 Glacier Deep Archive using a lifecycle policy."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #87\n\nA company recently deployed a new auditing system to centralize information about operating system versions, patching, and installed software for Amazon EC2 instances. A solutions architect must ensure all instances provisioned through EC2 Auto Scaling groups successfully send reports to the auditing system as soon as they are launched and terminated.Which solution achieves these goals MOST efficiently?",
        "answers": [
            1
        ],
        "options": [
            "Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send data to the audit system.",
            "Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when instances are launched and terminated.",
            "Use an EC2 Auto Scaling launch configuration to run a custom script through user data to send data to the audit system when instances are launched and terminated.",
            "Run a custom script on the instance operating system to send data to the audit system. Configure the script to be executed by the EC2 Auto Scaling group when the instance starts and is terminated."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #88\n\nA company recently implemented hybrid cloud connectivity using AWS Direct Connect and is migrating data to Amazon S3. The company is looking for a fully managed solution that will automate and accelerate the replication of data between the on-premises storage systems and AWS storage services.Which solution should a solutions architect recommend to keep the data private?",
        "answers": [
            0
        ],
        "options": [
            "Deploy an AWS DataSync agent for the on-premises environment. Configure a sync job to replicate the data and connect it with an AWS service endpoint.",
            "Deploy an AWS DataSync agent for the on-premises environment. Schedule a batch job to replicate point-in-time snapshots to AWS.",
            "Deploy an AWS Storage Gateway volume gateway for the on-premises environment. Configure it to store data locally, and asynchronously back up point-in- time snapshots to AWS.",
            "Deploy an AWS Storage Gateway file gateway for the on-premises environment. Configure it to store data locally, and asynchronously back up point-in-time snapshots to AWS."
        ],
        "explination": "Correct Answer: A \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #89\n\nA company has 150 TB of archived image data stored on-premises that needs to be moved to the AWS Cloud within the next month. The company's current network connection allows up to 100 Mbps uploads for this purpose during the night only.What is the MOST cost-effective mechanism to move this data and meet the migration deadline?",
        "answers": [
            1
        ],
        "options": [
            "Use AWS Snowmobile to ship the data to AWS.",
            "Order multiple AWS Snowball devices to ship the data to AWS.",
            "Enable Amazon S3 Transfer Acceleration and securely upload the data.",
            "Create an Amazon S3 VPC endpoint and establish a VPN to upload the data."
        ],
        "explination": "Correct Answer: B \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    },
    {
        "question": "Question #90\n\nA public-facing web application queries a database hosted on an Amazon EC2 instance in a private subnet. A large number of queries involve multiple table joins, and the application performance has been degrading due to an increase in complex queries. The application team will be performing updates to improve performance.What should a solutions architect recommend to the application team? ",
        "answers": [
            1,
            4
        ],
        "options": [
            "Cache query data in Amazon SQS",
            "Create a read replica to offload queries",
            "Migrate the database to Amazon Athena",
            "Implement Amazon DynamoDB Accelerator to cache data.",
            "Migrate the database to Amazon RDS"
        ],
        "explination": "Correct Answer: BE \ud83d\uddf3\ufe0f",
        "topic": "Topic 1"
    }
]
